# プロダクト要求定義書（PRD）

## 1. プロダクトビジョンと目的

### ビジョン
製造業の現場において、外部に持ち出せない機密性の高い社内技術資料・不具合データベースを安全かつ効果的に活用できる、完全オフライン動作の **Agentic RAG システム**を構築する。

### 目的
- 社内のエンジニアが、蓄積された技術資料や不具合対応履歴に対して自然言語で問い合わせを行い、専門的な回答を迅速に得られる環境を提供する。
- 単純な検索（RAG）にとどまらず、AI エージェントが質問の意図を推論し、回答が不十分な場合には自律的に再検索・再推論を繰り返す **Agentic RAG** として実現する。
- データを外部クラウドに送信しないローカル LLM アーキテクチャにより、機密情報の漏洩リスクをゼロにする。

---

## 2. ターゲットユーザーと課題・ニーズ

### ターゲットユーザー
| ユーザー | 説明 |
|---|---|
| 製造部門の品質管理エンジニア | 量産品の不具合対応・品質改善活動を担当。**製品の不具合事例検索は全エンジニアに共通して必要** |
| 生産技術・設備エンジニア | 設備の不具合履歴・技術仕様の調査を担当。製品不具合への対応も必要 |
| 製品設計・開発エンジニア | 製品仕様書・設計資料の調査、過去の設計変更履歴・製品不具合事例の参照を担当 |
| システム管理者 / 開発担当者 | システムの導入・維持・改善を担当 |

### 課題・ニーズ
- 製品の不具合事例・対応履歴の検索は、品質管理・生産技術・製品設計にわたるすべての現場エンジニアに共通する業務ニーズである。
- 過去の不具合事例・対応履歴が分散した紙資料や社内サーバに存在し、必要情報への到達に時間がかかる。
- 技術資料がクリーニングされていない未加工の状態（OCR誤変換、混在フォーマット等）であっても検索・活用できる必要がある。
- 機密性の高い社内データを外部クラウド（OpenAI API 等）に送信できないため、完全オフライン環境での運用が必須。
- 質問が複雑・多段階の場合に、1回の検索では不十分な回答しか得られない課題がある。

---

## 3. 主要な機能一覧

| # | 機能 | 概要 |
|---|---|---|
| F-01 | 自然言語による質問応答 | ユーザーが日本語で質問を入力し、社内資料に基づく回答を取得する |
| F-02 | Agentic RAG（自律的再検索） | 回答が不十分な場合に AI エージェントが自律的に推論・再検索を繰り返す |
| F-03 | データソース取り込み | 社内技術資料をベクトル DB に登録する。初期実装は PDF を対象とするが、将来的に対象データの種類に応じてデータ読み込み〜チャンク分割の前処理モジュールを差し替えられる設計とする |
| F-04 | マルチターンチャット | 会話履歴を保持しながら複数ターンの対話を継続できる |
| F-05 | 思考過程の可視化 | AI エージェントの推論ステップ（Thinking）をログとして出力する |
| F-06 | Gradio UI | ブラウザベースの GUI でチャット操作を行う |
| F-07 | 完全オフライン動作 | インターネット接続なしで LLM・Embedding・Reranker の全処理が完結する |

---

## 4. 成功の定義

### 定量指標
- 現場エンジニア（品質管理・生産技術・製品設計）が過去の不具合事例を調査する際の平均所要時間を現状比 **50% 以上削減**できる。
- 専門的な技術質問（量産品の品質管理・不具合対応）に対して、エンジニアが「有用」と評価する回答率が **80% 以上**である。

### 定性指標
- ローカル環境のみで動作し、機密データが外部に送信されないことをエンジニアが確認できる。
- 将来的なモデル変更（LLM / Embedding / Reranker の入れ替え）が、ドメインロジックを変更せずに実施できる。
- 取り込むデータソースの変更に伴うデータ読み込み〜チャンク分割の前処理実装の変更が、ドメインロジックに影響を与えずに実施できる。
- テストコードによりエージェントのドメインロジック単体での動作検証が可能である。

---

## 5. ビジネス要件

- 製造業の社内システムとして、機密情報保護の観点からすべてのデータ処理がオフライン・オンプレミスで完結すること。
- 機密データをクラウドに送信しないローカル LLM 環境を実現するため、LLM・Embedding・Reranker はすべてローカルで動作する商用利用可能な OSS モデルを使用すること。
- プロトタイプは Google Colab の計算資源を活用し、Python エコシステム標準ツールで構築すること。
- 将来的な本番環境への移行（オンプレ GPU サーバへの展開）を想定した設計とすること。

---

## 6. ユーザーストーリー

| ID | ユーザー | 目的 | 価値 |
|---|---|---|---|
| US-01 | 品質管理エンジニア | 特定の不具合モードに関する過去事例を自然言語で検索したい | 類似事例の対処法を迅速に把握し、対応時間を短縮できる |
| US-02 | 品質管理エンジニア | 複雑な不具合の因果関係を多段階の質問で掘り下げたい | 1回の質問では得られない深い知見を対話的に引き出せる |
| US-03 | 生産技術・設備エンジニア | 設備の不具合履歴・仕様書・技術資料から必要な情報を調べたい | 設備トラブル対応に必要な情報への到達時間を短縮できる |
| US-04 | 製品設計・開発エンジニア | 製品仕様書・設計資料・過去の製品不具合事例を横断して調査したい | 設計判断に必要な過去の知見を迅速に収集できる |
| US-05 | システム管理者 | AI の回答根拠と思考過程を確認したい | 回答の信頼性を検証し、ハルシネーションのリスクを管理できる |
| US-06 | システム管理者 | 対象資料（PDF）を追加・変更したい | ドメインの変化に合わせてシステムを柔軟に更新できる |

---

## 7. 受け入れ条件

| US | 受け入れ条件 |
|---|---|
| US-01 | 取り込んだ社内資料に基づいた回答が返却され、回答に関連する資料のソース情報が明示される |
| US-02 | チャット画面で複数ターンの対話が継続でき、前のターンの文脈が次の質問に反映される |
| US-03 | 未加工（OCR 誤変換等を含む）の資料から有用な情報が抽出できる |
| US-04 | 製品仕様書・設計資料・不具合事例を対象とした質問に対して、横断的な回答が得られる |
| US-05 | AI エージェントの思考過程（Thinking）がログとして出力・確認できる |
| US-06 | Gradio UI のファイルアップロード機能で PDF をアップロードし処理を実行することで、ベクトル DB に追加登録できる。※ Google Colab 実行環境ではローカルディレクトリへの直接アクセスができないため、Gradio のファイルアップロード機能経由でファイルを受け渡す方式とする |

---

## 8. 機能要件

### FR-01 : Agentic RAG ワークフロー
- ユーザーの質問に対し、検索・推論・回答評価・再検索のサイクルを自律的に実行する。
- 回答の品質が閾値を下回る場合、エージェントは再クエリ生成と再検索を繰り返す（自己修正ループ）。
- 自己修正ループには **最大反復回数の上限** を設け、無限ループを防止する。上限に達した場合は、その時点で得られた最善の回答を返す。
- LLM が所定のデータ形式（構造化出力・JSON スキーマ等）で返答しなかった場合のエラーハンドリングを実装し、ワークフローが異常終了しないようにする。
- LangGraph を用いたワークフローグラフとして実装する。

### FR-02 : データ取り込みパイプライン
- データソースからテキストを抽出し、チャンク分割・クリーニングを行いベクトル DB に格納する。
- 初期実装は PDF を対象とする。
- 未加工データ（OCR 誤変換・改行混在等）を前提とし、前処理で最低限のクリーニングを実施する。
- **チャンク分割は短い粒度**で行う。FR-03 の2段階検索戦略（第1段階で短いチャンクを広く収集し、第2段階の Reranker で精査する）に基づく方針である。
- データ読み込みからチャンク分割までの前処理は、データソースの種類ごとに実装が異なる。これらは Frameworks and Drivers 層に位置付け、ドメインロジック（エージェント推論・ワークフロー）への依存を持たない設計とする。
- Embedding モデルで数値化し、Chroma DB（インメモリ）に格納する。

### FR-03 : 2段階検索・Reranking
**検索戦略：短いチャンクで広く収集し、Reranker で精査する2段階構成**

- **第1段階（広域収集）**: キーワード検索（BM25 等）とベクトル検索（コサイン類似度）のハイブリッド検索で、関連性の高いチャンクを幅広く収集する。短いチャンク分割と組み合わせることで取りこぼしを減らす。
- **第2段階（精査）**: Reranker モデルで第1段階の取得結果を再ランキングし、最終的な回答生成に使用するチャンクを絞り込む。
- この2段階構成により、単一手法では達成しにくい「広い収集」と「高精度な絞り込み」を両立させる。

### FR-04 : LLM 推論
- Ollama 経由でローカル LLM（gpt-oss:20b 相当）を呼び出す。
- Thinking 機能（LLM の推論過程）を有効化し、ログに出力する。
- ツールコール（関数呼び出し）をネイティブサポートするモデルを使用する。

### FR-05 : マルチターンチャット
- LangGraph のメモリ管理機能を使用し、会話履歴を保持する。
- チャット履歴はセッション内で保持し、UI 上に表示する。

### FR-06 : Gradio UI
- テキスト入力欄とチャット表示エリアを持つシンプルな Web UI を提供する。
- ユーザーは Gradio の Web インターフェース経由でシステムを操作する。

---

## 9. 非機能要件

### NFR-01 : セキュリティ・プライバシー
- すべてのデータ処理（LLM 推論・Embedding・Reranking）がローカル環境内で完結し、外部ネットワークへのデータ送信は行わない。
- 本番環境では GPU 搭載のオンプレミスサーバでの動作を想定する。

### NFR-02 : 保守性・拡張性（アーキテクチャ方針）
- Robert C. Martin（Uncle Bob）著『Clean Architecture』の **The Dependency Rule（依存性のルール）** に従い、以下の層分離を徹底する。

  | 層 | 役割 | 実装要素 |
  |---|---|---|
  | **Entities / Use Cases** | ドメインロジック（AIエージェントの推論・ワークフロー）および外部依存の**インターフェース（抽象）定義** | LangGraph グラフ定義・エージェントノード・LLM/DB アクセス用インターフェース（抽象クラス） |
  | **Interface Adapters** | UI との橋渡し、および外部インフラをドメイン側インターフェースに適合させる**実装（具体）** | Gradio ハンドラ・OllamaAdapter・ChromaDBAdapter・データ取り込みアダプタ等 |
  | **Frameworks and Drivers** | 外部インフラ（具体的なライブラリ・フレームワーク） | Ollama・Chroma DB・Sentence Transformers |

- 上位層（Entities / Use Cases）は下位層（Frameworks and Drivers）に直接依存してはならない。ドメインロジックは**インターフェース（抽象）にのみ依存**し、具体的な実装（Ollama の呼び出し処理・Chroma DB の操作等）は **Dependency Injection（DI）によって外部から注入**する。これにより、以下の変更がドメインロジックに影響を与えない設計を技術的に担保する。
  - LLM モデルの変更・ベクトル DB の差し替え
  - 取り込むデータソースの変更に伴うデータ読み込み〜チャンク分割の前処理実装の変更

### NFR-03 : テスト容易性
- エージェントのドメインロジック（推論・ワークフロー）が、LLM・DB・UI などの外部依存を切り離した状態でユニットテスト可能な設計とする（TDD が目指すテスト容易性の確保）。
- テストコードは `tests/` ディレクトリに配置し、`pytest` で実行可能とする。

### NFR-04 : 実行環境の制約（プロトタイプ）
- プロトタイプの計算資源には **Google Colab** を使用するため、Main ルーチンは `.py` スクリプトではなく **Jupyter Notebook 形式（`.ipynb`）** として実装する。
- モジュール（`src/` 配下の `.py` ファイル）は Google Colab 環境から呼び出せる形式とする。

### NFR-05 : モデル要件
- LLM・Embedding・Reranker はすべて **商用利用可能な OSS モデル** を使用する。
- 日本語の技術文書に対して有効な性能を持つモデルを選定する（JMTEB Leaderboard 等を参照）。
