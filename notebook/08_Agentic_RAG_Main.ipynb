{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 08 Agentic RAG Mainルーチン（Clean Architecture）\n",
    "\n",
    "**`src/` 配下の Clean Architecture モジュールを `DIContainer` 経由で使用する Main ルーチン。**\n",
    "\n",
    "gpt-oss:20b を使用する構成のため、**Colab GPU は L4 を使用すること。**\n",
    "\n",
    "#### アーキテクチャ\n",
    "- **Domain 層**: ドメインモデル、Port（Protocol）、`WorkflowConfig`\n",
    "- **Use Cases 層**: `AgentWorkflow`（LangGraph）、`DataIngestion`、各ノード\n",
    "- **Interface Adapters 層**: `OllamaAdapter`、`ChromaDBAdapter`、`RerankerAdapter`、`PDFLoaderAdapter`、`GradioHandler`\n",
    "- **Infrastructure 層**: `DIContainer`（依存性の組み立て・注入）\n",
    "\n",
    "#### ワークフロー\n",
    "1. **task_planning**: ユーザの質問を分析し、サブタスク（目的＋検索クエリ）を作成\n",
    "2. **doc_search**: ハイブリッド検索（BM25 + ベクトル）+ Reranker\n",
    "3. **summarize**: 検索結果を要約（Judge の入力コンテキスト削減）\n",
    "4. **judge**: 情報の十分性を判定。不足なら doc_search に戻る\n",
    "5. **generate_answer**: 最終回答をストリーミング生成"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**必要なライブラリをインストール**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "opentelemetry-exporter-otlp-proto-http 1.38.0 requires opentelemetry-exporter-otlp-proto-common==1.38.0, but you have opentelemetry-exporter-otlp-proto-common 1.39.1 which is incompatible.\n",
      "opentelemetry-exporter-otlp-proto-http 1.38.0 requires opentelemetry-proto==1.38.0, but you have opentelemetry-proto 1.39.1 which is incompatible.\n",
      "opentelemetry-exporter-otlp-proto-http 1.38.0 requires opentelemetry-sdk~=1.38.0, but you have opentelemetry-sdk 1.39.1 which is incompatible.\n",
      "opentelemetry-exporter-gcp-logging 1.11.0a0 requires opentelemetry-sdk<1.39.0,>=1.35.0, but you have opentelemetry-sdk 1.39.1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# Google Colab に必要なライブラリをインストールする。\n",
    "# NOTE: Colab では uv ではなく pip を使う。\n",
    "#       uv は依存解決の過程で numpy 等をアップグレードし、\n",
    "#       プリインストール済みの scipy 等を壊すため。\n",
    "\n",
    "# fmt: off\n",
    "pkgs = [\n",
    "    \"ollama\", \"langchain-ollama\",\n",
    "    \"langchain>=1.2.8\", \"langchain-core>=1.2.8\", \"langgraph>=1.0.7\",\n",
    "    \"langchain-text-splitters\",\n",
    "    \"markitdown[all]\", \"chromadb\", \"rank-bm25\",\n",
    "    \"spacy\", \"ginza\", \"ja-ginza\", \"sentence-transformers\",\n",
    "    \"pydantic>=2.12\", \"pydantic-settings>=2.13\",\n",
    "    \"gradio>=6.0\",\n",
    "]\n",
    "# fmt: on\n",
    "\n",
    "%pip install -U -q {\" \".join(pkgs)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ja_ginza loaded: ginza\n"
     ]
    }
   ],
   "source": [
    "# GiNZA (ja_ginza) は pip install ginza ja-ginza で自動的にインストールされるため、\n",
    "# spacy download は不要。以下で読み込みを確認する。\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"ja_ginza\")\n",
    "print(f\"ja_ginza loaded: {nlp.meta['name']}\")\n",
    "del nlp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Google Colab に Ollama をセットアップ**\n",
    "- 詳細は [01_connect_oss_llm.ipynb](01_connect_oss_llm.ipynb) を参照。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selecting previously unselected package zstd.\n",
      "(Reading database ... 121852 files and directories currently installed.)\n",
      "Preparing to unpack .../zstd_1.4.8+dfsg-3build1_amd64.deb ...\n",
      "Unpacking zstd (1.4.8+dfsg-3build1) ...\n",
      "Setting up zstd (1.4.8+dfsg-3build1) ...\n",
      "Processing triggers for man-db (2.10.2-1) ...\n",
      ">>> Installing ollama to /usr/local\n",
      ">>> Downloading ollama-linux-amd64.tar.zst\n",
      "######################################################################## 100.0%\n",
      ">>> Creating ollama user...\n",
      ">>> Adding ollama user to video group...\n",
      ">>> Adding current user to ollama group...\n",
      ">>> Creating ollama systemd service...\n",
      "\u001b[1m\u001b[31mWARNING:\u001b[m systemd is not running\n",
      "\u001b[1m\u001b[31mWARNING:\u001b[m Unable to detect NVIDIA/AMD GPU. Install lspci or lshw to automatically detect and install GPU dependencies.\n",
      ">>> The Ollama API is now available at 127.0.0.1:11434.\n",
      ">>> Install complete. Run \"ollama\" from the command line.\n",
      "success                                                     \n",
      "gpt-oss:20b: Done!\n",
      "  Model\n",
      "    architecture        gptoss    \n",
      "    parameters          20.9B     \n",
      "    context length      131072    \n",
      "    embedding length    2880      \n",
      "    quantization        MXFP4     \n",
      "\n",
      "  Capabilities\n",
      "    completion    \n",
      "    tools         \n",
      "    thinking      \n",
      "\n",
      "  Parameters\n",
      "    temperature    1    \n",
      "\n",
      "  License\n",
      "    Apache License               \n",
      "    Version 2.0, January 2004    \n",
      "    ...                          \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Ollama のインストール・起動・モデルのダウンロード\n",
    "import subprocess\n",
    "import time\n",
    "import ollama  # type: ignore\n",
    "\n",
    "!apt-get install -y -qq zstd\n",
    "!curl -fsSL https://ollama.com/install.sh | sh\n",
    "\n",
    "process = subprocess.Popen(\n",
    "    [\"ollama\", \"serve\"],\n",
    "    stdout=subprocess.DEVNULL,\n",
    "    stderr=subprocess.DEVNULL,\n",
    ")\n",
    "time.sleep(5)\n",
    "\n",
    "\n",
    "def ollama_pull(model: str) -> None:\n",
    "    \"\"\"Ollama モデルをダウンロードし、進捗をインライン表示する。\"\"\"\n",
    "    for progress in ollama.pull(model, stream=True):\n",
    "        status = progress.get(\"status\", \"\")\n",
    "        total = progress.get(\"total\") or 0\n",
    "        completed = progress.get(\"completed\") or 0\n",
    "        if total:\n",
    "            line = f\"{status}: {completed / total:.0%}\"\n",
    "        else:\n",
    "            line = status\n",
    "        print(f\"\\r{line:<60}\", end=\"\", flush=True)\n",
    "    print(f\"\\n{model}: Done!\")\n",
    "\n",
    "\n",
    "model_name = \"gpt-oss:20b\"\n",
    "ollama_pull(model_name)\n",
    "!ollama show {model_name}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Google Drive から `src/` をローカルにコピーし、Python パスに追加**\n",
    "\n",
    "Google Drive マウント経由の import はネットワークアクセスが毎回発生して遅いため、\n",
    "起動時に `src/` を Colab ローカル（`/content/src/`）にコピーして、そこから import する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n",
      "src copied: /content/drive/MyDrive/GEN_AI_RAG/src -> /content/src\n",
      "  domain/: True\n",
      "  usecases/: True\n",
      "  interfaces/: True\n",
      "  infrastructure/: True\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "from google.colab import drive  # type: ignore\n",
    "\n",
    "# Google Drive をマウント\n",
    "drive.mount(\"/content/drive\")\n",
    "\n",
    "# Google Drive の src/ を Colab ローカルにコピー（ネットワーク遅延の回避）\n",
    "drive_src = Path(\"/content/drive/MyDrive/GEN_AI_RAG/src\")\n",
    "local_src = Path(\"/content/src\")\n",
    "\n",
    "if local_src.exists():\n",
    "    shutil.rmtree(local_src)\n",
    "shutil.copytree(drive_src, local_src)\n",
    "\n",
    "# ローカルコピーを Python パスに追加\n",
    "if str(local_src) not in sys.path:\n",
    "    sys.path.insert(0, str(local_src))\n",
    "\n",
    "print(f\"src copied: {drive_src} -> {local_src}\")\n",
    "for pkg in [\"domain\", \"usecases\", \"interfaces\", \"infrastructure\"]:\n",
    "    print(f\"  {pkg}/: {(local_src / pkg / '__init__.py').exists()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**DIContainer で依存性を組み立て、Gradio UI を起動**\n",
    "\n",
    "Clean Architecture の Infrastructure 層にある `DIContainer` が、\n",
    "すべての依存性（LLM・VectorStore・Reranker・DataLoader）を生成し、\n",
    "コンストラクタインジェクションで `AgentWorkflow` と `GradioHandler` に注入する。\n",
    "\n",
    "`WorkflowConfig` のハイパーパラメータは、環境変数（`RAG_` プレフィックス）で上書き可能。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:104: UserWarning: \n",
      "Error while fetching `HF_TOKEN` secret value from your vault: 'Requesting secret HF_TOKEN timed out. Secrets can only be fetched when running from the Colab UI.'.\n",
      "You are not authenticated with the Hugging Face Hub in this notebook.\n",
      "If the error persists, please let us know by opening an issue on GitHub (https://github.com/huggingface/huggingface_hub/issues/new).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36646b5d24ba4e40aa24d328cc74a113",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/229 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92ae8285cef249539a8b4d59ee5c5b83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/205 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n",
      "WARNING:huggingface_hub.utils._http:Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5eb3ca362a24a6c9aec7533b67c0b6b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40bb1fc306a3492aabdbb7268c5359af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/54.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03df800fd26a4637bfcbb0188d88530a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2d6aa060b084789ba65b93bb4057a56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.26G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3309a23355b49df875116fcf2a39dd9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/152 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "afd7ee8c2fb0436e83e60cd29ca5f984",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "808302a4afbf4154861a04208f7ec0d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a38e5c8f5a148e58fa0a10c1472da4b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/1.83M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13cc7b9f15e74905b8f09afdc343a90b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/968 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a63f05d38f0540d991a488d937bf8a15",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/296 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc0d8dab29694cb5b91a21c9a34be074",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1efa00a2ef73442c94528620de98745d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.26G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a113776f9b84a2c9faa9b1d1650df1a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/156 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63ff785da3314ee98624cbf55e3d8ecd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3a89eed3aad4206a2718f5d320e2ef3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42a8fc35ed964ac3b50db7972fe46fd5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/1.83M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7c3212f317c42d8aea6fc92bce18bf6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/968 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22375a85fd454b0d9f4bb95397ef4bc0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
      "* Running on public URL: https://5f953e1c24e273f853.gradio.live\n",
      "\n",
      "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://5f953e1c24e273f853.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import logging\n",
    "\n",
    "from domain.config import WorkflowConfig\n",
    "from infrastructure.di_container import DIContainer\n",
    "\n",
    "# ログ設定\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s [%(levelname)s] %(name)s: %(message)s\",\n",
    ")\n",
    "\n",
    "# 設定を生成（デフォルト値または環境変数から読み込み）\n",
    "config = WorkflowConfig()\n",
    "\n",
    "# DI コンテナで全依存性を組み立て\n",
    "container = DIContainer(config=config)\n",
    "\n",
    "# Gradio UI を生成・起動\n",
    "ui = container.create_ui()\n",
    "demo = ui.launch()\n",
    "demo.launch(share=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closing server running on port: 7860\n"
     ]
    }
   ],
   "source": [
    "demo.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
