{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c894267f",
   "metadata": {},
   "source": [
    "### 03 WebサーチをするAIエージェント\n",
    "#### 03_01 ReAct\n",
    "- Google Colabに必要なライブラリをインストール\n",
    "- transformersを使ってLLMモデルをHugging Faceから読み込み\n",
    "- MCPサーバ(ddg-search)によるweb検索の実装\n",
    "- LangGraphによるReActの実装\n",
    "- 動作確認"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2072c39",
   "metadata": {},
   "source": [
    "**Google Colabに必要なライブラリをインストール**\n",
    "- 1行にまとめることで pip が全パッケージの依存関係を一括解決する。\n",
    "- 分割すると後勝ちで依存関係が壊れるリスクがある。\n",
    "  > （transformersとlangchain-huggingfaceで、huggingface-hubのバージョンが衝突する等）\n",
    "- NOTE: Colab では uv ではなく pip を使う。\n",
    "  > uv は依存解決の過程でnumpy 等をアップグレードし、プリインストール済みの scipy 等を壊すため。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23577a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Google Colabに必要なライブラリをインストールする。\n",
    "# 1行にまとめることで pip が全パッケージの依存関係を一括解決する。\n",
    "# 分割すると後勝ちで依存関係が壊れるリスクがある。\n",
    "# （transformersとlangchain-huggingfaceで、huggingface-hubのバージョンが衝突する等）\n",
    "# NOTE: Colab では uv ではなく pip を使う。uv は依存解決の過程で\n",
    "#       numpy 等をアップグレードし、プリインストール済みの scipy 等を壊すため。\n",
    "# NOTE: langchain 関連は 1.x 系に明示的に指定する。\n",
    "#       Colab プリインストールの 0.3.x が残ると langchain-mcp-adapters が動作しない。\n",
    "%pip install -U transformers accelerate bitsandbytes \\\n",
    "     \"langchain>=1.2.8\" \"langchain-core>=1.2.8\" \\\n",
    "     \"langchain-huggingface>=1.2.0\" \"langgraph>=1.0.7\" \\\n",
    "     \"langchain-mcp-adapters>=0.2.1\" duckduckgo-mcp-server mcp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b944db5",
   "metadata": {},
   "source": [
    "**transformersを使ってLLMモデルをHugging Faceから読み込み**\n",
    "\n",
    "##### LLM バックエンドの選定\n",
    "\n",
    "このリポジトリでは **transformers + bitsandbytes** を採用。\n",
    "- Colab GPU との相性が良い\n",
    "- 量子化などの細かい設定が可能\n",
    "- 非同期処理に対応\n",
    "\n",
    "**将来の本番環境向け:**\n",
    "- 非同期/スケーラビリティ重視が必要となる場合、Ollamaなどサーバ管理されたものを使用する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2aea36c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ffee6333a7dc4a51b8eb213a74ca0d08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "モデルのメモリ消費量: 5.94 GB\n"
     ]
    }
   ],
   "source": [
    "# transformersを使ってLLMモデルをHugging Faceから読み込みする。\n",
    "import torch  # type: ignore\n",
    "from transformers import (  # type: ignore\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,  # noqa: F401\n",
    "    pipeline,\n",
    ")  # type: ignore\n",
    "from langchain_huggingface import HuggingFacePipeline, ChatHuggingFace  # type: ignore\n",
    "\n",
    "model_name = \"ibm-granite/granite-4.0-h-micro\"  # Hugging Faceのモデルの名称\n",
    "\n",
    "# LLMモデルの量子化の設定\n",
    "# quantization_config = BitsAndBytesConfig(\n",
    "#     load_in_4bit=True,  # 4 ビットに量子化された形式で読み込むように指定\n",
    "#     bnb_4bit_quant_type=\"nf4\",  # 4 ビット量子化のデータ型として NF4 を指定\n",
    "#     bnb_4bit_use_double_quant=True,  # 二重量子化の指定\n",
    "#     bnb_4bit_compute_dtype=torch.bfloat16,  # 計算時のデータ型を指定\n",
    "# )\n",
    "\n",
    "# LLMに生成させる文章の計算条件\n",
    "generation_params = {\n",
    "    \"do_sample\": True,\n",
    "    \"temperature\": 0.8,\n",
    "    \"top_k\": 40,\n",
    "    \"top_p\": 0.9,\n",
    "    \"repetition_penalty\": 1.1,\n",
    "    \"max_new_tokens\": 512,\n",
    "}\n",
    "\n",
    "# LLMモデルの読み込み\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,  # Hugging Faceから読み込みするモデルの名称\n",
    "    # quantization_config=quantization_config,\n",
    "    dtype=torch.bfloat16,  # 読み込みするモデルのデータ型（transformers 4.x では torch_dtype）\n",
    "    device_map=\"auto\",  # 読み込んだモデルを CPU / GPUに自動で割り当てする指示\n",
    ")\n",
    "\n",
    "# transformersのパイプライン\n",
    "hf_pipeline = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    return_full_text=False,  # 入力プロンプトを出力に含めない\n",
    ")\n",
    "\n",
    "# LangChainのHuggingFaceパイプラインに接続\n",
    "llm = HuggingFacePipeline(pipeline=hf_pipeline, pipeline_kwargs=generation_params)\n",
    "\n",
    "# チャットモデルインターフェースとして接続（チャットテンプレート対応）\n",
    "chat_llm = ChatHuggingFace(llm=llm)\n",
    "\n",
    "# モデルのメモリ消費量（バイト）を取得し、GBに変換して表示\n",
    "memory_footprint_bytes = model.get_memory_footprint()\n",
    "memory_footprint_gb = memory_footprint_bytes / (1024 * 1024 * 1024)\n",
    "print(f\"モデルのメモリ消費量: {memory_footprint_gb:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04c196c8",
   "metadata": {},
   "source": [
    "**MCPサーバ(ddg-search)によるweb検索の実装**\n",
    "- langchain-mcp-adapters の MultiServerMCPClient を使用。\n",
    "- MCPサーバのツールが自動的に LangChain ツールに変換される（@tool の手動定義が不要）。\n",
    "- ReActエージェントが bind_tools() で認識し、自律的にツールを呼び出せる形式になる。\n",
    "\n",
    "**MCPサーバ設定の補足**\n",
    "\n",
    "github に記載されているMCP接続の設定は以下\n",
    "> \"mcpServers\": {\"ddg-search\": {\"command\": \"uvx\", \"args\": [\"duckduckgo-mcp-server\"]}}\n",
    "\n",
    "今回は、pip install済なので、command=\"duckduckgo-mcp-server\" で直接起動。uvx経由ではないので、args=[] としてよい。\n",
    "\n",
    "**Colab の stderr 問題の回避（Colab 特有。通常の Python 環境では不要）**\n",
    "\n",
    "Colab の stderr は fileno() 未対応のため、MCP の stdio_client が失敗する。\n",
    "stdio_client の関数シグネチャ `errlog=sys.stderr` は**インポート時に評価が確定**するため、\n",
    "後から sys.stderr を差し替えても効果がない（Python のデフォルト引数の仕様）。\n",
    "そこで、langchain_mcp_adapters.sessions 内の stdio_client 参照自体を、\n",
    "errlog のデフォルトを /dev/null に変更したラッパー関数に差し替えて回避する。\n",
    "\n",
    "1. /dev/null を書き込みモードで開き、`_devnull` として保持する（stderr の代替出力先）。\n",
    "2. 差し替え前のオリジナル `stdio_client` への参照を `_original_stdio_client` に退避する。\n",
    "3. ラッパー関数 `_patched_stdio_client` を定義する。errlog のデフォルト値だけを `_devnull` に変更し、内部ではオリジナルの `stdio_client` を呼び出す。\n",
    "4. `@contextlib.asynccontextmanager` と `yield` により async with 構文に対応し、MCP セッション完了まで errlog の差し替えが維持される。\n",
    "5. `_sessions.stdio_client` の参照先をラッパー関数に差し替えることで、MultiServerMCPClient が内部で呼ぶ stdio_client もパッチ済みになる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23f71da7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MCPサーバ（duckduckgo-mcp-server）に接続し、LangChainツールを自動取得\n",
    "# langchain-mcp-adapters が MCP ツールを LangChain 互換に自動変換するため、\n",
    "# @tool による手動ラップが不要\n",
    "import os\n",
    "import contextlib\n",
    "from langchain_mcp_adapters import sessions as _sessions  # type: ignore\n",
    "from langchain_mcp_adapters.client import MultiServerMCPClient  # type: ignore\n",
    "\n",
    "# --- Colab stderr 問題の回避パッチ（Colab 特有。通常環境では不要） ---\n",
    "# stdio_client(server, errlog=sys.stderr) のデフォルト値はインポート時に確定する。\n",
    "# Colab の stderr は fileno() 未対応のため、デフォルトのまま呼ぶと失敗する。\n",
    "# → errlog のデフォルトを /dev/null に差し替えたラッパーで上書きして回避する。\n",
    "# NOTE: _devnull はセッション中ずっと開いたままにする（閉じると書き込み先がなくなる）\n",
    "_devnull = open(os.devnull, \"w\")\n",
    "_original_stdio_client = _sessions.stdio_client\n",
    "\n",
    "\n",
    "@contextlib.asynccontextmanager\n",
    "async def _patched_stdio_client(server, errlog=_devnull):\n",
    "    async with _original_stdio_client(server, errlog=errlog) as result:\n",
    "        yield result\n",
    "\n",
    "\n",
    "_sessions.stdio_client = _patched_stdio_client\n",
    "\n",
    "# --- MCPサーバに接続 ---\n",
    "mcp_client = MultiServerMCPClient(\n",
    "    {\n",
    "        \"ddg-search\": {\n",
    "            \"transport\": \"stdio\",\n",
    "            \"command\": \"duckduckgo-mcp-server\",\n",
    "            \"args\": [],\n",
    "        }\n",
    "    }\n",
    ")\n",
    "\n",
    "# MCP ツールを LangChain ツールとして自動取得\n",
    "tools = await mcp_client.get_tools()\n",
    "\n",
    "print(\"=== ReActエージェント用ツール ===\")\n",
    "for t in tools:\n",
    "    print(f\"  - {t.name}: {t.description}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6817f84c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbb67a96",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba3a4c0a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b3584f48",
   "metadata": {},
   "source": [
    "**LangGraphによるLLMのメモリ管理**\n",
    "- chat_nodeで、前のセルで作成したLLMモデルを呼び出した際の処理を設定。\n",
    "  > 今回は人間のプロンプトをLLMに入力して、LLMから応答を取得する処理。\n",
    "- LangGraphのグラフを構築し、ノードとエッジを設定し、グラフをcompileする。\n",
    "  > 今回は単純だが、各処理をつなげて自動化させる場合に効果を発揮する。\n",
    "- InMemorySaverを使用して、LLMとの会話をメモリに保存させる。\n",
    "  > LLMとのチャット履歴が保持されるようになる。\n",
    "- 作成したグラフを図示する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9289c3f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# LangGraph によるメモリ付きチャットグラフの構築\n",
    "# InMemorySaver を使って、スレッド（thread_id）ごとに会話履歴を保持する\n",
    "from langgraph.graph import StateGraph, MessagesState, START, END  # type: ignore\n",
    "from langgraph.checkpoint.memory import InMemorySaver  # type: ignore\n",
    "from IPython.display import Image, display\n",
    "\n",
    "\n",
    "# チャットノード：現在の会話履歴を LLM に送信し、返答を生成\n",
    "def chat_node(state: MessagesState):\n",
    "    response = chat_llm.invoke(state[\"messages\"])\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "\n",
    "# グラフの構築\n",
    "graph = StateGraph(MessagesState)\n",
    "graph.add_node(\"chat\", chat_node)\n",
    "graph.add_edge(START, \"chat\")\n",
    "graph.add_edge(\"chat\", END)\n",
    "\n",
    "# InMemorySaver：スレッドごとに会話履歴をインメモリで保持\n",
    "memory = InMemorySaver()\n",
    "app = graph.compile(checkpointer=memory)\n",
    "\n",
    "# 構築したグラフを図示\n",
    "display(Image(app.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be7046c4",
   "metadata": {},
   "source": [
    "**LangGraph/LangChainによるチャット**\n",
    "- SystemMessageでシステムプロンプトを、HumanMessageでユーザのプロンプトを設定する。\n",
    "- configで指定したthread_idごとに会話履歴が保持される。\n",
    "- 2回目の質問で前の会話を踏まえた回答が返れば、メモリが正常に動作している証拠。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "30341046",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 1回目の返答 ===\n",
      "日本で一番高い山は、富士山です。\n",
      "\n",
      "富士山は、山梨県と静岡県にまたがっています。標高は3,776.24mです。\n",
      "\n",
      "=== 2回目の返答 ===\n",
      "あなたの名前はジョンです。\n",
      "\n",
      "富士山は、標高3,776.24mです。\n"
     ]
    }
   ],
   "source": [
    "# マルチターン会話のテスト\n",
    "# 同じ thread_id でLLMを複数回呼び出すと、会話履歴が自動的に保持される\n",
    "from langchain_core.messages import HumanMessage, SystemMessage  # type: ignore\n",
    "\n",
    "config = {\"configurable\": {\"thread_id\": \"thread_1\"}}\n",
    "\n",
    "# 1回目の質問（システムプロンプト付き）\n",
    "response1 = app.invoke(\n",
    "    {\n",
    "        \"messages\": [\n",
    "            SystemMessage(content=\"日本語で回答してください。\"),\n",
    "            HumanMessage(content=\"私はジョンです。日本で一番高い山は？\"),\n",
    "        ]\n",
    "    },\n",
    "    config=config,\n",
    ")\n",
    "print(\"=== 1回目の返答 ===\")\n",
    "print(response1[\"messages\"][-1].content)\n",
    "\n",
    "# 2回目の質問（前の会話を踏まえて回答できれば、メモリが動作している証拠）\n",
    "response2 = app.invoke(\n",
    "    {\n",
    "        \"messages\": [\n",
    "            HumanMessage(\n",
    "                content=\"私の名前は何ですか？先ほど答えた山の高さは何メートルですか？\"\n",
    "            ),\n",
    "        ]\n",
    "    },\n",
    "    config=config,\n",
    ")\n",
    "print(\"\\n=== 2回目の返答 ===\")\n",
    "print(response2[\"messages\"][-1].content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d97caed",
   "metadata": {},
   "source": [
    "**MCPサーバ（duckduckgo-mcp-server）によるWeb検索**\n",
    "- MCPサーバ（duckduckgo-mcp-server）を stdio 経由で起動し、検索ツールを取得する。\n",
    "  > AIに自律的に検索ツールを使わせるのではなく、今回（notebook 02）はユーザがMCPサーバを実行。\n",
    "- 検索結果をコンテキストとして LLM に渡し、回答を生成させる。\n",
    "  > gemma-3-1b はツールコール非対応のため、検索→LLM回答の2段構成とする。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "510b73da",
   "metadata": {},
   "source": [
    "**MCPサーバ設定の補足**\n",
    "\n",
    "github に記載されているMCP接続の設定は以下\n",
    "> \"mcpServers\": {\"ddg-search\": {\"command\": \"uvx\", \"args\": [\"duckduckgo-mcp-server\"]}}\n",
    "\n",
    "今回は、pip install済なので、command=\"duckduckgo-mcp-server\" で直接起動。uvx経由ではないので、args=[] としてよい。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d145d2e0",
   "metadata": {},
   "source": [
    "**stdio_client接続の補足**\n",
    "\n",
    "async with stdio_client(server_params) as (read, write):を、次に修正している。\n",
    "  > async with stdio_client(server_params, errlog=open(os.devnull, \"w\")) as (read, write):\n",
    "\n",
    "これはColab環境特有の処置で、Colabの内部構造が標準的なstderrに対応していないため。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3u7ictw8s0m",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 利用可能なツール ===\n",
      "  - search: \n",
      "    Search DuckDuckGo and return formatted results.\n",
      "\n",
      "    Args:\n",
      "        query: The search query string\n",
      "        max_results: Maximum number of results to return (default: 10)\n",
      "        ctx: MCP context for logging\n",
      "    \n",
      "  - fetch_content: \n",
      "    Fetch and parse content from a webpage URL.\n",
      "\n",
      "    Args:\n",
      "        url: The webpage URL to fetch content from\n",
      "        ctx: MCP context for logging\n",
      "    \n",
      "\n",
      "=== 検索結果 ===\n",
      "Found 3 search results:\n",
      "\n",
      "1. 人口推計（2025年（令和7年）8月確定値、2026年（令和8年）1月概算値） （2026年1月20日公表）\n",
      "   URL: https://www.stat.go.jp/data/jinsui/new.htm\n",
      "   Summary: 2026年（令和8年）1月報 （2025年（令和7年）8月確定値、2026年（令和8年）1月概算値）（PDF：346KB） 統計表 統計表 （政府統計の総合窓口「e-Stat」 掲載の統計表） ※ その他の推計結果 各月1日現在人口 「全国：年齢（5歳階級）、男女別人口」 各年10月1日現在人口 「全国：年齢（各歳）、男女別 ...\n",
      "\n",
      "2. 我が国の人口について｜厚生労働省 - mhlw.go.jp\n",
      "   URL: https://www.mhlw.go.jp/stf/newpage_21481.html\n",
      "   Summary: 日本の人口は近年減少局面を迎えており、2070年には総人口が9,000万人を割り込み、高齢化率は39％の水準になると推計されています。 また、団塊の世代の方々が全て75歳となる2025年には、75歳以上の人口が全人口の約18％となり、2040年には65歳以上の人口が全人口の約35％となると推計されてい ...\n",
      "\n",
      "3. 2025年1月1日人口、日本人91万人減 外国人は過去最多でリゾート地でも増 総務省 - 産経ニュース\n",
      "   URL: https://www.sankei.com/article/20250806-IWQWQQFT3NADBLQB6SKV5NE2TM/\n",
      "   Summary: 総務省は6日、2025年1月1日時点の住民基本台帳に基づく全国の人口について、55万人減の1億2433万人だったと発表した。日本人は91万人減の1億2065万 ...\n",
      "\n",
      "\n",
      "=== LLMの回答 ===\n",
      "2025年日本人の総人口は、2026年1月概算値で2026年1月20日公表された統計表によると、2026年（令和8年）1月には約2億233万人と推計されています。\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# MCPサーバ（duckduckgo-mcp-server）に接続して検索し、結果をLLMに渡す\n",
    "# Colab環境ではasyncioのインポートなしで非同期処理（async/await）が使用可能\n",
    "# 通常のPython環境では、asyncioのインポートが必要\n",
    "import os\n",
    "from mcp import ClientSession, StdioServerParameters  # type: ignore\n",
    "from mcp.client.stdio import stdio_client  # type: ignore\n",
    "from langchain_core.messages import HumanMessage, SystemMessage  # type: ignore\n",
    "\n",
    "# MCPサーバへの接続パラメータ（stdio 経由で起動）\n",
    "server_params = StdioServerParameters(\n",
    "    command=\"duckduckgo-mcp-server\",\n",
    "    args=[],\n",
    ")\n",
    "\n",
    "query = \"2025年 日本の総人口\"\n",
    "\n",
    "# MCPサーバに接続し、検索ツールを取得して実行\n",
    "# NOTE: Colab の stderr は fileno() 未対応のため、errlog をファイルに迂回する\n",
    "async with stdio_client(server_params, errlog=open(os.devnull, \"w\")) as (read, write):\n",
    "    async with ClientSession(read, write) as session:\n",
    "        await session.initialize()\n",
    "\n",
    "        # MCPサーバが提供するツール一覧を表示\n",
    "        tools = await session.list_tools()\n",
    "        print(\"=== 利用可能なツール ===\")\n",
    "        for t in tools.tools:\n",
    "            print(f\"  - {t.name}: {t.description}\")\n",
    "\n",
    "        # 検索ツールを実行（max_results で検索結果数を制限）\n",
    "        result = await session.call_tool(\"search\", {\"query\": query, \"max_results\": 3})\n",
    "\n",
    "# 検索結果を取得\n",
    "search_results = result.content[0].text\n",
    "print(\"\\n=== 検索結果 ===\")\n",
    "print(search_results)\n",
    "\n",
    "# 検索結果をコンテキストとしてLLMに質問\n",
    "config = {\"configurable\": {\"thread_id\": \"thread_search\"}}\n",
    "response = app.invoke(\n",
    "    {\n",
    "        \"messages\": [\n",
    "            SystemMessage(\n",
    "                content=\"以下の検索結果をもとに、ユーザの質問に日本語で簡潔に回答してください。\"\n",
    "            ),\n",
    "            HumanMessage(content=f\"質問: {query}\\n\\n検索結果:\\n{search_results}\"),\n",
    "        ]\n",
    "    },\n",
    "    config=config,\n",
    ")\n",
    "print(\"\\n=== LLMの回答 ===\")\n",
    "print(response[\"messages\"][-1].content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
