{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 04 Embedding model（埋め込みモデル）と RAG（検索拡張生成）\n",
    "gpt-oss:20b＋MCPサーバ構成のため、**Colab GPU は L4 を使用すること。**\n",
    "- 必要なライブラリをインストール\n",
    "- Google Colab に Ollama をセットアップ\n",
    "  - LLM モデルは gpt-oss:20b を使用（Ollama）\n",
    "  - Embedding モデルは bge-m3 を使用（Ollama）\n",
    "  - Reranker モデルは BAAI/bge-reranker-v2-m3 を使用（Sentence Transformers）\n",
    "- data フォルダからデータを読み込み\n",
    "> [井澤克彦, 市川信一郎, 高速回転ホイール: 高速回転ホイール開発を通しての知見, 宇宙航空研究開発機構研究開発報告, 2008](https://jaxa.repo.nii.ac.jp/records/2149)\n",
    "- データの前処理\n",
    "  - markdown に変換（MarkItDown を使用）\n",
    "  - Unicode正規化 (NFKC)\n",
    "  - チャンク分割\n",
    "    - LangChain の SpacyTextSplitter を使用\n",
    "    - spaCy の日本語モデルは、ja_ginza を使用\n",
    "- ベクトルデータベースの構築（ChromaDB, インメモリ）\n",
    "- 検索機能の実装と単体動作確認\n",
    "　- キーワード検索 @ BM25（spaCyで形態素解析の前処理が必要）\n",
    "  - Embedding model によるセマンティック検索\n",
    "  - ハイブリッド検索\n",
    "  - Reranker による再順位付け\n",
    "- 検索機能をLLM の tool として定義\n",
    "- 動作確認"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**必要なライブラリをインストール**\n",
    "- 1行にまとめることで pip が全パッケージの依存関係を一括解決する。\n",
    "- 分割すると後勝ちで依存関係が壊れるリスクがある。\n",
    "- NOTE: Colab では uv ではなく pip を使う。\n",
    "> uv は依存解決の過程で numpy 等をアップグレードし、プリインストール済みの scipy 等を壊すため。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Google Colab に必要なライブラリをインストールする。\n",
    "# 1行にまとめることで pip が全パッケージの依存関係を一括解決する。\n",
    "# NOTE: Colab では uv ではなく pip を使う。uv は依存解決の過程で\n",
    "#       numpy 等をアップグレードし、プリインストール済みの scipy 等を壊すため。\n",
    "# NOTE: langchain 関連は 1.x 系に明示的に指定する。\n",
    "#       Colab プリインストールの 0.3.x が残ると langchain-mcp-adapters が動作しない。\n",
    "%pip install -U ollama langchain-ollama \\\n",
    "     \"langchain>=1.2.8\" \"langchain-core>=1.2.8\" \\\n",
    "     \"langgraph>=1.0.7\" \\\n",
    "     markitdown chromadb \\\n",
    "     \"langchain-text-splitters>=0.3\" \\\n",
    "     spacy ginza ja-ginza \\\n",
    "     rank-bm25 sentence-transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Google Colab に Ollama をセットアップ**\n",
    "- Ollama のインストール・起動・モデルのダウンロードを行う。\n",
    "- 詳細は [01_connect_oss_llm.ipynb](01_connect_oss_llm.ipynb) を参照。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ollama のインストール・起動・モデルのダウンロード\n",
    "# 詳細は 01_connect_oss_llm.ipynb を参照\n",
    "import subprocess\n",
    "import time\n",
    "import ollama  # type: ignore\n",
    "\n",
    "!apt-get install -y -qq zstd\n",
    "!curl -fsSL https://ollama.com/install.sh | sh\n",
    "\n",
    "process = subprocess.Popen(\n",
    "    [\"ollama\", \"serve\"],\n",
    "    stdout=subprocess.DEVNULL,\n",
    "    stderr=subprocess.DEVNULL,\n",
    ")\n",
    "time.sleep(5)\n",
    "\n",
    "\n",
    "def ollama_pull(model: str) -> None:\n",
    "    \"\"\"Ollama モデルをダウンロードし、進捗をインライン表示する。\n",
    "\n",
    "    NOTE: ollama pull のプログレスバーは Colab で文字化けするため、\n",
    "          Python API 経由でステータスのみ表示する。\n",
    "    \"\"\"\n",
    "    for progress in ollama.pull(model, stream=True):\n",
    "        status = progress.get(\"status\", \"\")\n",
    "        total = progress.get(\"total\") or 0\n",
    "        completed = progress.get(\"completed\") or 0\n",
    "        if total:\n",
    "            line = f\"{status}: {completed / total:.0%}\"\n",
    "        else:\n",
    "            line = status\n",
    "        print(f\"\\r{line:<60}\", end=\"\", flush=True)\n",
    "    print(f\"\\n{model}: Done!\")\n",
    "\n",
    "\n",
    "# AI エージェントにはツールコール対応モデルが必要。\n",
    "model_name = \"gpt-oss:20b\"\n",
    "ollama_pull(model_name)\n",
    "!ollama show {model_name}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ChatOllama で LLM に接続**\n",
    "- 詳細は [01_connect_oss_llm.ipynb](01_connect_oss_llm.ipynb) を参照。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ChatOllama で LLM に接続する。\n",
    "from langchain_ollama import ChatOllama  # type: ignore\n",
    "\n",
    "llm = ChatOllama(\n",
    "    model=\"gpt-oss:20b\",\n",
    "    num_ctx=16384,\n",
    "    num_predict=-1,\n",
    "    temperature=0.8,\n",
    "    top_k=40,\n",
    "    top_p=0.9,\n",
    "    repeat_penalty=1.1,\n",
    "    reasoning=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Embedding モデル（bge-m3）をダウンロード**\n",
    "- bge-m3 は多言語対応の Embedding モデル。日本語にも対応。\n",
    "- Ollama 経由で利用する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embedding モデル (bge-m3) をダウンロードする。\n",
    "embedding_model_name = \"bge-m3\"\n",
    "ollama_pull(embedding_model_name)\n",
    "!ollama show {embedding_model_name}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**OllamaEmbeddings と Reranker モデルのセットアップ**\n",
    "- OllamaEmbeddings: LangChain 経由で bge-m3 を Embedding に使用。\n",
    "- Reranker: Sentence Transformers の CrossEncoder で BAAI/bge-reranker-v2-m3 を使用。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OllamaEmbeddings: LangChain 経由で bge-m3 を Embedding に使用する。\n",
    "from langchain_ollama import OllamaEmbeddings  # type: ignore\n",
    "\n",
    "embeddings = OllamaEmbeddings(model=embedding_model_name)\n",
    "\n",
    "# 動作確認: 短いテキストを埋め込んでベクトル次元を確認する。\n",
    "test_vec = embeddings.embed_query(\"テスト文です\")\n",
    "print(f\"Embedding dim: {len(test_vec)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reranker: Sentence Transformers の CrossEncoder を使用する。\n",
    "# 初回実行時にモデルがダウンロードされる。\n",
    "from sentence_transformers import CrossEncoder  # type: ignore\n",
    "\n",
    "reranker = CrossEncoder(\"BAAI/bge-reranker-v2-m3\")\n",
    "print(\"Reranker model loaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**data フォルダからデータを読み込み**\n",
    "\n",
    "> [井澤克彦, 市川信一郎, 高速回転ホイール: 高速回転ホイール開発を通しての知見, 宇宙航空研究開発機構研究開発報告, 2008](https://jaxa.repo.nii.ac.jp/records/2149)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data フォルダから PDF を読み込み、MarkItDown で markdown に変換する。\n",
    "from pathlib import Path\n",
    "from markitdown import MarkItDown  # type: ignore\n",
    "\n",
    "data_dir = Path(\"data\")\n",
    "pdf_path = data_dir / \"高速回転ホイール_高速回転ホイール開発を通しての知見.pdf\"\n",
    "\n",
    "md = MarkItDown()\n",
    "result = md.convert(str(pdf_path))\n",
    "raw_text = result.text_content\n",
    "\n",
    "print(f\"文字数: {len(raw_text)}\")\n",
    "print(\"--- 先頭 500 文字 ---\")\n",
    "print(raw_text[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**データの前処理**\n",
    "- Unicode 正規化 (NFKC)\n",
    "- チャンク分割（SpacyTextSplitter + ja_ginza）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unicode 正規化 (NFKC) を適用する。\n",
    "# 全角英数→半角、半角カナ→全角 などを統一する。\n",
    "import unicodedata\n",
    "\n",
    "text = unicodedata.normalize(\"NFKC\", raw_text)\n",
    "print(f\"正規化後の文字数: {len(text)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SpacyTextSplitter でチャンク分割する。\n",
    "# spaCy の日本語モデル ja_ginza を使用し、文境界を考慮して分割する。\n",
    "from langchain_text_splitters import SpacyTextSplitter  # type: ignore\n",
    "\n",
    "CHUNK_SIZE = 1000\n",
    "CHUNK_OVERLAP = 200\n",
    "\n",
    "text_splitter = SpacyTextSplitter(\n",
    "    pipeline=\"ja_ginza\",\n",
    "    chunk_size=CHUNK_SIZE,\n",
    "    chunk_overlap=CHUNK_OVERLAP,\n",
    ")\n",
    "\n",
    "chunks = text_splitter.split_text(text)\n",
    "\n",
    "print(f\"チャンク数: {len(chunks)}\")\n",
    "for i, chunk in enumerate(chunks[:3]):\n",
    "    print(f\"\\n--- Chunk {i} ({len(chunk)} chars) ---\")\n",
    "    print(chunk[:200] + \"...\" if len(chunk) > 200 else chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ベクトルデータベースの構築（ChromaDB, インメモリ）**\n",
    "- チャンクを Embedding してベクトルデータベースに格納する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ChromaDB にチャンクを格納する（インメモリ）。\n",
    "import chromadb  # type: ignore\n",
    "\n",
    "chroma_client = chromadb.Client()\n",
    "\n",
    "collection = chroma_client.create_collection(\n",
    "    name=\"jaxa_wheel\",\n",
    "    metadata={\"hnsw:space\": \"cosine\"},\n",
    ")\n",
    "\n",
    "# チャンクを Embedding してデータベースに追加する。\n",
    "chunk_embeddings = embeddings.embed_documents(chunks)\n",
    "\n",
    "collection.add(\n",
    "    ids=[f\"chunk_{i}\" for i in range(len(chunks))],\n",
    "    documents=chunks,\n",
    "    embeddings=chunk_embeddings,\n",
    ")\n",
    "\n",
    "print(f\"ChromaDB に {collection.count()} 件のチャンクを格納しました。\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**検索機能の実装**\n",
    "- キーワード検索 @ BM25（spaCy で形態素解析の前処理が必要）\n",
    "- Embedding model によるセマンティック検索\n",
    "- ハイブリッド検索\n",
    "- Reranker による再順位付け"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BM25 用の前処理: spaCy (ja_ginza) で形態素解析してトークン化する。\n",
    "import spacy  # type: ignore\n",
    "from rank_bm25 import BM25Okapi  # type: ignore\n",
    "\n",
    "nlp = spacy.load(\"ja_ginza\")\n",
    "\n",
    "TOP_K = 5\n",
    "\n",
    "\n",
    "def tokenize(text: str) -> list[str]:\n",
    "    \"\"\"spaCy で形態素解析し、名詞・動詞・形容詞のレンマを返す。\"\"\"\n",
    "    doc = nlp(text)\n",
    "    return [\n",
    "        token.lemma_\n",
    "        for token in doc\n",
    "        if token.pos_ in (\"NOUN\", \"VERB\", \"ADJ\", \"PROPN\") and len(token.lemma_) > 1\n",
    "    ]\n",
    "\n",
    "\n",
    "# チャンクをトークン化して BM25 インデックスを構築する。\n",
    "tokenized_chunks = [tokenize(chunk) for chunk in chunks]\n",
    "bm25 = BM25Okapi(tokenized_chunks)\n",
    "\n",
    "print(f\"BM25 インデックス構築完了: {len(tokenized_chunks)} 件\")\n",
    "print(f\"トークン例（先頭チャンク）: {tokenized_chunks[0][:10]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "RETRIEVAL_TOP_K = 20  # 第1段検索（BM25 / セマンティック / ハイブリッド）の抽出数\n",
    "RERANK_TOP_K = 5  # 第2段検索（Reranker）の抽出数\n",
    "\n",
    "\n",
    "def search_bm25(query: str, top_k: int = RETRIEVAL_TOP_K) -> list[dict]:\n",
    "    \"\"\"BM25 によるキーワード検索を行う。\"\"\"\n",
    "    tokenized_query = tokenize(query)\n",
    "    scores = bm25.get_scores(tokenized_query)\n",
    "    top_indices = np.argsort(scores)[::-1][:top_k]\n",
    "    return [\n",
    "        {\n",
    "            \"rank\": rank + 1,\n",
    "            \"chunk_id\": int(idx),\n",
    "            \"score\": float(scores[idx]),\n",
    "            \"text\": chunks[idx],\n",
    "        }\n",
    "        for rank, idx in enumerate(top_indices)\n",
    "        if scores[idx] > 0\n",
    "    ]\n",
    "\n",
    "\n",
    "def search_semantic(query: str, top_k: int = RETRIEVAL_TOP_K) -> list[dict]:\n",
    "    \"\"\"Embedding model によるセマンティック検索を行う。\"\"\"\n",
    "    query_embedding = embeddings.embed_query(query)\n",
    "    results = collection.query(\n",
    "        query_embeddings=[query_embedding],\n",
    "        n_results=top_k,\n",
    "    )\n",
    "    return [\n",
    "        {\n",
    "            \"rank\": rank + 1,\n",
    "            \"chunk_id\": int(doc_id.split(\"_\")[1]),\n",
    "            \"score\": 1.0 - dist,\n",
    "            \"text\": doc,\n",
    "        }\n",
    "        for rank, (doc_id, doc, dist) in enumerate(\n",
    "            zip(results[\"ids\"][0], results[\"documents\"][0], results[\"distances\"][0])\n",
    "        )\n",
    "    ]\n",
    "\n",
    "\n",
    "def search_hybrid(\n",
    "    query: str, top_k: int = RETRIEVAL_TOP_K, bm25_weight: float = 0.3\n",
    ") -> list[dict]:\n",
    "    \"\"\"BM25 とセマンティック検索のハイブリッド検索を行う。\n",
    "\n",
    "    Reciprocal Rank Fusion (RRF) でスコアを統合する。\n",
    "    \"\"\"\n",
    "    k = 60  # RRF のハイパーパラメータ\n",
    "\n",
    "    bm25_results = search_bm25(query, top_k=top_k)\n",
    "    semantic_results = search_semantic(query, top_k=top_k)\n",
    "\n",
    "    # RRF スコアを計算する。\n",
    "    scores: dict[int, float] = {}\n",
    "    texts: dict[int, str] = {}\n",
    "\n",
    "    for r in bm25_results:\n",
    "        cid = r[\"chunk_id\"]\n",
    "        scores[cid] = scores.get(cid, 0) + bm25_weight / (k + r[\"rank\"])\n",
    "        texts[cid] = r[\"text\"]\n",
    "\n",
    "    semantic_weight = 1.0 - bm25_weight\n",
    "    for r in semantic_results:\n",
    "        cid = r[\"chunk_id\"]\n",
    "        scores[cid] = scores.get(cid, 0) + semantic_weight / (k + r[\"rank\"])\n",
    "        texts[cid] = r[\"text\"]\n",
    "\n",
    "    sorted_ids = sorted(scores, key=lambda cid: scores[cid], reverse=True)[:top_k]\n",
    "    return [\n",
    "        {\"rank\": rank + 1, \"chunk_id\": cid, \"score\": scores[cid], \"text\": texts[cid]}\n",
    "        for rank, cid in enumerate(sorted_ids)\n",
    "    ]\n",
    "\n",
    "\n",
    "def rerank(query: str, results: list[dict], top_k: int = RERANK_TOP_K) -> list[dict]:\n",
    "    \"\"\"Reranker (CrossEncoder) で検索結果を再順位付けする。\"\"\"\n",
    "    if not results:\n",
    "        return []\n",
    "    pairs = [(query, r[\"text\"]) for r in results]\n",
    "    scores = reranker.predict(pairs)\n",
    "    ranked_indices = np.argsort(scores)[::-1][:top_k]\n",
    "    return [\n",
    "        {\n",
    "            \"rank\": rank + 1,\n",
    "            \"chunk_id\": results[idx][\"chunk_id\"],\n",
    "            \"score\": float(scores[idx]),\n",
    "            \"text\": results[idx][\"text\"],\n",
    "        }\n",
    "        for rank, idx in enumerate(ranked_indices)\n",
    "    ]\n",
    "\n",
    "\n",
    "print(\"検索関数を定義しました: search_bm25, search_semantic, search_hybrid, rerank\")\n",
    "print(f\"  第1段検索 top_k: {RETRIEVAL_TOP_K}, 第2段 Rerank top_k: {RERANK_TOP_K}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**検索機能の単体動作確認**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 検索機能の単体動作確認\n",
    "test_query = \"高速回転ホイールの寿命試験\"\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(f\"テストクエリ: {test_query}\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# 1. BM25 キーワード検索\n",
    "print(\"\\n--- BM25 キーワード検索 ---\")\n",
    "bm25_results = search_bm25(test_query)\n",
    "for r in bm25_results[:3]:\n",
    "    print(f\"  Rank {r['rank']} (score={r['score']:.4f}, chunk_id={r['chunk_id']})\")\n",
    "    print(f\"    {r['text'][:100]}...\")\n",
    "\n",
    "# 2. セマンティック検索\n",
    "print(\"\\n--- セマンティック検索 ---\")\n",
    "sem_results = search_semantic(test_query)\n",
    "for r in sem_results[:3]:\n",
    "    print(f\"  Rank {r['rank']} (score={r['score']:.4f}, chunk_id={r['chunk_id']})\")\n",
    "    print(f\"    {r['text'][:100]}...\")\n",
    "\n",
    "# 3. ハイブリッド検索\n",
    "print(\"\\n--- ハイブリッド検索 ---\")\n",
    "hybrid_results = search_hybrid(test_query)\n",
    "for r in hybrid_results[:3]:\n",
    "    print(f\"  Rank {r['rank']} (score={r['score']:.6f}, chunk_id={r['chunk_id']})\")\n",
    "    print(f\"    {r['text'][:100]}...\")\n",
    "\n",
    "# 4. Reranker による再順位付け（ハイブリッド検索結果を入力）\n",
    "print(\"\\n--- Reranker 再順位付け ---\")\n",
    "reranked_results = rerank(test_query, hybrid_results)\n",
    "for r in reranked_results:\n",
    "    print(f\"  Rank {r['rank']} (score={r['score']:.4f}, chunk_id={r['chunk_id']})\")\n",
    "    print(f\"    {r['text'][:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**検索機能を LLM の tool として定義**\n",
    "- ハイブリッド検索 + Reranker を LangChain の `@tool` デコレータで定義する。\n",
    "- LLM がユーザの質問に対して自動的に検索ツールを呼び出し、RAG を実現する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 検索機能を LLM の tool として定義する。\n",
    "from langchain_core.tools import tool  # type: ignore\n",
    "\n",
    "\n",
    "@tool\n",
    "def search_document(query: str) -> str:\n",
    "    \"\"\"高速回転ホイールに関する技術文書を検索します。\n",
    "    ハイブリッド検索（BM25 + セマンティック検索）と Reranker を組み合わせて、\n",
    "    クエリに最も関連するテキストを返します。\n",
    "\n",
    "    Args:\n",
    "        query: 検索クエリ（日本語）\n",
    "    \"\"\"\n",
    "    hybrid_results = search_hybrid(query)\n",
    "    reranked = rerank(query, hybrid_results)\n",
    "\n",
    "    if not reranked:\n",
    "        return \"検索結果が見つかりませんでした。\"\n",
    "\n",
    "    passages = []\n",
    "    for r in reranked:\n",
    "        passages.append(\n",
    "            f\"[チャンク {r['chunk_id']}] (スコア: {r['score']:.4f})\\n{r['text']}\"\n",
    "        )\n",
    "    return \"\\n\\n---\\n\\n\".join(passages)\n",
    "\n",
    "\n",
    "tools = [search_document]\n",
    "\n",
    "print(\"=== RAG Tools ===\")\n",
    "for t in tools:\n",
    "    print(f\"  - {t.name}: {t.description[:80]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**動作確認**\n",
    "- ReAct エージェントで検索ツールを使った RAG の動作を確認する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ReAct エージェントを構築して RAG の動作確認を行う。\n",
    "from langchain.agents import create_agent  # type: ignore\n",
    "from langgraph.checkpoint.memory import InMemorySaver  # type: ignore\n",
    "\n",
    "memory = InMemorySaver()\n",
    "\n",
    "system_prompt = \"\"\"\n",
    "あなたは高速回転ホイールに関する技術文書の専門家AIアシスタントです。\n",
    "ユーザの質問に日本語で回答してください。\n",
    "必ず search_document ツールで文書を検索し、検索結果に基づいて回答してください。\n",
    "検索結果に含まれない内容は回答しないでください。\n",
    "\n",
    "回答の最後に、以下の形式で結論をまとめてください。\n",
    "\n",
    "# 結論\n",
    "- ユーザの質問: （質問内容）\n",
    "- 回答: （簡潔な回答）\n",
    "\"\"\"\n",
    "\n",
    "agent = create_agent(\n",
    "    model=llm,\n",
    "    tools=tools,\n",
    "    checkpointer=memory,\n",
    "    system_prompt=system_prompt,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 動作確認: エージェントに質問する。\n",
    "config = {\"configurable\": {\"thread_id\": \"rag-test\"}}\n",
    "\n",
    "response = await agent.ainvoke(\n",
    "    {\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": \"高速回転ホイールの寿命試験ではどのような結果が得られましたか？\",\n",
    "            }\n",
    "        ]\n",
    "    },\n",
    "    config=config,\n",
    ")\n",
    "\n",
    "print(\"=== RAG Agent Result ===\\n\")\n",
    "for msg in response[\"messages\"]:\n",
    "    if isinstance(msg.content, list):\n",
    "        msg.content = \"\\n\".join(\n",
    "            item[\"text\"] for item in msg.content if item.get(\"type\") == \"text\"\n",
    "        )\n",
    "    msg.pretty_print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
