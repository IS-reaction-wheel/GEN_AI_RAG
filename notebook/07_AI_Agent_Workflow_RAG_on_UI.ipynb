{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 07 AI Agent Workflow + RAG on Gradio UI\n",
    "gpt-oss:20bを使用する構成のため、**Colab GPU は L4 を使用すること。**\n",
    "\n",
    "#### コア機能と実装の概要\n",
    "- 必要なライブラリをインストール\n",
    "- Google Colab に Ollama をセットアップ\n",
    "  - LLM モデルは gpt-oss:20b を使用（Ollama）\n",
    "  - Embedding モデルは ruri-v3-310m を使用（Sentence Transformers）\n",
    "  - Reranker モデルは cl-nagoya/ruri-v3-reranker-310m を使用（Sentence Transformers）\n",
    "- JAXA（宇宙航空研究開発機構）のリポジトリからデータをダウンロードして読み込み\n",
    "> [井澤克彦, 市川信一郎, 高速回転ホイール: 高速回転ホイール開発を通しての知見, 宇宙航空研究開発機構研究開発報告, 2008](https://jaxa.repo.nii.ac.jp/records/2149)\n",
    "- データの前処理\n",
    "  - markdown に変換（MarkItDown を使用）\n",
    "  - Unicode正規化 (NFKC), 1文字行ブロックの除去, 空行圧縮\n",
    "  - チャンク分割\n",
    "    - LangChain の SpacyTextSplitter を使用\n",
    "    - spaCy の日本語モデルは、ja_ginza を使用\n",
    "- ベクトルデータベースの構築（ChromaDB, インメモリ）\n",
    "- 検索機能の実装\n",
    "  - キーワード検索 @ BM25（spaCyで形態素解析の前処理が必要）\n",
    "  - Embedding model によるセマンティック検索\n",
    "  - ハイブリッド検索\n",
    "  - Reranker による再順位付け\n",
    "  - 検索機能をLLM の tool として定義\n",
    "- LangGraph による Workflow の実装\n",
    "  1. ユーザの質問を入力。\n",
    "  2. ユーザの質問に回答するためのタスク分割, 作成。\n",
    "  3. tool による検索。\n",
    "  4. tool による検索を終えて回答作成に進むか判断。再調査なら 3 に戻る。\n",
    "  5. ユーザへの回答の作成と提示。\n",
    "\n",
    "#### Gradio UI 機能\n",
    "- Google Colab 環境で Gradio を立ち上げする。\n",
    "- Gradio UI に PDFファイルをドラッグアンドドロップで入力する。\n",
    "- PDF ファイルが入力されたら、markdown に変換・前処理する。\n",
    "- Gradio UI にチャットインターフェース（ユーザ入力 / AI 回答）を設ける。\n",
    "- チャットインタフェースとは別に、AI の思考過程を表示する。（タスクの内容、検索の内容、判断結果）\n",
    "- AI チャットは、マルチターン対応、ストリーミング応答を行う。\n",
    "- ユーザ入力 / AI 回答とも、画面をスクロールして、会話履歴含めた全文を確認できるようにする。\n",
    "- **会話クリアボタン**: PDF を保持したまま会話履歴をリセットし、新しいスレッドで再質問できる。\n",
    "- **システムプロンプト設定**: アコーディオン内で自由に編集可能。デフォルトは「日本語で回答してください。」\n",
    "- **Temperature スライダー**: LLM の応答の正確さ/創造性を 0.0〜1.0 で調整できる。\n",
    "- **生成停止ボタン**: ストリーミング中の応答を途中で中断できる。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**必要なライブラリをインストール**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "opentelemetry-exporter-otlp-proto-http 1.38.0 requires opentelemetry-exporter-otlp-proto-common==1.38.0, but you have opentelemetry-exporter-otlp-proto-common 1.39.1 which is incompatible.\n",
      "opentelemetry-exporter-otlp-proto-http 1.38.0 requires opentelemetry-proto==1.38.0, but you have opentelemetry-proto 1.39.1 which is incompatible.\n",
      "opentelemetry-exporter-otlp-proto-http 1.38.0 requires opentelemetry-sdk~=1.38.0, but you have opentelemetry-sdk 1.39.1 which is incompatible.\n",
      "opentelemetry-exporter-gcp-logging 1.11.0a0 requires opentelemetry-sdk<1.39.0,>=1.35.0, but you have opentelemetry-sdk 1.39.1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# Google Colab に必要なライブラリをインストールする。\n",
    "# 1行にまとめることで pip が全パッケージの依存関係を一括解決する。\n",
    "# NOTE: Colab では uv ではなく pip を使う。uv は依存解決の過程で\n",
    "#       numpy 等をアップグレードし、プリインストール済みの scipy 等を壊すため。\n",
    "# NOTE: langchain 関連は 1.x 系に明示的に指定する。\n",
    "#       Colab プリインストールの 0.3.x が残ると langchain-mcp-adapters が動作しない。\n",
    "# Pythonのリストとして定義することで、Pylanceの警告を防ぎ、可読性を高める。\n",
    "\n",
    "# fmt: off\n",
    "pkgs = [\n",
    "    \"ollama\", \"langchain-ollama\",\n",
    "    \"langchain>=1.2.8\", \"langchain-core>=1.2.8\", \"langgraph>=1.0.7\",\n",
    "    \"markitdown[all]\", \"chromadb\", \"langchain-text-splitters>=0.3\",\n",
    "    \"spacy\", \"ginza\", \"ja-ginza\", \"rank-bm25\",\n",
    "    \"sentence-transformers\",\n",
    "    \"gradio>=6.0\",\n",
    "]\n",
    "# fmt: on\n",
    "\n",
    "# リストを結合して pip に渡す\n",
    "# magic command内で {変数} を使うと展開される機能を利用\n",
    "%pip install -U -q {\" \".join(pkgs)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Google Colab に Ollama をセットアップ**\n",
    "- 詳細は [01_connect_oss_llm.ipynb](01_connect_oss_llm.ipynb) を参照。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selecting previously unselected package zstd.\n",
      "(Reading database ... 121852 files and directories currently installed.)\n",
      "Preparing to unpack .../zstd_1.4.8+dfsg-3build1_amd64.deb ...\n",
      "Unpacking zstd (1.4.8+dfsg-3build1) ...\n",
      "Setting up zstd (1.4.8+dfsg-3build1) ...\n",
      "Processing triggers for man-db (2.10.2-1) ...\n",
      ">>> Installing ollama to /usr/local\n",
      ">>> Downloading ollama-linux-amd64.tar.zst\n",
      "######################################################################## 100.0%\n",
      ">>> Creating ollama user...\n",
      ">>> Adding ollama user to video group...\n",
      ">>> Adding current user to ollama group...\n",
      ">>> Creating ollama systemd service...\n",
      "\u001b[1m\u001b[31mWARNING:\u001b[m systemd is not running\n",
      "\u001b[1m\u001b[31mWARNING:\u001b[m Unable to detect NVIDIA/AMD GPU. Install lspci or lshw to automatically detect and install GPU dependencies.\n",
      ">>> The Ollama API is now available at 127.0.0.1:11434.\n",
      ">>> Install complete. Run \"ollama\" from the command line.\n",
      "success                                                     \n",
      "gpt-oss:20b: Done!\n",
      "  Model\n",
      "    architecture        gptoss    \n",
      "    parameters          20.9B     \n",
      "    context length      131072    \n",
      "    embedding length    2880      \n",
      "    quantization        MXFP4     \n",
      "\n",
      "  Capabilities\n",
      "    completion    \n",
      "    tools         \n",
      "    thinking      \n",
      "\n",
      "  Parameters\n",
      "    temperature    1    \n",
      "\n",
      "  License\n",
      "    Apache License               \n",
      "    Version 2.0, January 2004    \n",
      "    ...                          \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Ollama のインストール・起動・モデルのダウンロード\n",
    "# 詳細は 01_connect_oss_llm.ipynb を参照\n",
    "import subprocess\n",
    "import time\n",
    "import ollama  # type: ignore\n",
    "\n",
    "!apt-get install -y -qq zstd\n",
    "!curl -fsSL https://ollama.com/install.sh | sh\n",
    "\n",
    "process = subprocess.Popen(\n",
    "    [\"ollama\", \"serve\"],\n",
    "    stdout=subprocess.DEVNULL,\n",
    "    stderr=subprocess.DEVNULL,\n",
    ")\n",
    "time.sleep(5)\n",
    "\n",
    "\n",
    "def ollama_pull(model: str) -> None:\n",
    "    \"\"\"Ollama モデルをダウンロードし、進捗をインライン表示する。\"\"\"\n",
    "    for progress in ollama.pull(model, stream=True):\n",
    "        status = progress.get(\"status\", \"\")\n",
    "        total = progress.get(\"total\") or 0\n",
    "        completed = progress.get(\"completed\") or 0\n",
    "        if total:\n",
    "            line = f\"{status}: {completed / total:.0%}\"\n",
    "        else:\n",
    "            line = status\n",
    "        print(f\"\\r{line:<60}\", end=\"\", flush=True)\n",
    "    print(f\"\\n{model}: Done!\")\n",
    "\n",
    "\n",
    "model_name = \"gpt-oss:20b\"\n",
    "ollama_pull(model_name)\n",
    "!ollama show {model_name}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ChatOllama で LLM に接続**\n",
    "- 詳細は [01_connect_oss_llm.ipynb](01_connect_oss_llm.ipynb) を参照。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ChatOllama で LLM に接続する。\n",
    "from langchain_ollama import ChatOllama  # type: ignore\n",
    "\n",
    "llm = ChatOllama(\n",
    "    model=\"gpt-oss:20b\",\n",
    "    num_ctx=16384,\n",
    "    num_predict=-1,\n",
    "    temperature=0.8,\n",
    "    top_k=40,\n",
    "    top_p=0.9,\n",
    "    repeat_penalty=1.1,\n",
    "    reasoning=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Embedding モデル（ruri-v3-310m）と Reranker モデルのセットアップ**\n",
    "- 詳細は [04_AI_Embedding_RAG.ipynb](04_AI_Embedding_RAG.ipynb) を参照。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:104: UserWarning: \n",
      "Error while fetching `HF_TOKEN` secret value from your vault: 'Requesting secret HF_TOKEN timed out. Secrets can only be fetched when running from the Colab UI.'.\n",
      "You are not authenticated with the Hugging Face Hub in this notebook.\n",
      "If the error persists, please let us know by opening an issue on GitHub (https://github.com/huggingface/huggingface_hub/issues/new).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e2d6300f05d4d64a49655c3cde3f8c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/229 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b358614b6f9a4d69b0ad1038816e6b17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/205 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b05494324f464f099a5498c616f13bcb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13155e8973f441dea2543eade536b3e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/54.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69e3aa84dced4166b8a17268c97ed772",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0d9f54b4af549bc9e68e5fdd60db8c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.26G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2a9019c093b4c068cffca9cb74f0f5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/152 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08b8fcbeaa0241f4a697db82b5b4d6b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "579d01c9cc8a485287a46736fdc43e69",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25e136b3e48c44b3850f6dcc9fe4053f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/1.83M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b62f2a07a85d4e8b9115ac0035f24c07",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/968 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "808e5be1d56a4877b40a0aa3ec8e2d3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/296 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding dim: 768\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39e5c5723f6447a58f95ed13cf8530b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f42a7855357748ea9beadb377011148d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.26G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0bd19a131e644b0f94caa0c4d7bdb5d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/156 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n",
      "WARNING:huggingface_hub.utils._http:Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd04aa9e6bb44ce881c7528a9365d935",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7c6c7a041d24eb48d8390e057fb77d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "158be37bff924e7b8b40a803eca94717",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/1.83M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c770bda3efaa422fa153504b4efba15f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/968 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82ae5a8fcb6e4e20b5285f304de0eb20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reranker model loaded.\n"
     ]
    }
   ],
   "source": [
    "# Embedding: ruri-v3-310m (Sentence Transformers 経由)\n",
    "from langchain_core.embeddings import Embeddings  # type: ignore\n",
    "from sentence_transformers import SentenceTransformer, CrossEncoder  # type: ignore\n",
    "\n",
    "\n",
    "class RuriEmbeddings(Embeddings):\n",
    "    \"\"\"ruri-v3 を LangChain の Embeddings インターフェースでラップする。\"\"\"\n",
    "\n",
    "    def __init__(self, model_name: str = \"cl-nagoya/ruri-v3-310m\") -> None:\n",
    "        self.model = SentenceTransformer(model_name)\n",
    "\n",
    "    def embed_documents(self, texts: list[str]) -> list[list[float]]:\n",
    "        prefixed = [f\"検索文書: {t}\" for t in texts]\n",
    "        return self.model.encode(prefixed).tolist()\n",
    "\n",
    "    def embed_query(self, text: str) -> list[float]:\n",
    "        return self.model.encode(f\"検索クエリ: {text}\").tolist()\n",
    "\n",
    "\n",
    "embeddings = RuriEmbeddings()\n",
    "test_vec = embeddings.embed_query(\"テスト文です\")\n",
    "print(f\"Embedding dim: {len(test_vec)}\")\n",
    "\n",
    "# Reranker: cl-nagoya/ruri-v3-reranker-310m\n",
    "reranker = CrossEncoder(\"cl-nagoya/ruri-v3-reranker-310m\")\n",
    "print(\"Reranker model loaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PDF 前処理・チャンク分割・インデックス構築関数の定義**\n",
    "- Gradio UI から PDF がアップロードされた際に呼び出す。\n",
    "- markdown 変換 → クリーニング → チャンク分割 → ChromaDB + BM25 インデックス構築を一括で行う。\n",
    "- 検索用データはモジュールレベルのグローバル変数に格納する（Colab はシングルユーザ前提）。\n",
    "- 詳細は [04_AI_Embedding_RAG.ipynb](04_AI_Embedding_RAG.ipynb) および [05_AI_Agent_Workflow_RAG.ipynb](05_AI_Agent_Workflow_RAG.ipynb) を参照。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "前処理・インデックス構築関数を定義しました。\n"
     ]
    }
   ],
   "source": [
    "# PDF 前処理・チャンク分割・インデックス構築\n",
    "# Gradio から PDF アップロード時に process_and_index_pdf() を呼び出す。\n",
    "import re\n",
    "import unicodedata\n",
    "import numpy as np  # type: ignore\n",
    "import spacy  # type: ignore\n",
    "from markitdown import MarkItDown  # type: ignore\n",
    "from langchain_text_splitters import SpacyTextSplitter  # type: ignore\n",
    "from rank_bm25 import BM25Okapi  # type: ignore\n",
    "import chromadb  # type: ignore\n",
    "\n",
    "nlp = spacy.load(\"ja_ginza\", disable=[\"parser\", \"ner\"])\n",
    "\n",
    "# --- チャンク分割パラメータ ---\n",
    "CHUNK_SIZE = 500\n",
    "CHUNK_OVERLAP = 100\n",
    "BLOCK_MAX_BYTES = 40_000\n",
    "BLOCK_OVERLAP_CHARS = CHUNK_SIZE\n",
    "\n",
    "# --- モジュールレベルのグローバル変数（PDF アップロード時に更新される） ---\n",
    "_chunks: list[str] = []\n",
    "_bm25: BM25Okapi | None = None\n",
    "_collection = None\n",
    "_chroma_client = chromadb.Client()\n",
    "\n",
    "\n",
    "def tokenize(text: str) -> list[str]:\n",
    "    \"\"\"spaCy で形態素解析し、BM25 用のトークンリストを返す。\"\"\"\n",
    "    doc = nlp(text)\n",
    "    tokens = []\n",
    "    include_pos = {\"NOUN\", \"VERB\", \"ADJ\", \"PROPN\", \"NUM\"}\n",
    "    for token in doc:\n",
    "        if token.pos_ not in include_pos:\n",
    "            continue\n",
    "        if token.is_stop:\n",
    "            continue\n",
    "        lemma = token.lemma_\n",
    "        if len(lemma) == 1 and re.match(r\"[ぁ-ん\\u30fc!-/:-@\\[-`{-~]\", lemma):\n",
    "            continue\n",
    "        tokens.append(lemma)\n",
    "    return tokens\n",
    "\n",
    "\n",
    "def clean_pdf_text(text: str) -> str:\n",
    "    \"\"\"1文字行ブロックの除去 + 空行圧縮。\"\"\"\n",
    "    text = re.sub(\n",
    "        r\"(^[^\\S\\n]*\\S[^\\S\\n]*$\\n?){3,}\",\n",
    "        \"\\n\",\n",
    "        text,\n",
    "        flags=re.MULTILINE,\n",
    "    )\n",
    "    text = re.sub(r\"\\n{3,}\", \"\\n\\n\", text)\n",
    "    return text.strip()\n",
    "\n",
    "\n",
    "def split_into_safe_blocks(\n",
    "    text: str,\n",
    "    max_bytes: int = BLOCK_MAX_BYTES,\n",
    "    overlap_chars: int = BLOCK_OVERLAP_CHARS,\n",
    ") -> list[str]:\n",
    "    \"\"\"テキストを段落区切りで max_bytes 以下のブロックに分割する。\"\"\"\n",
    "    paragraphs = text.split(\"\\n\\n\")\n",
    "    blocks: list[str] = []\n",
    "    current = \"\"\n",
    "    for para in paragraphs:\n",
    "        candidate = current + \"\\n\\n\" + para if current else para\n",
    "        if len(candidate.encode(\"utf-8\")) > max_bytes and current:\n",
    "            blocks.append(current)\n",
    "            current = current[-overlap_chars:] + \"\\n\\n\" + para\n",
    "        else:\n",
    "            current = candidate\n",
    "    if current:\n",
    "        blocks.append(current)\n",
    "    return blocks\n",
    "\n",
    "\n",
    "def process_and_index_pdf(filepath: str) -> str:\n",
    "    \"\"\"PDF を markdown 変換・前処理し、チャンク分割・インデックス構築を行う。\n",
    "\n",
    "    グローバル変数 _chunks, _bm25, _collection を更新する。\n",
    "\n",
    "    Args:\n",
    "        filepath: PDF ファイルのパス。\n",
    "\n",
    "    Returns:\n",
    "        str: 前処理済みのテキスト全文。\n",
    "    \"\"\"\n",
    "    global _chunks, _bm25, _collection\n",
    "\n",
    "    # PDF → markdown 変換 + クリーニング\n",
    "    md_converter = MarkItDown()\n",
    "    result = md_converter.convert(filepath)\n",
    "    text = unicodedata.normalize(\"NFKC\", result.text_content)\n",
    "    text = clean_pdf_text(text)\n",
    "    print(f\"テキスト文字数: {len(text)}\")\n",
    "\n",
    "    # チャンク分割\n",
    "    text_splitter = SpacyTextSplitter(\n",
    "        separator=\"\\n\\n\",\n",
    "        pipeline=\"ja_ginza\",\n",
    "        chunk_size=CHUNK_SIZE,\n",
    "        chunk_overlap=CHUNK_OVERLAP,\n",
    "    )\n",
    "    blocks = split_into_safe_blocks(text)\n",
    "    _chunks = []\n",
    "    for block in blocks:\n",
    "        _chunks.extend(text_splitter.split_text(block))\n",
    "    print(f\"チャンク数: {len(_chunks)}\")\n",
    "\n",
    "    # ChromaDB（既存コレクションがあれば削除して再作成）\n",
    "    try:\n",
    "        _chroma_client.delete_collection(\"rag_docs\")\n",
    "    except Exception:\n",
    "        pass\n",
    "    _collection = _chroma_client.create_collection(\n",
    "        name=\"rag_docs\",\n",
    "        metadata={\"hnsw:space\": \"cosine\"},\n",
    "    )\n",
    "    chunk_embs = embeddings.embed_documents(_chunks)\n",
    "    _collection.add(\n",
    "        ids=[f\"chunk_{i}\" for i in range(len(_chunks))],\n",
    "        documents=_chunks,\n",
    "        embeddings=chunk_embs,\n",
    "    )\n",
    "    print(f\"ChromaDB: {_collection.count()} 件格納\")\n",
    "\n",
    "    # BM25\n",
    "    tokenized = [tokenize(c) for c in _chunks]\n",
    "    _bm25 = BM25Okapi(tokenized)\n",
    "    print(f\"BM25 インデックス構築完了: {len(tokenized)} 件\")\n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "print(\"前処理・インデックス構築関数を定義しました。\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**検索機能の実装（BM25 + セマンティック + ハイブリッド + Reranker）と tool 定義**\n",
    "- 詳細は [04_AI_Embedding_RAG.ipynb](04_AI_Embedding_RAG.ipynb) を参照。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "検索関数を定義しました。RAG Tool: search_document\n"
     ]
    }
   ],
   "source": [
    "# 検索関数の定義（BM25, セマンティック, ハイブリッド, Reranker）+ tool 定義\n",
    "from langchain_core.tools import tool  # type: ignore\n",
    "\n",
    "RETRIEVAL_TOP_K = 20\n",
    "RERANK_TOP_K = 5\n",
    "BM25_WEIGHT = 0.3\n",
    "MAX_RETURN_CHARS = 8000\n",
    "\n",
    "\n",
    "def search_bm25(query: str, top_k: int = RETRIEVAL_TOP_K) -> list[dict]:\n",
    "    \"\"\"BM25 によるキーワード検索を行う。\"\"\"\n",
    "    if _bm25 is None:\n",
    "        return []\n",
    "    tokenized_query = tokenize(query)\n",
    "    scores = _bm25.get_scores(tokenized_query)\n",
    "    top_indices = np.argsort(scores)[::-1][:top_k]\n",
    "    return [\n",
    "        {\n",
    "            \"rank\": rank + 1,\n",
    "            \"chunk_id\": int(idx),\n",
    "            \"score\": float(scores[idx]),\n",
    "            \"text\": _chunks[idx],\n",
    "        }\n",
    "        for rank, idx in enumerate(top_indices)\n",
    "        if scores[idx] > 0\n",
    "    ]\n",
    "\n",
    "\n",
    "def search_semantic(query: str, top_k: int = RETRIEVAL_TOP_K) -> list[dict]:\n",
    "    \"\"\"Embedding model によるセマンティック検索を行う。\"\"\"\n",
    "    if _collection is None:\n",
    "        return []\n",
    "    query_embedding = embeddings.embed_query(query)\n",
    "    results = _collection.query(query_embeddings=[query_embedding], n_results=top_k)\n",
    "    return [\n",
    "        {\n",
    "            \"rank\": rank + 1,\n",
    "            \"chunk_id\": int(doc_id.split(\"_\")[1]),\n",
    "            \"score\": 1.0 - dist,\n",
    "            \"text\": doc,\n",
    "        }\n",
    "        for rank, (doc_id, doc, dist) in enumerate(\n",
    "            zip(results[\"ids\"][0], results[\"documents\"][0], results[\"distances\"][0])\n",
    "        )\n",
    "    ]\n",
    "\n",
    "\n",
    "def search_hybrid(\n",
    "    query: str, top_k: int = RETRIEVAL_TOP_K, bm25_weight: float = BM25_WEIGHT\n",
    ") -> list[dict]:\n",
    "    \"\"\"BM25 とセマンティック検索の RRF ハイブリッド検索を行う。\"\"\"\n",
    "    k = 60\n",
    "    bm25_results = search_bm25(query, top_k=top_k)\n",
    "    semantic_results = search_semantic(query, top_k=top_k)\n",
    "\n",
    "    scores: dict[int, float] = {}\n",
    "    texts: dict[int, str] = {}\n",
    "\n",
    "    for r in bm25_results:\n",
    "        cid = r[\"chunk_id\"]\n",
    "        scores[cid] = scores.get(cid, 0) + bm25_weight / (k + r[\"rank\"])\n",
    "        texts[cid] = r[\"text\"]\n",
    "\n",
    "    semantic_weight = 1.0 - bm25_weight\n",
    "    for r in semantic_results:\n",
    "        cid = r[\"chunk_id\"]\n",
    "        scores[cid] = scores.get(cid, 0) + semantic_weight / (k + r[\"rank\"])\n",
    "        texts[cid] = r[\"text\"]\n",
    "\n",
    "    sorted_ids = sorted(scores, key=lambda cid: scores[cid], reverse=True)[:top_k]\n",
    "    return [\n",
    "        {\"rank\": rank + 1, \"chunk_id\": cid, \"score\": scores[cid], \"text\": texts[cid]}\n",
    "        for rank, cid in enumerate(sorted_ids)\n",
    "    ]\n",
    "\n",
    "\n",
    "def rerank(query: str, results: list[dict], top_k: int = RERANK_TOP_K) -> list[dict]:\n",
    "    \"\"\"Reranker (CrossEncoder) で検索結果を再順位付けする。\"\"\"\n",
    "    if not results:\n",
    "        return []\n",
    "    pairs = [(query, r[\"text\"]) for r in results]\n",
    "    scores = reranker.predict(pairs)\n",
    "    ranked_indices = np.argsort(scores)[::-1][:top_k]\n",
    "    return [\n",
    "        {\n",
    "            \"rank\": rank + 1,\n",
    "            \"chunk_id\": results[idx][\"chunk_id\"],\n",
    "            \"score\": float(scores[idx]),\n",
    "            \"text\": results[idx][\"text\"],\n",
    "        }\n",
    "        for rank, idx in enumerate(ranked_indices)\n",
    "    ]\n",
    "\n",
    "\n",
    "@tool\n",
    "def search_document(query: str) -> str:\n",
    "    \"\"\"外部ナレッジベースから、クエリに関連する情報を検索・取得します。\n",
    "    ユーザーの質問に対し、具体的な事実、データ、あるいは詳細な文脈が必要な場合、\n",
    "    自身の知識だけで回答せずに必ずこのツールを使用してください。\n",
    "\n",
    "    Args:\n",
    "        query: 検索したい内容を表す、具体的かつ完全な文章（日本語）。\n",
    "    \"\"\"\n",
    "    try:\n",
    "        hybrid_results = search_hybrid(query)\n",
    "        reranked = rerank(query, hybrid_results)\n",
    "    except Exception as e:\n",
    "        return f\"検索中にエラーが発生しました: {e}\"\n",
    "\n",
    "    if not reranked:\n",
    "        return \"検索結果が見つかりませんでした。\"\n",
    "\n",
    "    passages = []\n",
    "    total_chars = 0\n",
    "    for r in reranked:\n",
    "        passage = f\"[チャンク {r['chunk_id']}] (スコア: {r['score']:.4f})\\n{r['text']}\"\n",
    "        total_chars += len(passage)\n",
    "        if total_chars > MAX_RETURN_CHARS:\n",
    "            break\n",
    "        passages.append(passage)\n",
    "    return \"\\n\\n---\\n\\n\".join(passages)\n",
    "\n",
    "\n",
    "search_tool = search_document\n",
    "print(f\"検索関数を定義しました。RAG Tool: {search_tool.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**LangGraph による Workflow の実装**\n",
    "\n",
    "notebook 05 の Workflow を Gradio UI 対応に適応する。\n",
    "- Workflow のノード処理を個別の async 関数として実装し、Gradio から直接呼び出す。\n",
    "  - これにより、思考過程のリアルタイム表示と最終回答のストリーミングを実現する。\n",
    "- LangGraph のグラフ定義は Workflow の可視化に使用する。\n",
    "- 詳細は [05_AI_Agent_Workflow_RAG.ipynb](05_AI_Agent_Workflow_RAG.ipynb) を参照。\n",
    "\n",
    "**Workflow の流れ**\n",
    "1. **task_planning**: ユーザの質問を受け取り、回答に必要なサブタスク（目的＋検索クエリ）を構造化して作成する。\n",
    "2. **doc_search**: 各サブタスクの検索クエリを `search_document` ツールで実行し、目的と紐付けた検索結果を蓄積する。\n",
    "3. **judge**: サブタスクの目的ごとに、検索結果が十分かを LLM が判断する。不足なら追加サブタスクを生成して doc_search に戻る。\n",
    "4. **generate_answer**: 目的ごとに整理された検索結果をもとに、ユーザの質問に対する最終回答をストリーミング生成する。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pydantic / with_structured_output による構造化出力**\n",
    "\n",
    "検索結果に LaTeX 数式が含まれるなどの場合、LLM の出力が引きずられて JSON パースが機能しない場合があった。このため、Pydantic / with_structured_output による構造化出力によって、望ましい出力を LLM に強制させる。\n",
    "- `@model_validator(mode=\"after\")` で、LLM の出力の論理矛盾を自動補正する。\n",
    "- 詳細は [05_AI_Agent_Workflow_RAG.ipynb](05_AI_Agent_Workflow_RAG.ipynb) を参照。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Workflow の状態定義・Pydantic モデル・システムプロンプトを定義しました。\n"
     ]
    }
   ],
   "source": [
    "# Workflow の状態定義・Pydantic モデル・システムプロンプト・ユーティリティ関数\n",
    "from typing import TypedDict\n",
    "from pydantic import BaseModel, Field, model_validator  # type: ignore\n",
    "from langchain_core.messages import HumanMessage, SystemMessage  # type: ignore\n",
    "from langgraph.graph import StateGraph, START, END  # type: ignore\n",
    "from IPython.display import Image, display\n",
    "\n",
    "# --- グローバル設定 ---\n",
    "MAX_LOOP_COUNT = 2  # judge → doc_search 再調査ループの上限回数\n",
    "\n",
    "\n",
    "# --- Workflow の状態 ---\n",
    "class WorkflowState(TypedDict):\n",
    "    question: str\n",
    "    subtasks: list[dict]  # [{\"purpose\": str, \"queries\": [str]}]\n",
    "    search_results: list[str]  # 目的と紐付けた検索結果\n",
    "    answer: str\n",
    "    loop_count: int\n",
    "\n",
    "\n",
    "# --- with_structured_output 用の Pydantic モデル ---\n",
    "class Subtask(BaseModel):\n",
    "    purpose: str = Field(description=\"このサブタスクで明らかにしたいこと\")\n",
    "    queries: list[str] = Field(description=\"検索クエリのリスト\")\n",
    "\n",
    "\n",
    "class TaskPlanningResult(BaseModel):\n",
    "    subtasks: list[Subtask] = Field(description=\"サブタスクのリスト（最大3個）\")\n",
    "\n",
    "\n",
    "class JudgeResult(BaseModel):\n",
    "    sufficient: bool = Field(description=\"情報が十分かどうか\")\n",
    "    reason: str = Field(description=\"判断理由を日本語で1文で\")\n",
    "    additional_subtasks: list[Subtask] | None = Field(\n",
    "        default=None,\n",
    "        description=\"不足時の追加サブタスク\",\n",
    "    )\n",
    "\n",
    "    @model_validator(mode=\"after\")\n",
    "    def force_consistency(self):\n",
    "        \"\"\"LLM 出力の論理矛盾を自動補正する。\"\"\"\n",
    "        if self.sufficient:\n",
    "            self.additional_subtasks = None\n",
    "        if not self.sufficient and not self.additional_subtasks:\n",
    "            self.sufficient = True\n",
    "            self.reason += (\n",
    "                \" (※追加調査事項が具体化できなかったため、現状の情報で回答します)\"\n",
    "            )\n",
    "            self.additional_subtasks = None\n",
    "        return self\n",
    "\n",
    "\n",
    "# --- 各ノードのシステムプロンプト ---\n",
    "SYSTEM_PROMPT_TASK_PLANNING = \"\"\"\\\n",
    "あなたはリサーチプランナーです。\n",
    "ユーザの質問に回答するために、ナレッジベース（技術文書）を検索するためのサブタスクを作成してください。\n",
    "\n",
    "サブタスクは最大3個までとしてください。\n",
    "purpose は判定ステップで「この目的に十分な情報が得られたか」を評価する基準になります。\n",
    "具体的かつ明確に書いてください。\n",
    "検索クエリは、技術文書から関連情報を検索するための日本語の具体的なフレーズにしてください。\n",
    "\"\"\"\n",
    "\n",
    "SYSTEM_PROMPT_JUDGE = \"\"\"\\\n",
    "あなたはリサーチの品質を判定する審査員です。\n",
    "ユーザの質問と検索結果を見て、回答に十分な情報があるか判断してください。\n",
    "検索結果には【目的: ...】タグが付いています。\n",
    "各目的について十分な情報が得られているかを確認してください。\n",
    "\n",
    "sufficient が true なら回答作成に進みます。\n",
    "sufficient が false なら、不足している目的について additional_subtasks を生成してください。\n",
    "\"\"\"\n",
    "\n",
    "SYSTEM_PROMPT_GENERATE_ANSWER = \"\"\"\\\n",
    "あなたはリサーチ結果をもとに回答するAIアシスタントです。\n",
    "検索結果を参考に、ユーザの質問に日本語で丁寧に回答してください。\n",
    "回答は必ず検索結果に基づいて作成し、検索結果に含まれない情報は含めないでください。\n",
    "回答の最後に、以下の形式で結論をまとめてください。\n",
    "\n",
    "# 結論\n",
    "- ユーザの質問: （質問内容）\n",
    "- 回答: （簡潔な回答）\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def _make_llm(temperature: float) -> ChatOllama:\n",
    "    \"\"\"指定した temperature で ChatOllama インスタンスを生成する。\"\"\"\n",
    "    return ChatOllama(\n",
    "        model=llm.model,\n",
    "        num_ctx=llm.num_ctx,\n",
    "        num_predict=llm.num_predict,\n",
    "        temperature=temperature,\n",
    "        top_k=llm.top_k,\n",
    "        top_p=llm.top_p,\n",
    "        repeat_penalty=llm.repeat_penalty,\n",
    "        reasoning=llm.reasoning,\n",
    "    )\n",
    "\n",
    "\n",
    "print(\"Workflow の状態定義・Pydantic モデル・システムプロンプトを定義しました。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Workflow 実行関数を定義しました: run_task_planning, run_doc_search, run_judge\n"
     ]
    }
   ],
   "source": [
    "# Workflow 実行関数の定義（Gradio から直接呼び出す async 関数）\n",
    "# 各関数は (結果, ログ文字列) のタプルを返し、Gradio 側で思考過程を表示する。\n",
    "\n",
    "\n",
    "async def run_task_planning(\n",
    "    question: str, temperature: float\n",
    ") -> tuple[list[dict], str]:\n",
    "    \"\"\"ユーザの質問を分析し、サブタスク（目的＋検索クエリ）を作成する。\"\"\"\n",
    "    structured_llm = _make_llm(temperature).with_structured_output(TaskPlanningResult)\n",
    "    log = \"\"\n",
    "\n",
    "    try:\n",
    "        result = await structured_llm.ainvoke(\n",
    "            [\n",
    "                SystemMessage(content=SYSTEM_PROMPT_TASK_PLANNING),\n",
    "                HumanMessage(content=question),\n",
    "            ]\n",
    "        )\n",
    "        subtasks = [st.model_dump() for st in result.subtasks]\n",
    "    except Exception as e:\n",
    "        log += f\"構造化出力失敗 → フォールバック: {e}\\n\"\n",
    "        subtasks = [{\"purpose\": \"基本調査\", \"queries\": [question]}]\n",
    "\n",
    "    log += f\"サブタスク数: {len(subtasks)}\\n\"\n",
    "    for i, st in enumerate(subtasks):\n",
    "        log += f\"  {i + 1}. 目的: {st['purpose']}\\n\"\n",
    "        log += f\"     クエリ: {st['queries']}\\n\"\n",
    "    return subtasks, log\n",
    "\n",
    "\n",
    "async def run_doc_search(subtasks: list[dict]) -> tuple[list[str], str]:\n",
    "    \"\"\"各サブタスクの検索クエリを search_document ツールで実行する。\"\"\"\n",
    "    results: list[str] = []\n",
    "    log = \"\"\n",
    "\n",
    "    for st in subtasks:\n",
    "        purpose = st[\"purpose\"]\n",
    "        log += f\"目的: {purpose}\\n\"\n",
    "        for query in st[\"queries\"]:\n",
    "            log += f\"  検索中: {query}\\n\"\n",
    "            try:\n",
    "                result = search_tool.invoke({\"query\": query})\n",
    "            except Exception as e:\n",
    "                log += f\"  エラー: {e}\\n\"\n",
    "                continue\n",
    "            if not result or result == \"検索結果が見つかりませんでした。\":\n",
    "                log += f\"  結果なし\\n\"\n",
    "                continue\n",
    "            results.append(f\"【目的: {purpose}】\\n【クエリ: {query}】\\n{result}\")\n",
    "            log += f\"  → 取得完了\\n\"\n",
    "    return results, log\n",
    "\n",
    "\n",
    "async def run_judge(\n",
    "    question: str, search_results: list[str], temperature: float\n",
    ") -> tuple[JudgeResult, str]:\n",
    "    \"\"\"検索結果が十分かを判断し、不足なら追加サブタスクを生成する。\"\"\"\n",
    "    results_text = \"\\n\\n\".join(search_results)\n",
    "    structured_llm = _make_llm(temperature).with_structured_output(JudgeResult)\n",
    "    log = \"\"\n",
    "\n",
    "    try:\n",
    "        judgment = await structured_llm.ainvoke(\n",
    "            [\n",
    "                SystemMessage(content=SYSTEM_PROMPT_JUDGE),\n",
    "                HumanMessage(content=f\"質問: {question}\\n\\n検索結果:\\n{results_text}\"),\n",
    "            ]\n",
    "        )\n",
    "    except Exception as e:\n",
    "        log += f\"判定失敗: {e}\\n\"\n",
    "        judgment = JudgeResult(sufficient=True, reason=\"判定エラーのため回答生成へ\")\n",
    "\n",
    "    if judgment.sufficient:\n",
    "        log += f\"情報十分（理由: {judgment.reason}）\\n\"\n",
    "    else:\n",
    "        log += f\"情報不足（理由: {judgment.reason}）\\n\"\n",
    "        for i, st in enumerate(judgment.additional_subtasks or []):\n",
    "            log += f\"  追加 {i + 1}. {st.purpose}: {st.queries}\\n\"\n",
    "    return judgment, log\n",
    "\n",
    "\n",
    "print(\"Workflow 実行関数を定義しました: run_task_planning, run_doc_search, run_judge\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# LangGraph Workflow グラフの構築と可視化\n",
    "# Gradio では各ノード関数を直接呼び出すが、グラフ定義は Workflow の構造を示すために保持する。\n",
    "\n",
    "\n",
    "async def _node_task_planning(state: WorkflowState) -> dict:\n",
    "    subtasks, _ = await run_task_planning(state[\"question\"], llm.temperature)\n",
    "    return {\"subtasks\": subtasks, \"search_results\": [], \"loop_count\": 0}\n",
    "\n",
    "\n",
    "async def _node_doc_search(state: WorkflowState) -> dict:\n",
    "    results, _ = await run_doc_search(state[\"subtasks\"])\n",
    "    existing = list(state.get(\"search_results\") or [])\n",
    "    return {\"search_results\": existing + results, \"subtasks\": []}\n",
    "\n",
    "\n",
    "async def _node_judge(state: WorkflowState) -> dict:\n",
    "    loop_count = state.get(\"loop_count\", 0)\n",
    "    if loop_count >= MAX_LOOP_COUNT:\n",
    "        return {\"subtasks\": [], \"loop_count\": loop_count}\n",
    "    judgment, _ = await run_judge(\n",
    "        state[\"question\"], state[\"search_results\"], llm.temperature\n",
    "    )\n",
    "    additional = []\n",
    "    if not judgment.sufficient and judgment.additional_subtasks:\n",
    "        additional = [st.model_dump() for st in judgment.additional_subtasks]\n",
    "    return {\"subtasks\": additional, \"loop_count\": loop_count + 1}\n",
    "\n",
    "\n",
    "def _should_continue(state: WorkflowState) -> str:\n",
    "    return \"doc_search\" if state.get(\"subtasks\") else \"generate_answer\"\n",
    "\n",
    "\n",
    "async def _node_generate_answer(state: WorkflowState) -> dict:\n",
    "    results_text = \"\\n\\n\".join(state[\"search_results\"])\n",
    "    response = await llm.ainvoke(\n",
    "        [\n",
    "            SystemMessage(content=SYSTEM_PROMPT_GENERATE_ANSWER),\n",
    "            HumanMessage(\n",
    "                content=f\"質問: {state['question']}\\n\\n検索結果:\\n{results_text}\"\n",
    "            ),\n",
    "        ]\n",
    "    )\n",
    "    return {\"answer\": response.content or \"\"}\n",
    "\n",
    "\n",
    "# グラフ構築\n",
    "wf = StateGraph(WorkflowState)\n",
    "wf.add_node(\"task_planning\", _node_task_planning)\n",
    "wf.add_node(\"doc_search\", _node_doc_search)\n",
    "wf.add_node(\"judge\", _node_judge)\n",
    "wf.add_node(\"generate_answer\", _node_generate_answer)\n",
    "wf.add_edge(START, \"task_planning\")\n",
    "wf.add_edge(\"task_planning\", \"doc_search\")\n",
    "wf.add_edge(\"doc_search\", \"judge\")\n",
    "wf.add_conditional_edges(\n",
    "    \"judge\",\n",
    "    _should_continue,\n",
    "    {\"doc_search\": \"doc_search\", \"generate_answer\": \"generate_answer\"},\n",
    ")\n",
    "wf.add_edge(\"generate_answer\", END)\n",
    "workflow_app = wf.compile()\n",
    "\n",
    "# グラフの可視化\n",
    "display(Image(workflow_app.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Gradio UI の実装**\n",
    "- 左カラム: PDF ファイルのドラッグ＆ドロップ入力と、AI の思考過程の表示\n",
    "- 右カラム: マルチターン対応のチャットインターフェース（会話履歴スクロール可能）\n",
    "  - システムプロンプト設定（アコーディオン内、デフォルト非表示）\n",
    "  - Temperature スライダー（0.0〜1.0）で応答の正確さ/創造性を調整\n",
    "  - 送信 / 生成停止 / 会話クリア ボタンを配置\n",
    "- PDF をアップロードすると、前処理・チャンク分割・インデックス構築が実行される。\n",
    "- ユーザの質問ごとに RAG Workflow（タスク分割→検索→判定→回答生成）が実行される。\n",
    "- 思考過程（タスクの内容、検索の内容、判断結果）はチャットとは別の領域に表示する。\n",
    "- 最終回答はストリーミングで表示する。\n",
    "- `gr.State()` でユーザー（ブラウザタブ）ごとにセッション状態を管理する。\n",
    "- `share=True` で Colab 環境からでもパブリック URL でアクセス可能。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
      "* Running on public URL: https://10e8eadf53240ffc0c.gradio.live\n",
      "\n",
      "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://10e8eadf53240ffc0c.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "テキスト文字数: 79515\n",
      "チャンク数: 214\n",
      "ChromaDB: 214 件格納\n",
      "BM25 インデックス構築完了: 214 件\n"
     ]
    }
   ],
   "source": [
    "# Gradio UI: PDF アップロード + RAG Workflow チャット（ストリーミング応答）\n",
    "import uuid\n",
    "import gradio as gr  # type: ignore\n",
    "\n",
    "\n",
    "def on_pdf_upload(filepath: str, state: dict) -> tuple[str, list, dict, str]:\n",
    "    \"\"\"PDF アップロード時: 前処理・インデックス構築を行い、ステータスを返す。\n",
    "\n",
    "    Args:\n",
    "        filepath: Gradio の File コンポーネントが生成した一時ファイルパス。\n",
    "        state: セッション状態。\n",
    "\n",
    "    Returns:\n",
    "        tuple: (ステータス表示, チャット履歴リセット, 更新state, 思考ログリセット)\n",
    "    \"\"\"\n",
    "    if filepath is None:\n",
    "        return \"PDF ファイルをドラッグ＆ドロップしてください。\", [], state, \"\"\n",
    "\n",
    "    text = process_and_index_pdf(filepath)\n",
    "\n",
    "    new_state = state.copy()\n",
    "    new_state[\"pdf_text\"] = text\n",
    "    new_state[\"thread_id\"] = str(uuid.uuid4())\n",
    "\n",
    "    status = f\"PDF 読み込み完了: {len(_chunks)} チャンク\\n文字数: {len(text)}\"\n",
    "    return (\n",
    "        status,\n",
    "        [],\n",
    "        new_state,\n",
    "        f\"PDF インデックス構築完了（{len(_chunks)} チャンク）\\n\",\n",
    "    )\n",
    "\n",
    "\n",
    "def on_clear_chat(state: dict) -> tuple[list, dict, str]:\n",
    "    \"\"\"会話履歴をクリアし、新しいスレッド ID を発行する。\"\"\"\n",
    "    new_state = state.copy()\n",
    "    new_state[\"thread_id\"] = str(uuid.uuid4())\n",
    "    return [], new_state, \"\"\n",
    "\n",
    "\n",
    "async def respond(\n",
    "    message: str,\n",
    "    chat_history: list,\n",
    "    state: dict,\n",
    "    system_prompt: str,\n",
    "    temperature: float,\n",
    "):\n",
    "    \"\"\"ユーザのメッセージに対し RAG Workflow を実行し、ストリーミングで応答する。\n",
    "\n",
    "    思考過程（タスク分割・検索・判定）をリアルタイム表示し、\n",
    "    最終回答をトークン単位でストリーミングする。\n",
    "\n",
    "    Args:\n",
    "        message: ユーザが入力したメッセージ。\n",
    "        chat_history: Gradio Chatbot 形式の会話履歴。\n",
    "        state: セッション状態。\n",
    "        system_prompt: LLM に与えるシステムプロンプト。\n",
    "        temperature: LLM の temperature パラメータ。\n",
    "\n",
    "    Yields:\n",
    "        tuple: (入力欄クリア, 会話履歴, state, 思考ログ)\n",
    "    \"\"\"\n",
    "    # PDF 未読み込みチェック\n",
    "    if not _chunks:\n",
    "        chat_history = chat_history + [\n",
    "            {\"role\": \"user\", \"content\": message},\n",
    "            {\n",
    "                \"role\": \"assistant\",\n",
    "                \"content\": \"PDF ファイルをアップロードしてください。\",\n",
    "            },\n",
    "        ]\n",
    "        yield \"\", chat_history, state, \"⚠️ PDF が未読み込みです。\\n\"\n",
    "        return\n",
    "\n",
    "    # 空入力の防止\n",
    "    if not message.strip():\n",
    "        yield \"\", chat_history, state, \"\"\n",
    "        return\n",
    "\n",
    "    chat_history = chat_history + [\n",
    "        {\"role\": \"user\", \"content\": message},\n",
    "        {\"role\": \"assistant\", \"content\": \"\"},\n",
    "    ]\n",
    "    thinking = \"\"\n",
    "\n",
    "    # --- Phase 1: タスク分割 ---\n",
    "    thinking += \"📋 タスク分割中...\\n\"\n",
    "    yield \"\", chat_history, state, thinking\n",
    "\n",
    "    subtasks, log = await run_task_planning(message, temperature)\n",
    "    thinking += log + \"\\n\"\n",
    "    yield \"\", chat_history, state, thinking\n",
    "\n",
    "    # --- Phase 2: 検索 + 判定ループ ---\n",
    "    search_results: list[str] = []\n",
    "    loop_count = 0\n",
    "    current_subtasks = subtasks\n",
    "\n",
    "    while current_subtasks:\n",
    "        thinking += \"🔍 ドキュメント検索中...\\n\"\n",
    "        yield \"\", chat_history, state, thinking\n",
    "\n",
    "        new_results, log = await run_doc_search(current_subtasks)\n",
    "        search_results.extend(new_results)\n",
    "        thinking += log + \"\\n\"\n",
    "        yield \"\", chat_history, state, thinking\n",
    "\n",
    "        loop_count += 1\n",
    "        if loop_count > MAX_LOOP_COUNT:\n",
    "            thinking += \"⚠️ ループ上限に到達 → 回答作成へ\\n\\n\"\n",
    "            yield \"\", chat_history, state, thinking\n",
    "            break\n",
    "\n",
    "        thinking += \"⚖️ 情報の十分性を判定中...\\n\"\n",
    "        yield \"\", chat_history, state, thinking\n",
    "\n",
    "        judgment, log = await run_judge(message, search_results, temperature)\n",
    "        thinking += log + \"\\n\"\n",
    "        yield \"\", chat_history, state, thinking\n",
    "\n",
    "        if judgment.sufficient:\n",
    "            break\n",
    "        current_subtasks = [\n",
    "            st.model_dump() for st in (judgment.additional_subtasks or [])\n",
    "        ]\n",
    "\n",
    "    # --- Phase 3: 回答生成（ストリーミング） ---\n",
    "    thinking += \"✏️ 回答を生成中...\\n\"\n",
    "    yield \"\", chat_history, state, thinking\n",
    "\n",
    "    results_text = \"\\n\\n\".join(search_results)\n",
    "\n",
    "    # システムプロンプト構築\n",
    "    sys_content = system_prompt + \"\\n\\n\" + SYSTEM_PROMPT_GENERATE_ANSWER\n",
    "\n",
    "    # マルチターン対応: 直近の会話履歴をコンテキストに含める\n",
    "    # 現在の質問（末尾2件）は除外し、直近4メッセージ（2往復）まで含める\n",
    "    recent_history = chat_history[:-2][-4:]\n",
    "    history_lines = []\n",
    "    for msg in recent_history:\n",
    "        role = \"ユーザ\" if msg[\"role\"] == \"user\" else \"AI\"\n",
    "        history_lines.append(f\"{role}: {msg['content'][:500]}\")\n",
    "\n",
    "    user_content = \"\"\n",
    "    if history_lines:\n",
    "        user_content += \"会話履歴:\\n\" + \"\\n\".join(history_lines) + \"\\n\\n\"\n",
    "    user_content += f\"質問: {message}\\n\\n検索結果:\\n{results_text}\"\n",
    "\n",
    "    llm_instance = _make_llm(temperature)\n",
    "    bot_reply = \"\"\n",
    "    async for chunk in llm_instance.astream(\n",
    "        [\n",
    "            SystemMessage(content=sys_content),\n",
    "            HumanMessage(content=user_content),\n",
    "        ]\n",
    "    ):\n",
    "        if chunk.content:\n",
    "            bot_reply += chunk.content\n",
    "            chat_history = chat_history[:-1] + [\n",
    "                {\"role\": \"assistant\", \"content\": bot_reply}\n",
    "            ]\n",
    "            yield \"\", chat_history, state, thinking\n",
    "\n",
    "    thinking += \"✅ 回答生成完了\\n\"\n",
    "    yield \"\", chat_history, state, thinking\n",
    "\n",
    "\n",
    "# --- Gradio UI の構築 ---\n",
    "with gr.Blocks(title=\"RAG チャットアシスタント\") as demo:\n",
    "    gr.Markdown(\"### RAG チャットアシスタント（AI Agent Workflow + RAG）\")\n",
    "\n",
    "    session_state = gr.State(\n",
    "        {\n",
    "            \"pdf_text\": \"\",\n",
    "            \"thread_id\": str(uuid.uuid4()),\n",
    "        }\n",
    "    )\n",
    "\n",
    "    with gr.Row():\n",
    "        # 左カラム: PDF アップロード + 思考過程\n",
    "        with gr.Column(scale=1):\n",
    "            pdf_input = gr.File(\n",
    "                label=\"PDF ファイルをドラッグ＆ドロップ\",\n",
    "                file_types=[\".pdf\"],\n",
    "                type=\"filepath\",\n",
    "            )\n",
    "            pdf_status = gr.Textbox(\n",
    "                label=\"PDF ステータス\",\n",
    "                lines=2,\n",
    "                max_lines=2,\n",
    "                interactive=False,\n",
    "            )\n",
    "            thinking_display = gr.Textbox(\n",
    "                label=\"AI の思考過程\",\n",
    "                lines=25,\n",
    "                max_lines=25,\n",
    "                interactive=False,\n",
    "            )\n",
    "\n",
    "        # 右カラム: チャットインターフェース\n",
    "        with gr.Column(scale=1):\n",
    "            chatbot = gr.Chatbot(\n",
    "                label=\"AI チャット\",\n",
    "                height=400,\n",
    "            )\n",
    "\n",
    "            with gr.Accordion(\"システムプロンプト設定 (任意)\", open=False):\n",
    "                system_prompt_input = gr.Textbox(\n",
    "                    label=\"システムプロンプト\",\n",
    "                    value=\"日本語で回答してください。\",\n",
    "                    lines=2,\n",
    "                )\n",
    "\n",
    "            temp_slider = gr.Slider(\n",
    "                minimum=0.0,\n",
    "                maximum=1.0,\n",
    "                value=llm.temperature,\n",
    "                step=0.1,\n",
    "                label=\"Temperature (低いほど正確、高いほど創造的)\",\n",
    "            )\n",
    "\n",
    "            msg_input = gr.Textbox(\n",
    "                label=\"メッセージを入力\",\n",
    "                placeholder=\"質問を入力してください...\",\n",
    "                lines=2,\n",
    "            )\n",
    "            with gr.Row():\n",
    "                send_btn = gr.Button(\"送信\", variant=\"primary\")\n",
    "                stop_btn = gr.Button(\"生成を停止\", variant=\"stop\")\n",
    "                clear_btn = gr.Button(\"会話をクリア\", variant=\"secondary\")\n",
    "\n",
    "    # --- イベントハンドラの設定 ---\n",
    "\n",
    "    # PDF アップロード\n",
    "    pdf_input.change(\n",
    "        on_pdf_upload,\n",
    "        inputs=[pdf_input, session_state],\n",
    "        outputs=[pdf_status, chatbot, session_state, thinking_display],\n",
    "    )\n",
    "\n",
    "    # 送信（ストリーミング応答）\n",
    "    submit_args = dict(\n",
    "        fn=respond,\n",
    "        inputs=[\n",
    "            msg_input,\n",
    "            chatbot,\n",
    "            session_state,\n",
    "            system_prompt_input,\n",
    "            temp_slider,\n",
    "        ],\n",
    "        outputs=[msg_input, chatbot, session_state, thinking_display],\n",
    "    )\n",
    "    submit_event_click = send_btn.click(**submit_args)\n",
    "    submit_event_enter = msg_input.submit(**submit_args)\n",
    "\n",
    "    # 生成停止ボタン\n",
    "    stop_btn.click(\n",
    "        fn=None,\n",
    "        inputs=None,\n",
    "        outputs=None,\n",
    "        cancels=[submit_event_click, submit_event_enter],\n",
    "    )\n",
    "\n",
    "    # 会話クリアボタン\n",
    "    clear_btn.click(\n",
    "        on_clear_chat,\n",
    "        inputs=[session_state],\n",
    "        outputs=[chatbot, session_state, thinking_display],\n",
    "    )\n",
    "\n",
    "# Colab 環境では share=True でパブリック URL を生成する\n",
    "demo.launch(share=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closing server running on port: 7860\n"
     ]
    }
   ],
   "source": [
    "demo.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
