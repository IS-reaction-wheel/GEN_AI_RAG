{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 07 AI Agent Workflow + RAG on Gradio UI\n",
    "gpt-oss:20bã‚’ä½¿ç”¨ã™ã‚‹æ§‹æˆã®ãŸã‚ã€**Colab GPU ã¯ L4 ã‚’ä½¿ç”¨ã™ã‚‹ã“ã¨ã€‚**\n",
    "\n",
    "#### ã‚³ã‚¢æ©Ÿèƒ½ã¨å®Ÿè£…ã®æ¦‚è¦\n",
    "- å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«\n",
    "- Google Colab ã« Ollama ã‚’ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—\n",
    "  - LLM ãƒ¢ãƒ‡ãƒ«ã¯ gpt-oss:20b ã‚’ä½¿ç”¨ï¼ˆOllamaï¼‰\n",
    "  - Embedding ãƒ¢ãƒ‡ãƒ«ã¯ ruri-v3-310m ã‚’ä½¿ç”¨ï¼ˆSentence Transformersï¼‰\n",
    "  - Reranker ãƒ¢ãƒ‡ãƒ«ã¯ cl-nagoya/ruri-v3-reranker-310m ã‚’ä½¿ç”¨ï¼ˆSentence Transformersï¼‰\n",
    "- JAXAï¼ˆå®‡å®™èˆªç©ºç ”ç©¶é–‹ç™ºæ©Ÿæ§‹ï¼‰ã®ãƒªãƒã‚¸ãƒˆãƒªã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã¦èª­ã¿è¾¼ã¿\n",
    "> [äº•æ¾¤å…‹å½¦, å¸‚å·ä¿¡ä¸€éƒ, é«˜é€Ÿå›è»¢ãƒ›ã‚¤ãƒ¼ãƒ«: é«˜é€Ÿå›è»¢ãƒ›ã‚¤ãƒ¼ãƒ«é–‹ç™ºã‚’é€šã—ã¦ã®çŸ¥è¦‹, å®‡å®™èˆªç©ºç ”ç©¶é–‹ç™ºæ©Ÿæ§‹ç ”ç©¶é–‹ç™ºå ±å‘Š, 2008](https://jaxa.repo.nii.ac.jp/records/2149)\n",
    "- ãƒ‡ãƒ¼ã‚¿ã®å‰å‡¦ç†\n",
    "  - markdown ã«å¤‰æ›ï¼ˆMarkItDown ã‚’ä½¿ç”¨ï¼‰\n",
    "  - Unicodeæ­£è¦åŒ– (NFKC), 1æ–‡å­—è¡Œãƒ–ãƒ­ãƒƒã‚¯ã®é™¤å», ç©ºè¡Œåœ§ç¸®\n",
    "  - ãƒãƒ£ãƒ³ã‚¯åˆ†å‰²\n",
    "    - LangChain ã® SpacyTextSplitter ã‚’ä½¿ç”¨\n",
    "    - spaCy ã®æ—¥æœ¬èªãƒ¢ãƒ‡ãƒ«ã¯ã€ja_ginza ã‚’ä½¿ç”¨\n",
    "- ãƒ™ã‚¯ãƒˆãƒ«ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã®æ§‹ç¯‰ï¼ˆChromaDB, ã‚¤ãƒ³ãƒ¡ãƒ¢ãƒªï¼‰\n",
    "- æ¤œç´¢æ©Ÿèƒ½ã®å®Ÿè£…\n",
    "  - ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ¤œç´¢ @ BM25ï¼ˆspaCyã§å½¢æ…‹ç´ è§£æã®å‰å‡¦ç†ãŒå¿…è¦ï¼‰\n",
    "  - Embedding model ã«ã‚ˆã‚‹ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯æ¤œç´¢\n",
    "  - ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰æ¤œç´¢\n",
    "  - Reranker ã«ã‚ˆã‚‹å†é †ä½ä»˜ã‘\n",
    "  - æ¤œç´¢æ©Ÿèƒ½ã‚’LLM ã® tool ã¨ã—ã¦å®šç¾©\n",
    "- LangGraph ã«ã‚ˆã‚‹ Workflow ã®å®Ÿè£…\n",
    "  1. ãƒ¦ãƒ¼ã‚¶ã®è³ªå•ã‚’å…¥åŠ›ã€‚\n",
    "  2. ãƒ¦ãƒ¼ã‚¶ã®è³ªå•ã«å›ç­”ã™ã‚‹ãŸã‚ã®ã‚¿ã‚¹ã‚¯åˆ†å‰², ä½œæˆã€‚\n",
    "  3. tool ã«ã‚ˆã‚‹æ¤œç´¢ã€‚\n",
    "  4. tool ã«ã‚ˆã‚‹æ¤œç´¢ã‚’çµ‚ãˆã¦å›ç­”ä½œæˆã«é€²ã‚€ã‹åˆ¤æ–­ã€‚å†èª¿æŸ»ãªã‚‰ 3 ã«æˆ»ã‚‹ã€‚\n",
    "  5. ãƒ¦ãƒ¼ã‚¶ã¸ã®å›ç­”ã®ä½œæˆã¨æç¤ºã€‚\n",
    "\n",
    "#### Gradio UI æ©Ÿèƒ½\n",
    "- Google Colab ç’°å¢ƒã§ Gradio ã‚’ç«‹ã¡ä¸Šã’ã™ã‚‹ã€‚\n",
    "- Gradio UI ã« PDFãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ‰ãƒ©ãƒƒã‚°ã‚¢ãƒ³ãƒ‰ãƒ‰ãƒ­ãƒƒãƒ—ã§å…¥åŠ›ã™ã‚‹ã€‚\n",
    "- PDF ãƒ•ã‚¡ã‚¤ãƒ«ãŒå…¥åŠ›ã•ã‚ŒãŸã‚‰ã€markdown ã«å¤‰æ›ãƒ»å‰å‡¦ç†ã™ã‚‹ã€‚\n",
    "- Gradio UI ã«ãƒãƒ£ãƒƒãƒˆã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ï¼ˆãƒ¦ãƒ¼ã‚¶å…¥åŠ› / AI å›ç­”ï¼‰ã‚’è¨­ã‘ã‚‹ã€‚\n",
    "- ãƒãƒ£ãƒƒãƒˆã‚¤ãƒ³ã‚¿ãƒ•ã‚§ãƒ¼ã‚¹ã¨ã¯åˆ¥ã«ã€AI ã®æ€è€ƒéç¨‹ã‚’è¡¨ç¤ºã™ã‚‹ã€‚ï¼ˆã‚¿ã‚¹ã‚¯ã®å†…å®¹ã€æ¤œç´¢ã®å†…å®¹ã€åˆ¤æ–­çµæœï¼‰\n",
    "- AI ãƒãƒ£ãƒƒãƒˆã¯ã€ãƒãƒ«ãƒã‚¿ãƒ¼ãƒ³å¯¾å¿œã€ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°å¿œç­”ã‚’è¡Œã†ã€‚\n",
    "- ãƒ¦ãƒ¼ã‚¶å…¥åŠ› / AI å›ç­”ã¨ã‚‚ã€ç”»é¢ã‚’ã‚¹ã‚¯ãƒ­ãƒ¼ãƒ«ã—ã¦ã€ä¼šè©±å±¥æ­´å«ã‚ãŸå…¨æ–‡ã‚’ç¢ºèªã§ãã‚‹ã‚ˆã†ã«ã™ã‚‹ã€‚\n",
    "- **ä¼šè©±ã‚¯ãƒªã‚¢ãƒœã‚¿ãƒ³**: PDF ã‚’ä¿æŒã—ãŸã¾ã¾ä¼šè©±å±¥æ­´ã‚’ãƒªã‚»ãƒƒãƒˆã—ã€æ–°ã—ã„ã‚¹ãƒ¬ãƒƒãƒ‰ã§å†è³ªå•ã§ãã‚‹ã€‚\n",
    "- **ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆè¨­å®š**: ã‚¢ã‚³ãƒ¼ãƒ‡ã‚£ã‚ªãƒ³å†…ã§è‡ªç”±ã«ç·¨é›†å¯èƒ½ã€‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯ã€Œæ—¥æœ¬èªã§å›ç­”ã—ã¦ãã ã•ã„ã€‚ã€\n",
    "- **Temperature ã‚¹ãƒ©ã‚¤ãƒ€ãƒ¼**: LLM ã®å¿œç­”ã®æ­£ç¢ºã•/å‰µé€ æ€§ã‚’ 0.0ã€œ1.0 ã§èª¿æ•´ã§ãã‚‹ã€‚\n",
    "- **ç”Ÿæˆåœæ­¢ãƒœã‚¿ãƒ³**: ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ä¸­ã®å¿œç­”ã‚’é€”ä¸­ã§ä¸­æ–­ã§ãã‚‹ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Google Colab ã«å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã™ã‚‹ã€‚\n# 1è¡Œã«ã¾ã¨ã‚ã‚‹ã“ã¨ã§ pip ãŒå…¨ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã®ä¾å­˜é–¢ä¿‚ã‚’ä¸€æ‹¬è§£æ±ºã™ã‚‹ã€‚\n# NOTE: Colab ã§ã¯ uv ã§ã¯ãªã pip ã‚’ä½¿ã†ã€‚uv ã¯ä¾å­˜è§£æ±ºã®éç¨‹ã§\n#       numpy ç­‰ã‚’ã‚¢ãƒƒãƒ—ã‚°ãƒ¬ãƒ¼ãƒ‰ã—ã€ãƒ—ãƒªã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«æ¸ˆã¿ã® scipy ç­‰ã‚’å£Šã™ãŸã‚ã€‚\n# NOTE: langchain é–¢é€£ã¯ 1.x ç³»ã«æ˜ç¤ºçš„ã«æŒ‡å®šã™ã‚‹ã€‚\n#       Colab ãƒ—ãƒªã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã® 0.3.x ãŒæ®‹ã‚‹ã¨ langchain-mcp-adapters ãŒå‹•ä½œã—ãªã„ã€‚\n# Pythonã®ãƒªã‚¹ãƒˆã¨ã—ã¦å®šç¾©ã™ã‚‹ã“ã¨ã§ã€Pylanceã®è­¦å‘Šã‚’é˜²ãã€å¯èª­æ€§ã‚’é«˜ã‚ã‚‹ã€‚\n\n# fmt: off\npkgs = [\n    \"ollama\", \"langchain-ollama\",\n    \"langchain>=1.2.8\", \"langchain-core>=1.2.8\", \"langgraph>=1.0.7\",\n    \"markitdown[all]\", \"chromadb\", \"langchain-text-splitters>=0.3\",\n    \"spacy\", \"ginza\", \"ja-ginza\", \"rank-bm25\",\n    \"sentence-transformers\",\n    \"gradio>=6.0\",\n]\n# fmt: on\n\n# ãƒªã‚¹ãƒˆã‚’çµåˆã—ã¦ pip ã«æ¸¡ã™\n# magic commandå†…ã§ {å¤‰æ•°} ã‚’ä½¿ã†ã¨å±•é–‹ã•ã‚Œã‚‹æ©Ÿèƒ½ã‚’åˆ©ç”¨\n%pip install -U -q {\" \".join(pkgs)}"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Google Colab ã« Ollama ã‚’ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—**\n",
    "- è©³ç´°ã¯ [01_connect_oss_llm.ipynb](01_connect_oss_llm.ipynb) ã‚’å‚ç…§ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selecting previously unselected package zstd.\n",
      "(Reading database ... 121852 files and directories currently installed.)\n",
      "Preparing to unpack .../zstd_1.4.8+dfsg-3build1_amd64.deb ...\n",
      "Unpacking zstd (1.4.8+dfsg-3build1) ...\n",
      "Setting up zstd (1.4.8+dfsg-3build1) ...\n",
      "Processing triggers for man-db (2.10.2-1) ...\n",
      ">>> Installing ollama to /usr/local\n",
      ">>> Downloading ollama-linux-amd64.tar.zst\n",
      "######################################################################## 100.0%\n",
      ">>> Creating ollama user...\n",
      ">>> Adding ollama user to video group...\n",
      ">>> Adding current user to ollama group...\n",
      ">>> Creating ollama systemd service...\n",
      "\u001b[1m\u001b[31mWARNING:\u001b[m systemd is not running\n",
      "\u001b[1m\u001b[31mWARNING:\u001b[m Unable to detect NVIDIA/AMD GPU. Install lspci or lshw to automatically detect and install GPU dependencies.\n",
      ">>> The Ollama API is now available at 127.0.0.1:11434.\n",
      ">>> Install complete. Run \"ollama\" from the command line.\n",
      "success                                                     \n",
      "gpt-oss:20b: Done!\n",
      "  Model\n",
      "    architecture        gptoss    \n",
      "    parameters          20.9B     \n",
      "    context length      131072    \n",
      "    embedding length    2880      \n",
      "    quantization        MXFP4     \n",
      "\n",
      "  Capabilities\n",
      "    completion    \n",
      "    tools         \n",
      "    thinking      \n",
      "\n",
      "  Parameters\n",
      "    temperature    1    \n",
      "\n",
      "  License\n",
      "    Apache License               \n",
      "    Version 2.0, January 2004    \n",
      "    ...                          \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Ollama ã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ãƒ»èµ·å‹•ãƒ»ãƒ¢ãƒ‡ãƒ«ã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰\n",
    "# è©³ç´°ã¯ 01_connect_oss_llm.ipynb ã‚’å‚ç…§\n",
    "import subprocess\n",
    "import time\n",
    "import ollama  # type: ignore\n",
    "\n",
    "!apt-get install -y -qq zstd\n",
    "!curl -fsSL https://ollama.com/install.sh | sh\n",
    "\n",
    "process = subprocess.Popen(\n",
    "    [\"ollama\", \"serve\"],\n",
    "    stdout=subprocess.DEVNULL,\n",
    "    stderr=subprocess.DEVNULL,\n",
    ")\n",
    "time.sleep(5)\n",
    "\n",
    "\n",
    "def ollama_pull(model: str) -> None:\n",
    "    \"\"\"Ollama ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã€é€²æ—ã‚’ã‚¤ãƒ³ãƒ©ã‚¤ãƒ³è¡¨ç¤ºã™ã‚‹ã€‚\"\"\"\n",
    "    for progress in ollama.pull(model, stream=True):\n",
    "        status = progress.get(\"status\", \"\")\n",
    "        total = progress.get(\"total\") or 0\n",
    "        completed = progress.get(\"completed\") or 0\n",
    "        if total:\n",
    "            line = f\"{status}: {completed / total:.0%}\"\n",
    "        else:\n",
    "            line = status\n",
    "        print(f\"\\r{line:<60}\", end=\"\", flush=True)\n",
    "    print(f\"\\n{model}: Done!\")\n",
    "\n",
    "\n",
    "model_name = \"gpt-oss:20b\"\n",
    "ollama_pull(model_name)\n",
    "!ollama show {model_name}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ChatOllama ã§ LLM ã«æ¥ç¶š**\n",
    "- è©³ç´°ã¯ [01_connect_oss_llm.ipynb](01_connect_oss_llm.ipynb) ã‚’å‚ç…§ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ChatOllama ã§ LLM ã«æ¥ç¶šã™ã‚‹ã€‚\n",
    "from langchain_ollama import ChatOllama  # type: ignore\n",
    "\n",
    "llm = ChatOllama(\n",
    "    model=\"gpt-oss:20b\",\n",
    "    num_ctx=16384,\n",
    "    num_predict=-1,\n",
    "    temperature=0.8,\n",
    "    top_k=40,\n",
    "    top_p=0.9,\n",
    "    repeat_penalty=1.1,\n",
    "    reasoning=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Embedding ãƒ¢ãƒ‡ãƒ«ï¼ˆruri-v3-310mï¼‰ã¨ Reranker ãƒ¢ãƒ‡ãƒ«ã®ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—**\n",
    "- è©³ç´°ã¯ [04_AI_Embedding_RAG.ipynb](04_AI_Embedding_RAG.ipynb) ã‚’å‚ç…§ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:104: UserWarning: \n",
      "Error while fetching `HF_TOKEN` secret value from your vault: 'Requesting secret HF_TOKEN timed out. Secrets can only be fetched when running from the Colab UI.'.\n",
      "You are not authenticated with the Hugging Face Hub in this notebook.\n",
      "If the error persists, please let us know by opening an issue on GitHub (https://github.com/huggingface/huggingface_hub/issues/new).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b9691018f2345b38bc911bc4b7f0bb7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/229 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9b3c65ebb50453db434db0a12aa7e56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/205 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "261a07c43cbd47e39d540d6056952fa1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b66384c87c540cfa95b1a82b8a51ce8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/54.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n",
      "WARNING:huggingface_hub.utils._http:Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "436f38976bb8456a8cdd871f40d5cd05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1475d03a2324705a1f63047c0cd0cec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.26G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76b17143c7d84856969b8b2d43316a4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/152 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f33005f297d546b58b24966b5b98fa25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33568697025d42f7b4614a0ad2c62fc8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac04d66f6fd7435a824c9eb63dffac5b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/1.83M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "845fe2272823436a8da11b424dd589da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/968 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed34b6074abd45eeac12b4e6717c7b83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/296 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding dim: 768\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f5a09191b2c4b45b0a77a5913a5f5d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21b99505304e4c5f871d3b6bcd80871c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.26G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76ca3aa5287e4fbe80cc69b05261bc7e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/156 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66cad922920a4850a14acfd77df9dfb3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "edaee711eb9f400cbd0524fb4d3b3256",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "770a85cfaef440da860522ec5adef35f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/1.83M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2306922c9a35475f9642a4ab801795b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/968 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e5be6339eab4390ba168e7d6e461961",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reranker model loaded.\n"
     ]
    }
   ],
   "source": [
    "# Embedding: ruri-v3-310m (Sentence Transformers çµŒç”±)\n",
    "from langchain_core.embeddings import Embeddings  # type: ignore\n",
    "from sentence_transformers import SentenceTransformer, CrossEncoder  # type: ignore\n",
    "\n",
    "\n",
    "class RuriEmbeddings(Embeddings):\n",
    "    \"\"\"ruri-v3 ã‚’ LangChain ã® Embeddings ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ã§ãƒ©ãƒƒãƒ—ã™ã‚‹ã€‚\"\"\"\n",
    "\n",
    "    def __init__(self, model_name: str = \"cl-nagoya/ruri-v3-310m\") -> None:\n",
    "        self.model = SentenceTransformer(model_name)\n",
    "\n",
    "    def embed_documents(self, texts: list[str]) -> list[list[float]]:\n",
    "        prefixed = [f\"æ¤œç´¢æ–‡æ›¸: {t}\" for t in texts]\n",
    "        return self.model.encode(prefixed).tolist()\n",
    "\n",
    "    def embed_query(self, text: str) -> list[float]:\n",
    "        return self.model.encode(f\"æ¤œç´¢ã‚¯ã‚¨ãƒª: {text}\").tolist()\n",
    "\n",
    "\n",
    "embeddings = RuriEmbeddings()\n",
    "test_vec = embeddings.embed_query(\"ãƒ†ã‚¹ãƒˆæ–‡ã§ã™\")\n",
    "print(f\"Embedding dim: {len(test_vec)}\")\n",
    "\n",
    "# Reranker: cl-nagoya/ruri-v3-reranker-310m\n",
    "reranker = CrossEncoder(\"cl-nagoya/ruri-v3-reranker-310m\")\n",
    "print(\"Reranker model loaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "**PDF å‰å‡¦ç†ãƒ»ãƒãƒ£ãƒ³ã‚¯åˆ†å‰²ãƒ»ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æ§‹ç¯‰é–¢æ•°ã®å®šç¾©**\n- Gradio UI ã‹ã‚‰ PDF ãŒã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚ŒãŸéš›ã«å‘¼ã³å‡ºã™ã€‚\n- markdown å¤‰æ› â†’ ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚° â†’ ãƒãƒ£ãƒ³ã‚¯åˆ†å‰² â†’ ChromaDB + BM25 ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æ§‹ç¯‰ã‚’ä¸€æ‹¬ã§è¡Œã†ã€‚\n- æ¤œç´¢ç”¨ãƒ‡ãƒ¼ã‚¿ã¯ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ãƒ¬ãƒ™ãƒ«ã®ã‚°ãƒ­ãƒ¼ãƒãƒ«å¤‰æ•°ã«æ ¼ç´ã™ã‚‹ï¼ˆColab ã¯ã‚·ãƒ³ã‚°ãƒ«ãƒ¦ãƒ¼ã‚¶å‰æï¼‰ã€‚\n- è©³ç´°ã¯ [04_AI_Embedding_RAG.ipynb](04_AI_Embedding_RAG.ipynb) ãŠã‚ˆã³ [05_AI_Agent_Workflow_RAG.ipynb](05_AI_Agent_Workflow_RAG.ipynb) ã‚’å‚ç…§ã€‚"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# PDF å‰å‡¦ç†ãƒ»ãƒãƒ£ãƒ³ã‚¯åˆ†å‰²ãƒ»ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æ§‹ç¯‰\n# Gradio ã‹ã‚‰ PDF ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰æ™‚ã« process_and_index_pdf() ã‚’å‘¼ã³å‡ºã™ã€‚\nimport re\nimport unicodedata\nimport numpy as np  # type: ignore\nimport spacy  # type: ignore\nfrom markitdown import MarkItDown  # type: ignore\nfrom langchain_text_splitters import SpacyTextSplitter  # type: ignore\nfrom rank_bm25 import BM25Okapi  # type: ignore\nimport chromadb  # type: ignore\n\nnlp = spacy.load(\"ja_ginza\", disable=[\"parser\", \"ner\"])\n\n# --- ãƒãƒ£ãƒ³ã‚¯åˆ†å‰²ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ ---\nCHUNK_SIZE = 500\nCHUNK_OVERLAP = 100\nBLOCK_MAX_BYTES = 40_000\nBLOCK_OVERLAP_CHARS = CHUNK_SIZE\n\n# --- ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ãƒ¬ãƒ™ãƒ«ã®ã‚°ãƒ­ãƒ¼ãƒãƒ«å¤‰æ•°ï¼ˆPDF ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰æ™‚ã«æ›´æ–°ã•ã‚Œã‚‹ï¼‰ ---\n_chunks: list[str] = []\n_bm25: BM25Okapi | None = None\n_collection = None\n_chroma_client = chromadb.Client()\n\n\ndef tokenize(text: str) -> list[str]:\n    \"\"\"spaCy ã§å½¢æ…‹ç´ è§£æã—ã€BM25 ç”¨ã®ãƒˆãƒ¼ã‚¯ãƒ³ãƒªã‚¹ãƒˆã‚’è¿”ã™ã€‚\"\"\"\n    doc = nlp(text)\n    tokens = []\n    include_pos = {\"NOUN\", \"VERB\", \"ADJ\", \"PROPN\", \"NUM\"}\n    for token in doc:\n        if token.pos_ not in include_pos:\n            continue\n        if token.is_stop:\n            continue\n        lemma = token.lemma_\n        if len(lemma) == 1 and re.match(r\"[ã-ã‚“\\u30fc!-/:-@\\[-`{-~]\", lemma):\n            continue\n        tokens.append(lemma)\n    return tokens\n\n\ndef clean_pdf_text(text: str) -> str:\n    \"\"\"1æ–‡å­—è¡Œãƒ–ãƒ­ãƒƒã‚¯ã®é™¤å» + ç©ºè¡Œåœ§ç¸®ã€‚\"\"\"\n    text = re.sub(\n        r\"(^[^\\S\\n]*\\S[^\\S\\n]*$\\n?){3,}\",\n        \"\\n\",\n        text,\n        flags=re.MULTILINE,\n    )\n    text = re.sub(r\"\\n{3,}\", \"\\n\\n\", text)\n    return text.strip()\n\n\ndef split_into_safe_blocks(\n    text: str,\n    max_bytes: int = BLOCK_MAX_BYTES,\n    overlap_chars: int = BLOCK_OVERLAP_CHARS,\n) -> list[str]:\n    \"\"\"ãƒ†ã‚­ã‚¹ãƒˆã‚’æ®µè½åŒºåˆ‡ã‚Šã§ max_bytes ä»¥ä¸‹ã®ãƒ–ãƒ­ãƒƒã‚¯ã«åˆ†å‰²ã™ã‚‹ã€‚\"\"\"\n    paragraphs = text.split(\"\\n\\n\")\n    blocks: list[str] = []\n    current = \"\"\n    for para in paragraphs:\n        candidate = current + \"\\n\\n\" + para if current else para\n        if len(candidate.encode(\"utf-8\")) > max_bytes and current:\n            blocks.append(current)\n            current = current[-overlap_chars:] + \"\\n\\n\" + para\n        else:\n            current = candidate\n    if current:\n        blocks.append(current)\n    return blocks\n\n\ndef process_and_index_pdf(filepath: str) -> str:\n    \"\"\"PDF ã‚’ markdown å¤‰æ›ãƒ»å‰å‡¦ç†ã—ã€ãƒãƒ£ãƒ³ã‚¯åˆ†å‰²ãƒ»ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æ§‹ç¯‰ã‚’è¡Œã†ã€‚\n\n    ã‚°ãƒ­ãƒ¼ãƒãƒ«å¤‰æ•° _chunks, _bm25, _collection ã‚’æ›´æ–°ã™ã‚‹ã€‚\n\n    Args:\n        filepath: PDF ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹ã€‚\n\n    Returns:\n        str: å‰å‡¦ç†æ¸ˆã¿ã®ãƒ†ã‚­ã‚¹ãƒˆå…¨æ–‡ã€‚\n    \"\"\"\n    global _chunks, _bm25, _collection\n\n    # PDF â†’ markdown å¤‰æ› + ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°\n    md_converter = MarkItDown()\n    result = md_converter.convert(filepath)\n    text = unicodedata.normalize(\"NFKC\", result.text_content)\n    text = clean_pdf_text(text)\n    print(f\"ãƒ†ã‚­ã‚¹ãƒˆæ–‡å­—æ•°: {len(text)}\")\n\n    # ãƒãƒ£ãƒ³ã‚¯åˆ†å‰²\n    text_splitter = SpacyTextSplitter(\n        separator=\"\\n\\n\",\n        pipeline=\"ja_ginza\",\n        chunk_size=CHUNK_SIZE,\n        chunk_overlap=CHUNK_OVERLAP,\n    )\n    blocks = split_into_safe_blocks(text)\n    _chunks = []\n    for block in blocks:\n        _chunks.extend(text_splitter.split_text(block))\n    print(f\"ãƒãƒ£ãƒ³ã‚¯æ•°: {len(_chunks)}\")\n\n    # ChromaDBï¼ˆæ—¢å­˜ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ãŒã‚ã‚Œã°å‰Šé™¤ã—ã¦å†ä½œæˆï¼‰\n    try:\n        _chroma_client.delete_collection(\"rag_docs\")\n    except Exception:\n        pass\n    _collection = _chroma_client.create_collection(\n        name=\"rag_docs\",\n        metadata={\"hnsw:space\": \"cosine\"},\n    )\n    chunk_embs = embeddings.embed_documents(_chunks)\n    _collection.add(\n        ids=[f\"chunk_{i}\" for i in range(len(_chunks))],\n        documents=_chunks,\n        embeddings=chunk_embs,\n    )\n    print(f\"ChromaDB: {_collection.count()} ä»¶æ ¼ç´\")\n\n    # BM25\n    tokenized = [tokenize(c) for c in _chunks]\n    _bm25 = BM25Okapi(tokenized)\n    print(f\"BM25 ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æ§‹ç¯‰å®Œäº†: {len(tokenized)} ä»¶\")\n\n    return text\n\n\nprint(\"å‰å‡¦ç†ãƒ»ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æ§‹ç¯‰é–¢æ•°ã‚’å®šç¾©ã—ã¾ã—ãŸã€‚\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "**æ¤œç´¢æ©Ÿèƒ½ã®å®Ÿè£…ï¼ˆBM25 + ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯ + ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ + Rerankerï¼‰ã¨ tool å®šç¾©**\n- è©³ç´°ã¯ [04_AI_Embedding_RAG.ipynb](04_AI_Embedding_RAG.ipynb) ã‚’å‚ç…§ã€‚"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# æ¤œç´¢é–¢æ•°ã®å®šç¾©ï¼ˆBM25, ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯, ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰, Rerankerï¼‰+ tool å®šç¾©\nfrom langchain_core.tools import tool  # type: ignore\n\nRETRIEVAL_TOP_K = 20\nRERANK_TOP_K = 5\nBM25_WEIGHT = 0.3\nMAX_RETURN_CHARS = 8000\n\n\ndef search_bm25(query: str, top_k: int = RETRIEVAL_TOP_K) -> list[dict]:\n    \"\"\"BM25 ã«ã‚ˆã‚‹ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ¤œç´¢ã‚’è¡Œã†ã€‚\"\"\"\n    if _bm25 is None:\n        return []\n    tokenized_query = tokenize(query)\n    scores = _bm25.get_scores(tokenized_query)\n    top_indices = np.argsort(scores)[::-1][:top_k]\n    return [\n        {\n            \"rank\": rank + 1,\n            \"chunk_id\": int(idx),\n            \"score\": float(scores[idx]),\n            \"text\": _chunks[idx],\n        }\n        for rank, idx in enumerate(top_indices)\n        if scores[idx] > 0\n    ]\n\n\ndef search_semantic(query: str, top_k: int = RETRIEVAL_TOP_K) -> list[dict]:\n    \"\"\"Embedding model ã«ã‚ˆã‚‹ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯æ¤œç´¢ã‚’è¡Œã†ã€‚\"\"\"\n    if _collection is None:\n        return []\n    query_embedding = embeddings.embed_query(query)\n    results = _collection.query(query_embeddings=[query_embedding], n_results=top_k)\n    return [\n        {\n            \"rank\": rank + 1,\n            \"chunk_id\": int(doc_id.split(\"_\")[1]),\n            \"score\": 1.0 - dist,\n            \"text\": doc,\n        }\n        for rank, (doc_id, doc, dist) in enumerate(\n            zip(results[\"ids\"][0], results[\"documents\"][0], results[\"distances\"][0])\n        )\n    ]\n\n\ndef search_hybrid(\n    query: str, top_k: int = RETRIEVAL_TOP_K, bm25_weight: float = BM25_WEIGHT\n) -> list[dict]:\n    \"\"\"BM25 ã¨ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯æ¤œç´¢ã® RRF ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰æ¤œç´¢ã‚’è¡Œã†ã€‚\"\"\"\n    k = 60\n    bm25_results = search_bm25(query, top_k=top_k)\n    semantic_results = search_semantic(query, top_k=top_k)\n\n    scores: dict[int, float] = {}\n    texts: dict[int, str] = {}\n\n    for r in bm25_results:\n        cid = r[\"chunk_id\"]\n        scores[cid] = scores.get(cid, 0) + bm25_weight / (k + r[\"rank\"])\n        texts[cid] = r[\"text\"]\n\n    semantic_weight = 1.0 - bm25_weight\n    for r in semantic_results:\n        cid = r[\"chunk_id\"]\n        scores[cid] = scores.get(cid, 0) + semantic_weight / (k + r[\"rank\"])\n        texts[cid] = r[\"text\"]\n\n    sorted_ids = sorted(scores, key=lambda cid: scores[cid], reverse=True)[:top_k]\n    return [\n        {\"rank\": rank + 1, \"chunk_id\": cid, \"score\": scores[cid], \"text\": texts[cid]}\n        for rank, cid in enumerate(sorted_ids)\n    ]\n\n\ndef rerank(query: str, results: list[dict], top_k: int = RERANK_TOP_K) -> list[dict]:\n    \"\"\"Reranker (CrossEncoder) ã§æ¤œç´¢çµæœã‚’å†é †ä½ä»˜ã‘ã™ã‚‹ã€‚\"\"\"\n    if not results:\n        return []\n    pairs = [(query, r[\"text\"]) for r in results]\n    scores = reranker.predict(pairs)\n    ranked_indices = np.argsort(scores)[::-1][:top_k]\n    return [\n        {\n            \"rank\": rank + 1,\n            \"chunk_id\": results[idx][\"chunk_id\"],\n            \"score\": float(scores[idx]),\n            \"text\": results[idx][\"text\"],\n        }\n        for rank, idx in enumerate(ranked_indices)\n    ]\n\n\n@tool\ndef search_document(query: str) -> str:\n    \"\"\"å¤–éƒ¨ãƒŠãƒ¬ãƒƒã‚¸ãƒ™ãƒ¼ã‚¹ã‹ã‚‰ã€ã‚¯ã‚¨ãƒªã«é–¢é€£ã™ã‚‹æƒ…å ±ã‚’æ¤œç´¢ãƒ»å–å¾—ã—ã¾ã™ã€‚\n    ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•ã«å¯¾ã—ã€å…·ä½“çš„ãªäº‹å®Ÿã€ãƒ‡ãƒ¼ã‚¿ã€ã‚ã‚‹ã„ã¯è©³ç´°ãªæ–‡è„ˆãŒå¿…è¦ãªå ´åˆã€\n    è‡ªèº«ã®çŸ¥è­˜ã ã‘ã§å›ç­”ã›ãšã«å¿…ãšã“ã®ãƒ„ãƒ¼ãƒ«ã‚’ä½¿ç”¨ã—ã¦ãã ã•ã„ã€‚\n\n    Args:\n        query: æ¤œç´¢ã—ãŸã„å†…å®¹ã‚’è¡¨ã™ã€å…·ä½“çš„ã‹ã¤å®Œå…¨ãªæ–‡ç« ï¼ˆæ—¥æœ¬èªï¼‰ã€‚\n    \"\"\"\n    try:\n        hybrid_results = search_hybrid(query)\n        reranked = rerank(query, hybrid_results)\n    except Exception as e:\n        return f\"æ¤œç´¢ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}\"\n\n    if not reranked:\n        return \"æ¤œç´¢çµæœãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚\"\n\n    passages = []\n    total_chars = 0\n    for r in reranked:\n        passage = f\"[ãƒãƒ£ãƒ³ã‚¯ {r['chunk_id']}] (ã‚¹ã‚³ã‚¢: {r['score']:.4f})\\n{r['text']}\"\n        total_chars += len(passage)\n        if total_chars > MAX_RETURN_CHARS:\n            break\n        passages.append(passage)\n    return \"\\n\\n---\\n\\n\".join(passages)\n\n\nsearch_tool = search_document\nprint(f\"æ¤œç´¢é–¢æ•°ã‚’å®šç¾©ã—ã¾ã—ãŸã€‚RAG Tool: {search_tool.name}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "**LangGraph ã«ã‚ˆã‚‹ Workflow ã®å®Ÿè£…**\n\nnotebook 05 ã® Workflow ã‚’ Gradio UI å¯¾å¿œã«é©å¿œã™ã‚‹ã€‚\n- Workflow ã®ãƒãƒ¼ãƒ‰å‡¦ç†ã‚’å€‹åˆ¥ã® async é–¢æ•°ã¨ã—ã¦å®Ÿè£…ã—ã€Gradio ã‹ã‚‰ç›´æ¥å‘¼ã³å‡ºã™ã€‚\n  - ã“ã‚Œã«ã‚ˆã‚Šã€æ€è€ƒéç¨‹ã®ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ è¡¨ç¤ºã¨æœ€çµ‚å›ç­”ã®ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ã‚’å®Ÿç¾ã™ã‚‹ã€‚\n- LangGraph ã®ã‚°ãƒ©ãƒ•å®šç¾©ã¯ Workflow ã®å¯è¦–åŒ–ã«ä½¿ç”¨ã™ã‚‹ã€‚\n- è©³ç´°ã¯ [05_AI_Agent_Workflow_RAG.ipynb](05_AI_Agent_Workflow_RAG.ipynb) ã‚’å‚ç…§ã€‚\n\n**Workflow ã®æµã‚Œ**\n1. **task_planning**: ãƒ¦ãƒ¼ã‚¶ã®è³ªå•ã‚’å—ã‘å–ã‚Šã€å›ç­”ã«å¿…è¦ãªã‚µãƒ–ã‚¿ã‚¹ã‚¯ï¼ˆç›®çš„ï¼‹æ¤œç´¢ã‚¯ã‚¨ãƒªï¼‰ã‚’æ§‹é€ åŒ–ã—ã¦ä½œæˆã™ã‚‹ã€‚\n2. **doc_search**: å„ã‚µãƒ–ã‚¿ã‚¹ã‚¯ã®æ¤œç´¢ã‚¯ã‚¨ãƒªã‚’ `search_document` ãƒ„ãƒ¼ãƒ«ã§å®Ÿè¡Œã—ã€ç›®çš„ã¨ç´ä»˜ã‘ãŸæ¤œç´¢çµæœã‚’è“„ç©ã™ã‚‹ã€‚\n3. **judge**: ã‚µãƒ–ã‚¿ã‚¹ã‚¯ã®ç›®çš„ã”ã¨ã«ã€æ¤œç´¢çµæœãŒååˆ†ã‹ã‚’ LLM ãŒåˆ¤æ–­ã™ã‚‹ã€‚ä¸è¶³ãªã‚‰è¿½åŠ ã‚µãƒ–ã‚¿ã‚¹ã‚¯ã‚’ç”Ÿæˆã—ã¦ doc_search ã«æˆ»ã‚‹ã€‚\n4. **generate_answer**: ç›®çš„ã”ã¨ã«æ•´ç†ã•ã‚ŒãŸæ¤œç´¢çµæœã‚’ã‚‚ã¨ã«ã€ãƒ¦ãƒ¼ã‚¶ã®è³ªå•ã«å¯¾ã™ã‚‹æœ€çµ‚å›ç­”ã‚’ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ç”Ÿæˆã™ã‚‹ã€‚"
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "**Pydantic / with_structured_output ã«ã‚ˆã‚‹æ§‹é€ åŒ–å‡ºåŠ›**\n\næ¤œç´¢çµæœã« LaTeX æ•°å¼ãŒå«ã¾ã‚Œã‚‹ãªã©ã®å ´åˆã€LLM ã®å‡ºåŠ›ãŒå¼•ããšã‚‰ã‚Œã¦ JSON ãƒ‘ãƒ¼ã‚¹ãŒæ©Ÿèƒ½ã—ãªã„å ´åˆãŒã‚ã£ãŸã€‚ã“ã®ãŸã‚ã€Pydantic / with_structured_output ã«ã‚ˆã‚‹æ§‹é€ åŒ–å‡ºåŠ›ã«ã‚ˆã£ã¦ã€æœ›ã¾ã—ã„å‡ºåŠ›ã‚’ LLM ã«å¼·åˆ¶ã•ã›ã‚‹ã€‚\n- `@model_validator(mode=\"after\")` ã§ã€LLM ã®å‡ºåŠ›ã®è«–ç†çŸ›ç›¾ã‚’è‡ªå‹•è£œæ­£ã™ã‚‹ã€‚\n- è©³ç´°ã¯ [05_AI_Agent_Workflow_RAG.ipynb](05_AI_Agent_Workflow_RAG.ipynb) ã‚’å‚ç…§ã€‚"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Workflow ã®çŠ¶æ…‹å®šç¾©ãƒ»Pydantic ãƒ¢ãƒ‡ãƒ«ãƒ»ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ»ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£é–¢æ•°\nfrom typing import TypedDict\nfrom pydantic import BaseModel, Field, model_validator  # type: ignore\nfrom langchain_core.messages import HumanMessage, SystemMessage  # type: ignore\nfrom langgraph.graph import StateGraph, START, END  # type: ignore\nfrom IPython.display import Image, display\n\n# --- ã‚°ãƒ­ãƒ¼ãƒãƒ«è¨­å®š ---\nMAX_LOOP_COUNT = 2  # judge â†’ doc_search å†èª¿æŸ»ãƒ«ãƒ¼ãƒ—ã®ä¸Šé™å›æ•°\n\n\n# --- Workflow ã®çŠ¶æ…‹ ---\nclass WorkflowState(TypedDict):\n    question: str\n    subtasks: list[dict]  # [{\"purpose\": str, \"queries\": [str]}]\n    search_results: list[str]  # ç›®çš„ã¨ç´ä»˜ã‘ãŸæ¤œç´¢çµæœ\n    answer: str\n    loop_count: int\n\n\n# --- with_structured_output ç”¨ã® Pydantic ãƒ¢ãƒ‡ãƒ« ---\nclass Subtask(BaseModel):\n    purpose: str = Field(description=\"ã“ã®ã‚µãƒ–ã‚¿ã‚¹ã‚¯ã§æ˜ã‚‰ã‹ã«ã—ãŸã„ã“ã¨\")\n    queries: list[str] = Field(description=\"æ¤œç´¢ã‚¯ã‚¨ãƒªã®ãƒªã‚¹ãƒˆ\")\n\n\nclass TaskPlanningResult(BaseModel):\n    subtasks: list[Subtask] = Field(description=\"ã‚µãƒ–ã‚¿ã‚¹ã‚¯ã®ãƒªã‚¹ãƒˆï¼ˆæœ€å¤§3å€‹ï¼‰\")\n\n\nclass JudgeResult(BaseModel):\n    sufficient: bool = Field(description=\"æƒ…å ±ãŒååˆ†ã‹ã©ã†ã‹\")\n    reason: str = Field(description=\"åˆ¤æ–­ç†ç”±ã‚’æ—¥æœ¬èªã§1æ–‡ã§\")\n    additional_subtasks: list[Subtask] | None = Field(\n        default=None,\n        description=\"ä¸è¶³æ™‚ã®è¿½åŠ ã‚µãƒ–ã‚¿ã‚¹ã‚¯\",\n    )\n\n    @model_validator(mode=\"after\")\n    def force_consistency(self):\n        \"\"\"LLM å‡ºåŠ›ã®è«–ç†çŸ›ç›¾ã‚’è‡ªå‹•è£œæ­£ã™ã‚‹ã€‚\"\"\"\n        if self.sufficient:\n            self.additional_subtasks = None\n        if not self.sufficient and not self.additional_subtasks:\n            self.sufficient = True\n            self.reason += (\n                \" (â€»è¿½åŠ èª¿æŸ»äº‹é …ãŒå…·ä½“åŒ–ã§ããªã‹ã£ãŸãŸã‚ã€ç¾çŠ¶ã®æƒ…å ±ã§å›ç­”ã—ã¾ã™)\"\n            )\n            self.additional_subtasks = None\n        return self\n\n\n# --- å„ãƒãƒ¼ãƒ‰ã®ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ ---\nSYSTEM_PROMPT_TASK_PLANNING = \"\"\"\\\nã‚ãªãŸã¯ãƒªã‚µãƒ¼ãƒãƒ—ãƒ©ãƒ³ãƒŠãƒ¼ã§ã™ã€‚\nãƒ¦ãƒ¼ã‚¶ã®è³ªå•ã«å›ç­”ã™ã‚‹ãŸã‚ã«ã€ãƒŠãƒ¬ãƒƒã‚¸ãƒ™ãƒ¼ã‚¹ï¼ˆæŠ€è¡“æ–‡æ›¸ï¼‰ã‚’æ¤œç´¢ã™ã‚‹ãŸã‚ã®ã‚µãƒ–ã‚¿ã‚¹ã‚¯ã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚\n\nã‚µãƒ–ã‚¿ã‚¹ã‚¯ã¯æœ€å¤§3å€‹ã¾ã§ã¨ã—ã¦ãã ã•ã„ã€‚\npurpose ã¯åˆ¤å®šã‚¹ãƒ†ãƒƒãƒ—ã§ã€Œã“ã®ç›®çš„ã«ååˆ†ãªæƒ…å ±ãŒå¾—ã‚‰ã‚ŒãŸã‹ã€ã‚’è©•ä¾¡ã™ã‚‹åŸºæº–ã«ãªã‚Šã¾ã™ã€‚\nå…·ä½“çš„ã‹ã¤æ˜ç¢ºã«æ›¸ã„ã¦ãã ã•ã„ã€‚\næ¤œç´¢ã‚¯ã‚¨ãƒªã¯ã€æŠ€è¡“æ–‡æ›¸ã‹ã‚‰é–¢é€£æƒ…å ±ã‚’æ¤œç´¢ã™ã‚‹ãŸã‚ã®æ—¥æœ¬èªã®å…·ä½“çš„ãªãƒ•ãƒ¬ãƒ¼ã‚ºã«ã—ã¦ãã ã•ã„ã€‚\n\"\"\"\n\nSYSTEM_PROMPT_JUDGE = \"\"\"\\\nã‚ãªãŸã¯ãƒªã‚µãƒ¼ãƒã®å“è³ªã‚’åˆ¤å®šã™ã‚‹å¯©æŸ»å“¡ã§ã™ã€‚\nãƒ¦ãƒ¼ã‚¶ã®è³ªå•ã¨æ¤œç´¢çµæœã‚’è¦‹ã¦ã€å›ç­”ã«ååˆ†ãªæƒ…å ±ãŒã‚ã‚‹ã‹åˆ¤æ–­ã—ã¦ãã ã•ã„ã€‚\næ¤œç´¢çµæœã«ã¯ã€ç›®çš„: ...ã€‘ã‚¿ã‚°ãŒä»˜ã„ã¦ã„ã¾ã™ã€‚\nå„ç›®çš„ã«ã¤ã„ã¦ååˆ†ãªæƒ…å ±ãŒå¾—ã‚‰ã‚Œã¦ã„ã‚‹ã‹ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚\n\nsufficient ãŒ true ãªã‚‰å›ç­”ä½œæˆã«é€²ã¿ã¾ã™ã€‚\nsufficient ãŒ false ãªã‚‰ã€ä¸è¶³ã—ã¦ã„ã‚‹ç›®çš„ã«ã¤ã„ã¦ additional_subtasks ã‚’ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚\n\"\"\"\n\nSYSTEM_PROMPT_GENERATE_ANSWER = \"\"\"\\\nã‚ãªãŸã¯ãƒªã‚µãƒ¼ãƒçµæœã‚’ã‚‚ã¨ã«å›ç­”ã™ã‚‹AIã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚\næ¤œç´¢çµæœã‚’å‚è€ƒã«ã€ãƒ¦ãƒ¼ã‚¶ã®è³ªå•ã«æ—¥æœ¬èªã§ä¸å¯§ã«å›ç­”ã—ã¦ãã ã•ã„ã€‚\nå›ç­”ã¯å¿…ãšæ¤œç´¢çµæœã«åŸºã¥ã„ã¦ä½œæˆã—ã€æ¤œç´¢çµæœã«å«ã¾ã‚Œãªã„æƒ…å ±ã¯å«ã‚ãªã„ã§ãã ã•ã„ã€‚\nå›ç­”ã®æœ€å¾Œã«ã€ä»¥ä¸‹ã®å½¢å¼ã§çµè«–ã‚’ã¾ã¨ã‚ã¦ãã ã•ã„ã€‚\n\n# çµè«–\n- ãƒ¦ãƒ¼ã‚¶ã®è³ªå•: ï¼ˆè³ªå•å†…å®¹ï¼‰\n- å›ç­”: ï¼ˆç°¡æ½”ãªå›ç­”ï¼‰\n\"\"\"\n\n\ndef _make_llm(temperature: float) -> ChatOllama:\n    \"\"\"æŒ‡å®šã—ãŸ temperature ã§ ChatOllama ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’ç”Ÿæˆã™ã‚‹ã€‚\"\"\"\n    return ChatOllama(\n        model=llm.model,\n        num_ctx=llm.num_ctx,\n        num_predict=llm.num_predict,\n        temperature=temperature,\n        top_k=llm.top_k,\n        top_p=llm.top_p,\n        repeat_penalty=llm.repeat_penalty,\n        reasoning=llm.reasoning,\n    )\n\n\nprint(\"Workflow ã®çŠ¶æ…‹å®šç¾©ãƒ»Pydantic ãƒ¢ãƒ‡ãƒ«ãƒ»ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’å®šç¾©ã—ã¾ã—ãŸã€‚\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Workflow å®Ÿè¡Œé–¢æ•°ã®å®šç¾©ï¼ˆGradio ã‹ã‚‰ç›´æ¥å‘¼ã³å‡ºã™ async é–¢æ•°ï¼‰\n# å„é–¢æ•°ã¯ (çµæœ, ãƒ­ã‚°æ–‡å­—åˆ—) ã®ã‚¿ãƒ—ãƒ«ã‚’è¿”ã—ã€Gradio å´ã§æ€è€ƒéç¨‹ã‚’è¡¨ç¤ºã™ã‚‹ã€‚\n\n\nasync def run_task_planning(\n    question: str, temperature: float\n) -> tuple[list[dict], str]:\n    \"\"\"ãƒ¦ãƒ¼ã‚¶ã®è³ªå•ã‚’åˆ†æã—ã€ã‚µãƒ–ã‚¿ã‚¹ã‚¯ï¼ˆç›®çš„ï¼‹æ¤œç´¢ã‚¯ã‚¨ãƒªï¼‰ã‚’ä½œæˆã™ã‚‹ã€‚\"\"\"\n    structured_llm = _make_llm(temperature).with_structured_output(TaskPlanningResult)\n    log = \"\"\n\n    try:\n        result = await structured_llm.ainvoke(\n            [\n                SystemMessage(content=SYSTEM_PROMPT_TASK_PLANNING),\n                HumanMessage(content=question),\n            ]\n        )\n        subtasks = [st.model_dump() for st in result.subtasks]\n    except Exception as e:\n        log += f\"æ§‹é€ åŒ–å‡ºåŠ›å¤±æ•— â†’ ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: {e}\\n\"\n        subtasks = [{\"purpose\": \"åŸºæœ¬èª¿æŸ»\", \"queries\": [question]}]\n\n    log += f\"ã‚µãƒ–ã‚¿ã‚¹ã‚¯æ•°: {len(subtasks)}\\n\"\n    for i, st in enumerate(subtasks):\n        log += f\"  {i + 1}. ç›®çš„: {st['purpose']}\\n\"\n        log += f\"     ã‚¯ã‚¨ãƒª: {st['queries']}\\n\"\n    return subtasks, log\n\n\nasync def run_doc_search(subtasks: list[dict]) -> tuple[list[str], str]:\n    \"\"\"å„ã‚µãƒ–ã‚¿ã‚¹ã‚¯ã®æ¤œç´¢ã‚¯ã‚¨ãƒªã‚’ search_document ãƒ„ãƒ¼ãƒ«ã§å®Ÿè¡Œã™ã‚‹ã€‚\"\"\"\n    results: list[str] = []\n    log = \"\"\n\n    for st in subtasks:\n        purpose = st[\"purpose\"]\n        log += f\"ç›®çš„: {purpose}\\n\"\n        for query in st[\"queries\"]:\n            log += f\"  æ¤œç´¢ä¸­: {query}\\n\"\n            try:\n                result = search_tool.invoke({\"query\": query})\n            except Exception as e:\n                log += f\"  ã‚¨ãƒ©ãƒ¼: {e}\\n\"\n                continue\n            if not result or result == \"æ¤œç´¢çµæœãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚\":\n                log += f\"  çµæœãªã—\\n\"\n                continue\n            results.append(f\"ã€ç›®çš„: {purpose}ã€‘\\nã€ã‚¯ã‚¨ãƒª: {query}ã€‘\\n{result}\")\n            log += f\"  â†’ å–å¾—å®Œäº†\\n\"\n    return results, log\n\n\nasync def run_judge(\n    question: str, search_results: list[str], temperature: float\n) -> tuple[JudgeResult, str]:\n    \"\"\"æ¤œç´¢çµæœãŒååˆ†ã‹ã‚’åˆ¤æ–­ã—ã€ä¸è¶³ãªã‚‰è¿½åŠ ã‚µãƒ–ã‚¿ã‚¹ã‚¯ã‚’ç”Ÿæˆã™ã‚‹ã€‚\"\"\"\n    results_text = \"\\n\\n\".join(search_results)\n    structured_llm = _make_llm(temperature).with_structured_output(JudgeResult)\n    log = \"\"\n\n    try:\n        judgment = await structured_llm.ainvoke(\n            [\n                SystemMessage(content=SYSTEM_PROMPT_JUDGE),\n                HumanMessage(content=f\"è³ªå•: {question}\\n\\næ¤œç´¢çµæœ:\\n{results_text}\"),\n            ]\n        )\n    except Exception as e:\n        log += f\"åˆ¤å®šå¤±æ•—: {e}\\n\"\n        judgment = JudgeResult(sufficient=True, reason=\"åˆ¤å®šã‚¨ãƒ©ãƒ¼ã®ãŸã‚å›ç­”ç”Ÿæˆã¸\")\n\n    if judgment.sufficient:\n        log += f\"æƒ…å ±ååˆ†ï¼ˆç†ç”±: {judgment.reason}ï¼‰\\n\"\n    else:\n        log += f\"æƒ…å ±ä¸è¶³ï¼ˆç†ç”±: {judgment.reason}ï¼‰\\n\"\n        for i, st in enumerate(judgment.additional_subtasks or []):\n            log += f\"  è¿½åŠ  {i + 1}. {st.purpose}: {st.queries}\\n\"\n    return judgment, log\n\n\nprint(\"Workflow å®Ÿè¡Œé–¢æ•°ã‚’å®šç¾©ã—ã¾ã—ãŸ: run_task_planning, run_doc_search, run_judge\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# LangGraph Workflow ã‚°ãƒ©ãƒ•ã®æ§‹ç¯‰ã¨å¯è¦–åŒ–\n# Gradio ã§ã¯å„ãƒãƒ¼ãƒ‰é–¢æ•°ã‚’ç›´æ¥å‘¼ã³å‡ºã™ãŒã€ã‚°ãƒ©ãƒ•å®šç¾©ã¯ Workflow ã®æ§‹é€ ã‚’ç¤ºã™ãŸã‚ã«ä¿æŒã™ã‚‹ã€‚\n\n\nasync def _node_task_planning(state: WorkflowState) -> dict:\n    subtasks, _ = await run_task_planning(state[\"question\"], llm.temperature)\n    return {\"subtasks\": subtasks, \"search_results\": [], \"loop_count\": 0}\n\n\nasync def _node_doc_search(state: WorkflowState) -> dict:\n    results, _ = await run_doc_search(state[\"subtasks\"])\n    existing = list(state.get(\"search_results\") or [])\n    return {\"search_results\": existing + results, \"subtasks\": []}\n\n\nasync def _node_judge(state: WorkflowState) -> dict:\n    loop_count = state.get(\"loop_count\", 0)\n    if loop_count >= MAX_LOOP_COUNT:\n        return {\"subtasks\": [], \"loop_count\": loop_count}\n    judgment, _ = await run_judge(\n        state[\"question\"], state[\"search_results\"], llm.temperature\n    )\n    additional = []\n    if not judgment.sufficient and judgment.additional_subtasks:\n        additional = [st.model_dump() for st in judgment.additional_subtasks]\n    return {\"subtasks\": additional, \"loop_count\": loop_count + 1}\n\n\ndef _should_continue(state: WorkflowState) -> str:\n    return \"doc_search\" if state.get(\"subtasks\") else \"generate_answer\"\n\n\nasync def _node_generate_answer(state: WorkflowState) -> dict:\n    results_text = \"\\n\\n\".join(state[\"search_results\"])\n    response = await llm.ainvoke(\n        [\n            SystemMessage(content=SYSTEM_PROMPT_GENERATE_ANSWER),\n            HumanMessage(\n                content=f\"è³ªå•: {state['question']}\\n\\næ¤œç´¢çµæœ:\\n{results_text}\"\n            ),\n        ]\n    )\n    return {\"answer\": response.content or \"\"}\n\n\n# ã‚°ãƒ©ãƒ•æ§‹ç¯‰\nwf = StateGraph(WorkflowState)\nwf.add_node(\"task_planning\", _node_task_planning)\nwf.add_node(\"doc_search\", _node_doc_search)\nwf.add_node(\"judge\", _node_judge)\nwf.add_node(\"generate_answer\", _node_generate_answer)\nwf.add_edge(START, \"task_planning\")\nwf.add_edge(\"task_planning\", \"doc_search\")\nwf.add_edge(\"doc_search\", \"judge\")\nwf.add_conditional_edges(\n    \"judge\",\n    _should_continue,\n    {\"doc_search\": \"doc_search\", \"generate_answer\": \"generate_answer\"},\n)\nwf.add_edge(\"generate_answer\", END)\nworkflow_app = wf.compile()\n\n# ã‚°ãƒ©ãƒ•ã®å¯è¦–åŒ–\ndisplay(Image(workflow_app.get_graph().draw_mermaid_png()))"
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "**Gradio UI ã®å®Ÿè£…**\n- å·¦ã‚«ãƒ©ãƒ : PDF ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‰ãƒ©ãƒƒã‚°ï¼†ãƒ‰ãƒ­ãƒƒãƒ—å…¥åŠ›ã¨ã€AI ã®æ€è€ƒéç¨‹ã®è¡¨ç¤º\n- å³ã‚«ãƒ©ãƒ : ãƒãƒ«ãƒã‚¿ãƒ¼ãƒ³å¯¾å¿œã®ãƒãƒ£ãƒƒãƒˆã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ï¼ˆä¼šè©±å±¥æ­´ã‚¹ã‚¯ãƒ­ãƒ¼ãƒ«å¯èƒ½ï¼‰\n  - ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆè¨­å®šï¼ˆã‚¢ã‚³ãƒ¼ãƒ‡ã‚£ã‚ªãƒ³å†…ã€ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆéè¡¨ç¤ºï¼‰\n  - Temperature ã‚¹ãƒ©ã‚¤ãƒ€ãƒ¼ï¼ˆ0.0ã€œ1.0ï¼‰ã§å¿œç­”ã®æ­£ç¢ºã•/å‰µé€ æ€§ã‚’èª¿æ•´\n  - é€ä¿¡ / ç”Ÿæˆåœæ­¢ / ä¼šè©±ã‚¯ãƒªã‚¢ ãƒœã‚¿ãƒ³ã‚’é…ç½®\n- PDF ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã™ã‚‹ã¨ã€å‰å‡¦ç†ãƒ»ãƒãƒ£ãƒ³ã‚¯åˆ†å‰²ãƒ»ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æ§‹ç¯‰ãŒå®Ÿè¡Œã•ã‚Œã‚‹ã€‚\n- ãƒ¦ãƒ¼ã‚¶ã®è³ªå•ã”ã¨ã« RAG Workflowï¼ˆã‚¿ã‚¹ã‚¯åˆ†å‰²â†’æ¤œç´¢â†’åˆ¤å®šâ†’å›ç­”ç”Ÿæˆï¼‰ãŒå®Ÿè¡Œã•ã‚Œã‚‹ã€‚\n- æ€è€ƒéç¨‹ï¼ˆã‚¿ã‚¹ã‚¯ã®å†…å®¹ã€æ¤œç´¢ã®å†…å®¹ã€åˆ¤æ–­çµæœï¼‰ã¯ãƒãƒ£ãƒƒãƒˆã¨ã¯åˆ¥ã®é ˜åŸŸã«è¡¨ç¤ºã™ã‚‹ã€‚\n- æœ€çµ‚å›ç­”ã¯ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ã§è¡¨ç¤ºã™ã‚‹ã€‚\n- `gr.State()` ã§ãƒ¦ãƒ¼ã‚¶ãƒ¼ï¼ˆãƒ–ãƒ©ã‚¦ã‚¶ã‚¿ãƒ–ï¼‰ã”ã¨ã«ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹ã‚’ç®¡ç†ã™ã‚‹ã€‚\n- `share=True` ã§ Colab ç’°å¢ƒã‹ã‚‰ã§ã‚‚ãƒ‘ãƒ–ãƒªãƒƒã‚¯ URL ã§ã‚¢ã‚¯ã‚»ã‚¹å¯èƒ½ã€‚"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Gradio UI: PDF ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ + RAG Workflow ãƒãƒ£ãƒƒãƒˆï¼ˆã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°å¿œç­”ï¼‰\nimport uuid\nimport gradio as gr  # type: ignore\n\n\ndef on_pdf_upload(filepath: str, state: dict) -> tuple[str, list, dict, str]:\n    \"\"\"PDF ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰æ™‚: å‰å‡¦ç†ãƒ»ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æ§‹ç¯‰ã‚’è¡Œã„ã€ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã‚’è¿”ã™ã€‚\n\n    Args:\n        filepath: Gradio ã® File ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆãŒç”Ÿæˆã—ãŸä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã€‚\n        state: ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹ã€‚\n\n    Returns:\n        tuple: (ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹è¡¨ç¤º, ãƒãƒ£ãƒƒãƒˆå±¥æ­´ãƒªã‚»ãƒƒãƒˆ, æ›´æ–°state, æ€è€ƒãƒ­ã‚°ãƒªã‚»ãƒƒãƒˆ)\n    \"\"\"\n    if filepath is None:\n        return \"PDF ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ‰ãƒ©ãƒƒã‚°ï¼†ãƒ‰ãƒ­ãƒƒãƒ—ã—ã¦ãã ã•ã„ã€‚\", [], state, \"\"\n\n    text = process_and_index_pdf(filepath)\n\n    new_state = state.copy()\n    new_state[\"pdf_text\"] = text\n    new_state[\"thread_id\"] = str(uuid.uuid4())\n\n    status = f\"PDF èª­ã¿è¾¼ã¿å®Œäº†: {len(_chunks)} ãƒãƒ£ãƒ³ã‚¯\\næ–‡å­—æ•°: {len(text)}\"\n    return status, [], new_state, f\"PDF ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æ§‹ç¯‰å®Œäº†ï¼ˆ{len(_chunks)} ãƒãƒ£ãƒ³ã‚¯ï¼‰\\n\"\n\n\ndef on_clear_chat(state: dict) -> tuple[list, dict, str]:\n    \"\"\"ä¼šè©±å±¥æ­´ã‚’ã‚¯ãƒªã‚¢ã—ã€æ–°ã—ã„ã‚¹ãƒ¬ãƒƒãƒ‰ ID ã‚’ç™ºè¡Œã™ã‚‹ã€‚\"\"\"\n    new_state = state.copy()\n    new_state[\"thread_id\"] = str(uuid.uuid4())\n    return [], new_state, \"\"\n\n\nasync def respond(\n    message: str,\n    chat_history: list,\n    state: dict,\n    system_prompt: str,\n    temperature: float,\n):\n    \"\"\"ãƒ¦ãƒ¼ã‚¶ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã«å¯¾ã— RAG Workflow ã‚’å®Ÿè¡Œã—ã€ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ã§å¿œç­”ã™ã‚‹ã€‚\n\n    æ€è€ƒéç¨‹ï¼ˆã‚¿ã‚¹ã‚¯åˆ†å‰²ãƒ»æ¤œç´¢ãƒ»åˆ¤å®šï¼‰ã‚’ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ è¡¨ç¤ºã—ã€\n    æœ€çµ‚å›ç­”ã‚’ãƒˆãƒ¼ã‚¯ãƒ³å˜ä½ã§ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ã™ã‚‹ã€‚\n\n    Args:\n        message: ãƒ¦ãƒ¼ã‚¶ãŒå…¥åŠ›ã—ãŸãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã€‚\n        chat_history: Gradio Chatbot å½¢å¼ã®ä¼šè©±å±¥æ­´ã€‚\n        state: ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹ã€‚\n        system_prompt: LLM ã«ä¸ãˆã‚‹ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã€‚\n        temperature: LLM ã® temperature ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã€‚\n\n    Yields:\n        tuple: (å…¥åŠ›æ¬„ã‚¯ãƒªã‚¢, ä¼šè©±å±¥æ­´, state, æ€è€ƒãƒ­ã‚°)\n    \"\"\"\n    # PDF æœªèª­ã¿è¾¼ã¿ãƒã‚§ãƒƒã‚¯\n    if not _chunks:\n        chat_history = chat_history + [\n            {\"role\": \"user\", \"content\": message},\n            {\"role\": \"assistant\", \"content\": \"PDF ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„ã€‚\"},\n        ]\n        yield \"\", chat_history, state, \"âš ï¸ PDF ãŒæœªèª­ã¿è¾¼ã¿ã§ã™ã€‚\\n\"\n        return\n\n    # ç©ºå…¥åŠ›ã®é˜²æ­¢\n    if not message.strip():\n        yield \"\", chat_history, state, \"\"\n        return\n\n    chat_history = chat_history + [\n        {\"role\": \"user\", \"content\": message},\n        {\"role\": \"assistant\", \"content\": \"\"},\n    ]\n    thinking = \"\"\n\n    # --- Phase 1: ã‚¿ã‚¹ã‚¯åˆ†å‰² ---\n    thinking += \"ğŸ“‹ ã‚¿ã‚¹ã‚¯åˆ†å‰²ä¸­...\\n\"\n    yield \"\", chat_history, state, thinking\n\n    subtasks, log = await run_task_planning(message, temperature)\n    thinking += log + \"\\n\"\n    yield \"\", chat_history, state, thinking\n\n    # --- Phase 2: æ¤œç´¢ + åˆ¤å®šãƒ«ãƒ¼ãƒ— ---\n    search_results: list[str] = []\n    loop_count = 0\n    current_subtasks = subtasks\n\n    while current_subtasks:\n        thinking += \"ğŸ” ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæ¤œç´¢ä¸­...\\n\"\n        yield \"\", chat_history, state, thinking\n\n        new_results, log = await run_doc_search(current_subtasks)\n        search_results.extend(new_results)\n        thinking += log + \"\\n\"\n        yield \"\", chat_history, state, thinking\n\n        loop_count += 1\n        if loop_count > MAX_LOOP_COUNT:\n            thinking += \"âš ï¸ ãƒ«ãƒ¼ãƒ—ä¸Šé™ã«åˆ°é” â†’ å›ç­”ä½œæˆã¸\\n\\n\"\n            yield \"\", chat_history, state, thinking\n            break\n\n        thinking += \"âš–ï¸ æƒ…å ±ã®ååˆ†æ€§ã‚’åˆ¤å®šä¸­...\\n\"\n        yield \"\", chat_history, state, thinking\n\n        judgment, log = await run_judge(message, search_results, temperature)\n        thinking += log + \"\\n\"\n        yield \"\", chat_history, state, thinking\n\n        if judgment.sufficient:\n            break\n        current_subtasks = [\n            st.model_dump() for st in (judgment.additional_subtasks or [])\n        ]\n\n    # --- Phase 3: å›ç­”ç”Ÿæˆï¼ˆã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ï¼‰ ---\n    thinking += \"âœï¸ å›ç­”ã‚’ç”Ÿæˆä¸­...\\n\"\n    yield \"\", chat_history, state, thinking\n\n    results_text = \"\\n\\n\".join(search_results)\n\n    # ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæ§‹ç¯‰\n    sys_content = system_prompt + \"\\n\\n\" + SYSTEM_PROMPT_GENERATE_ANSWER\n\n    # ãƒãƒ«ãƒã‚¿ãƒ¼ãƒ³å¯¾å¿œ: ç›´è¿‘ã®ä¼šè©±å±¥æ­´ã‚’ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã«å«ã‚ã‚‹\n    # ç¾åœ¨ã®è³ªå•ï¼ˆæœ«å°¾2ä»¶ï¼‰ã¯é™¤å¤–ã—ã€ç›´è¿‘4ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ï¼ˆ2å¾€å¾©ï¼‰ã¾ã§å«ã‚ã‚‹\n    recent_history = chat_history[:-2][-4:]\n    history_lines = []\n    for msg in recent_history:\n        role = \"ãƒ¦ãƒ¼ã‚¶\" if msg[\"role\"] == \"user\" else \"AI\"\n        history_lines.append(f\"{role}: {msg['content'][:500]}\")\n\n    user_content = \"\"\n    if history_lines:\n        user_content += \"ä¼šè©±å±¥æ­´:\\n\" + \"\\n\".join(history_lines) + \"\\n\\n\"\n    user_content += f\"è³ªå•: {message}\\n\\næ¤œç´¢çµæœ:\\n{results_text}\"\n\n    llm_instance = _make_llm(temperature)\n    bot_reply = \"\"\n    async for chunk in llm_instance.astream(\n        [\n            SystemMessage(content=sys_content),\n            HumanMessage(content=user_content),\n        ]\n    ):\n        if chunk.content:\n            bot_reply += chunk.content\n            chat_history = chat_history[:-1] + [\n                {\"role\": \"assistant\", \"content\": bot_reply}\n            ]\n            yield \"\", chat_history, state, thinking\n\n    thinking += \"âœ… å›ç­”ç”Ÿæˆå®Œäº†\\n\"\n    yield \"\", chat_history, state, thinking\n\n\n# --- Gradio UI ã®æ§‹ç¯‰ ---\nwith gr.Blocks(title=\"RAG ãƒãƒ£ãƒƒãƒˆã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆ\") as demo:\n    gr.Markdown(\"### RAG ãƒãƒ£ãƒƒãƒˆã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆï¼ˆAI Agent Workflow + RAGï¼‰\")\n\n    session_state = gr.State(\n        {\n            \"pdf_text\": \"\",\n            \"thread_id\": str(uuid.uuid4()),\n        }\n    )\n\n    with gr.Row():\n        # å·¦ã‚«ãƒ©ãƒ : PDF ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ + æ€è€ƒéç¨‹\n        with gr.Column(scale=1):\n            pdf_input = gr.File(\n                label=\"PDF ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ‰ãƒ©ãƒƒã‚°ï¼†ãƒ‰ãƒ­ãƒƒãƒ—\",\n                file_types=[\".pdf\"],\n                type=\"filepath\",\n            )\n            pdf_status = gr.Textbox(\n                label=\"PDF ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹\",\n                lines=2,\n                max_lines=2,\n                interactive=False,\n            )\n            thinking_display = gr.Textbox(\n                label=\"AI ã®æ€è€ƒéç¨‹\",\n                lines=25,\n                max_lines=25,\n                interactive=False,\n            )\n\n        # å³ã‚«ãƒ©ãƒ : ãƒãƒ£ãƒƒãƒˆã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹\n        with gr.Column(scale=1):\n            chatbot = gr.Chatbot(\n                label=\"AI ãƒãƒ£ãƒƒãƒˆ\",\n                height=400,\n            )\n\n            with gr.Accordion(\"ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆè¨­å®š (ä»»æ„)\", open=False):\n                system_prompt_input = gr.Textbox(\n                    label=\"ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ\",\n                    value=\"æ—¥æœ¬èªã§å›ç­”ã—ã¦ãã ã•ã„ã€‚\",\n                    lines=2,\n                )\n\n            temp_slider = gr.Slider(\n                minimum=0.0,\n                maximum=1.0,\n                value=llm.temperature,\n                step=0.1,\n                label=\"Temperature (ä½ã„ã»ã©æ­£ç¢ºã€é«˜ã„ã»ã©å‰µé€ çš„)\",\n            )\n\n            msg_input = gr.Textbox(\n                label=\"ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’å…¥åŠ›\",\n                placeholder=\"è³ªå•ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„...\",\n                lines=2,\n            )\n            with gr.Row():\n                send_btn = gr.Button(\"é€ä¿¡\", variant=\"primary\")\n                stop_btn = gr.Button(\"ç”Ÿæˆã‚’åœæ­¢\", variant=\"stop\")\n                clear_btn = gr.Button(\"ä¼šè©±ã‚’ã‚¯ãƒªã‚¢\", variant=\"secondary\")\n\n    # --- ã‚¤ãƒ™ãƒ³ãƒˆãƒãƒ³ãƒ‰ãƒ©ã®è¨­å®š ---\n\n    # PDF ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰\n    pdf_input.change(\n        on_pdf_upload,\n        inputs=[pdf_input, session_state],\n        outputs=[pdf_status, chatbot, session_state, thinking_display],\n    )\n\n    # é€ä¿¡ï¼ˆã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°å¿œç­”ï¼‰\n    submit_args = dict(\n        fn=respond,\n        inputs=[\n            msg_input,\n            chatbot,\n            session_state,\n            system_prompt_input,\n            temp_slider,\n        ],\n        outputs=[msg_input, chatbot, session_state, thinking_display],\n    )\n    submit_event_click = send_btn.click(**submit_args)\n    submit_event_enter = msg_input.submit(**submit_args)\n\n    # ç”Ÿæˆåœæ­¢ãƒœã‚¿ãƒ³\n    stop_btn.click(\n        fn=None,\n        inputs=None,\n        outputs=None,\n        cancels=[submit_event_click, submit_event_enter],\n    )\n\n    # ä¼šè©±ã‚¯ãƒªã‚¢ãƒœã‚¿ãƒ³\n    clear_btn.click(\n        on_clear_chat,\n        inputs=[session_state],\n        outputs=[chatbot, session_state, thinking_display],\n    )\n\n# Colab ç’°å¢ƒã§ã¯ share=True ã§ãƒ‘ãƒ–ãƒªãƒƒã‚¯ URL ã‚’ç”Ÿæˆã™ã‚‹\ndemo.launch(share=True)"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "demo.close()"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}