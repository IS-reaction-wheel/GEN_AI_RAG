{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 06 Colab 環境での Gradio UI の実装\n",
    "#### 機能\n",
    "- Google Colab 環境で Gradio を立ち上げする。\n",
    "- Gradio UI に PDFファイルをドラッグアンドドロップで入力する。\n",
    "- PDF ファイルが入力されたら、markdown に変換・前処理し、PDF の内容の先頭1000行を表示する。\n",
    "- PDF の内容表示は、画面をスクロールして、表示内容の全文を確認できるようにする。\n",
    "- Gradio UI にチャットインターフェース（ユーザ入力 / AI 回答）を設ける。\n",
    "- AI チャットは、マルチターンチャット可能とする。\n",
    "- ユーザ入力 / AI 回答とも、画面をスクロールして、会話履歴含めた全文を確認できるようにする。\n",
    "\n",
    "#### 環境・要件\n",
    "- Google Colab に Ollama をセットアップ\n",
    "- ChatOllama で LLM に接続\n",
    "- LangGraph によるLLMのメモリ管理\n",
    "- LangGraph/LangChain によるチャット\n",
    "- データの前処理\n",
    "  - markdown に変換（MarkItDown を使用）\n",
    "  - Unicode正規化 (NFKC), 1文字行ブロックの除去, 空行圧縮"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**必要なライブラリをインストール**\n",
    "- 1行にまとめることで pip が全パッケージの依存関係を一括解決する。\n",
    "- 分割すると後勝ちで依存関係が壊れるリスクがある。\n",
    "- NOTE: Colab では uv ではなく pip を使う。\n",
    "> uv は依存解決の過程で numpy 等をアップグレードし、プリインストール済みの scipy 等を壊すため。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Google Colab に必要なライブラリをインストールする。\n",
    "# 1行にまとめることで pip が全パッケージの依存関係を一括解決する。\n",
    "# NOTE: Colab では uv ではなく pip を使う。uv は依存解決の過程で\n",
    "#       numpy 等をアップグレードし、プリインストール済みの scipy 等を壊すため。\n",
    "# NOTE: langchain 関連は 1.x 系に明示的に指定する。\n",
    "#       Colab プリインストールの 0.3.x が残ると langchain-mcp-adapters が動作しない。\n",
    "# Pythonのリストとして定義することで、Pylanceの警告を防ぎ、可読性を高める。\n",
    "\n",
    "# fmt: off\n",
    "pkgs = [\n",
    "    \"ollama\", \"langchain-ollama\",\n",
    "    \"langchain>=1.2.8\", \"langchain-core>=1.2.8\", \"langgraph>=1.0.7\",\n",
    "    \"markitdown[all]\", \"gradio\",\n",
    "]\n",
    "# fmt: on\n",
    "\n",
    "# リストを結合して pip に渡す\n",
    "# magic command内で {変数} を使うと展開される機能を利用\n",
    "%pip install -U -q {\" \".join(pkgs)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Google Colab に Ollama をセットアップ**\n",
    "- Ollama のインストール, 起動, モデルのダウンロードを行う。\n",
    "- 詳細は [01_connect_oss_llm.ipynb](01_connect_oss_llm.ipynb) を参照。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selecting previously unselected package zstd.\n",
      "(Reading database ... 121689 files and directories currently installed.)\n",
      "Preparing to unpack .../zstd_1.4.8+dfsg-3build1_amd64.deb ...\n",
      "Unpacking zstd (1.4.8+dfsg-3build1) ...\n",
      "Setting up zstd (1.4.8+dfsg-3build1) ...\n",
      "Processing triggers for man-db (2.10.2-1) ...\n",
      ">>> Installing ollama to /usr/local\n",
      ">>> Downloading ollama-linux-amd64.tar.zst\n",
      "######################################################################## 100.0%\n",
      ">>> Creating ollama user...\n",
      ">>> Adding ollama user to video group...\n",
      ">>> Adding current user to ollama group...\n",
      ">>> Creating ollama systemd service...\n",
      "\u001b[1m\u001b[31mWARNING:\u001b[m systemd is not running\n",
      "\u001b[1m\u001b[31mWARNING:\u001b[m Unable to detect NVIDIA/AMD GPU. Install lspci or lshw to automatically detect and install GPU dependencies.\n",
      ">>> The Ollama API is now available at 127.0.0.1:11434.\n",
      ">>> Install complete. Run \"ollama\" from the command line.\n",
      "Done!\n",
      "  Model\n",
      "    architecture        gemma3     \n",
      "    parameters          999.89M    \n",
      "    context length      32768      \n",
      "    embedding length    1152       \n",
      "    quantization        Q4_0       \n",
      "\n",
      "  Capabilities\n",
      "    completion    \n",
      "\n",
      "  Parameters\n",
      "    stop           \"<end_of_turn>\"    \n",
      "    temperature    1                  \n",
      "    top_k          64                 \n",
      "    top_p          0.95               \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Ollama のインストール・起動・モデルのダウンロード\n",
    "# 詳細は 01_connect_oss_llm.ipynb を参照\n",
    "import subprocess\n",
    "import time\n",
    "import ollama  # type: ignore\n",
    "\n",
    "!apt-get install -y -qq zstd\n",
    "!curl -fsSL https://ollama.com/install.sh | sh\n",
    "\n",
    "process = subprocess.Popen(\n",
    "    [\"ollama\", \"serve\"],\n",
    "    stdout=subprocess.DEVNULL,\n",
    "    stderr=subprocess.DEVNULL,\n",
    ")\n",
    "time.sleep(5)\n",
    "\n",
    "\n",
    "def ollama_pull(model: str) -> None:\n",
    "    \"\"\"Ollama モデルをダウンロードし、進捗をインライン表示する。\n",
    "\n",
    "    NOTE: ollama pull のプログレスバーは Colab で文字化けするため、\n",
    "          Python API 経由でステータスのみ表示する。\n",
    "    \"\"\"\n",
    "    for progress in ollama.pull(model, stream=True):\n",
    "        status = progress.get(\"status\", \"\")\n",
    "        total = progress.get(\"total\") or 0\n",
    "        completed = progress.get(\"completed\") or 0\n",
    "        if total:\n",
    "            line = f\"{status}: {completed / total:.0%}\"\n",
    "        else:\n",
    "            line = status\n",
    "        print(f\"\\r{line:<60}\", end=\"\", flush=True)\n",
    "    print(f\"\\n{model}: Done!\")\n",
    "\n",
    "\n",
    "# AI エージェントにはツールコール対応モデルが必要。\n",
    "model_name = \"gpt-oss:20b\"\n",
    "ollama_pull(model_name)\n",
    "!ollama show {model_name}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ChatOllama で LLM に接続**\n",
    "- 詳細は [01_connect_oss_llm.ipynb](01_connect_oss_llm.ipynb) を参照。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ChatOllama で LLM に接続する。\n",
    "from langchain_ollama import ChatOllama  # type: ignore\n",
    "\n",
    "llm = ChatOllama(\n",
    "    model=\"gpt-oss:20b\",\n",
    "    num_ctx=16384,\n",
    "    num_predict=-1,\n",
    "    temperature=0.8,\n",
    "    top_k=40,\n",
    "    top_p=0.9,\n",
    "    repeat_penalty=1.1,\n",
    "    reasoning=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**LangGraph によるLLMのメモリ管理**\n",
    "- chat_node で、LLM を呼び出した際の処理を設定。\n",
    "> 今回は人間のプロンプトを LLM に入力して、LLM から応答を取得する処理。\n",
    "- LangGraph のグラフを構築し、ノードとエッジを設定し、グラフを compile する。\n",
    "> 今回は単純だが、各処理をつなげて自動化させる場合に効果を発揮する。\n",
    "- InMemorySaver を使用して、LLM との会話をメモリに保存させる。\n",
    "> LLM とのチャット履歴が保持されるようになる。\n",
    "- 作成したグラフを図示する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# LangGraph によるメモリ付きチャットグラフの構築\n",
    "# InMemorySaver を使って、スレッド（thread_id）ごとに会話履歴を保持する\n",
    "from langgraph.graph import StateGraph, MessagesState, START, END  # type: ignore\n",
    "from langgraph.checkpoint.memory import InMemorySaver  # type: ignore\n",
    "from IPython.display import Image, display\n",
    "\n",
    "\n",
    "# チャットノード：現在の会話履歴を LLM に送信し、返答を生成\n",
    "def chat_node(state: MessagesState):\n",
    "    response = llm.invoke(state[\"messages\"])\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "\n",
    "# グラフの構築\n",
    "graph = StateGraph(MessagesState)\n",
    "graph.add_node(\"chat\", chat_node)\n",
    "graph.add_edge(START, \"chat\")\n",
    "graph.add_edge(\"chat\", END)\n",
    "\n",
    "# InMemorySaver：スレッドごとに会話履歴をインメモリで保持\n",
    "memory = InMemorySaver()\n",
    "app = graph.compile(checkpointer=memory)\n",
    "\n",
    "# 構築したグラフを図示\n",
    "display(Image(app.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**データの前処理関数の定義**\n",
    "- MarkItDown で PDF → markdown に変換する。\n",
    "- 前処理: Unicode正規化 (NFKC), 1文字行ブロックの除去, 空行圧縮を行う。\n",
    "- 詳細は [04_AI_Embedding_RAG.ipynb](04_AI_Embedding_RAG.ipynb) を参照。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PDF → markdown 変換 + 前処理（Unicode正規化, ゴミ除去, 空行圧縮）\n",
    "# 詳細は 04_AI_Embedding_RAG.ipynb を参照\n",
    "import re\n",
    "import unicodedata\n",
    "from markitdown import MarkItDown  # type: ignore\n",
    "\n",
    "\n",
    "def clean_pdf_text(text: str) -> str:\n",
    "    \"\"\"PDF 抽出テキストの汎用クリーニングを行う。\n",
    "\n",
    "    1. 1文字行が 3 行以上連続するブロックを除去（図表・縦書き由来のゴミ）\n",
    "    2. 連続する空行を 2 つまでに圧縮\n",
    "    \"\"\"\n",
    "    text = re.sub(\n",
    "        r\"(^[^\\S\\n]*\\S[^\\S\\n]*$\\n?){3,}\",\n",
    "        \"\\n\",\n",
    "        text,\n",
    "        flags=re.MULTILINE,\n",
    "    )\n",
    "    text = re.sub(r\"\\n{3,}\", \"\\n\\n\", text)\n",
    "    return text.strip()\n",
    "\n",
    "\n",
    "def process_pdf(filepath: str) -> str:\n",
    "    \"\"\"PDF ファイルを markdown に変換し、前処理を行う。\"\"\"\n",
    "    md = MarkItDown()\n",
    "    result = md.convert(filepath)\n",
    "    text = result.text_content\n",
    "\n",
    "    # Unicode 正規化 (NFKC)\n",
    "    text = unicodedata.normalize(\"NFKC\", text)\n",
    "\n",
    "    # クリーニング\n",
    "    text = clean_pdf_text(text)\n",
    "    return text\n",
    "\n",
    "\n",
    "print(\"process_pdf 関数を定義しました。\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Gradio UI の実装**\n",
    "- 左カラム: PDF ファイルのドラッグ＆ドロップ入力と、変換後テキストの表示（先頭1000行、スクロール可能）\n",
    "- 右カラム: マルチターン対応のチャットインターフェース（会話履歴スクロール可能）\n",
    "- PDF をアップロードすると、新しい会話スレッドが開始される。\n",
    "- `share=True` で Colab 環境からでもパブリック URL でアクセス可能。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
      "* Running on public URL: https://19aa702d0d5fa3bf33.gradio.live\n",
      "\n",
      "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://19aa702d0d5fa3bf33.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Gradio UI: PDF アップロード + AI チャット\n",
    "import uuid\n",
    "import gradio as gr  # type: ignore\n",
    "from langchain_core.messages import HumanMessage, SystemMessage  # type: ignore\n",
    "\n",
    "# セッション状態: PDF テキストとスレッド ID を保持する\n",
    "session = {\n",
    "    \"pdf_text\": \"\",\n",
    "    \"thread_id\": str(uuid.uuid4()),\n",
    "    \"first_message\": True,\n",
    "}\n",
    "\n",
    "\n",
    "def on_pdf_upload(filepath: str) -> tuple[str, list]:\n",
    "    \"\"\"PDF アップロード時: markdown 変換・前処理し、先頭1000行を返す。\n",
    "    同時にチャット履歴をリセットし、新しい会話スレッドを開始する。\n",
    "    \"\"\"\n",
    "    if filepath is None:\n",
    "        session[\"pdf_text\"] = \"\"\n",
    "        return \"PDF ファイルをドラッグ＆ドロップしてください。\", []\n",
    "\n",
    "    text = process_pdf(filepath)\n",
    "    session[\"pdf_text\"] = text\n",
    "    session[\"thread_id\"] = str(uuid.uuid4())\n",
    "    session[\"first_message\"] = True\n",
    "\n",
    "    # 先頭1000行を表示用に切り出す\n",
    "    lines = text.split(\"\\n\")[:1000]\n",
    "    display_text = \"\\n\".join(lines)\n",
    "    if len(text.split(\"\\n\")) > 1000:\n",
    "        display_text += \"\\n\\n... (1000行以降は省略)\"\n",
    "\n",
    "    return display_text, []  # チャット履歴もリセット\n",
    "\n",
    "\n",
    "def respond(message: str, chat_history: list) -> tuple[str, list]:\n",
    "    \"\"\"ユーザのメッセージを LLM に送信し、応答をチャット履歴に追加する。\"\"\"\n",
    "    if not message.strip():\n",
    "        return \"\", chat_history\n",
    "\n",
    "    config = {\"configurable\": {\"thread_id\": session[\"thread_id\"]}}\n",
    "    messages = []\n",
    "\n",
    "    # 最初のメッセージ時にシステムプロンプトを設定する\n",
    "    if session[\"first_message\"]:\n",
    "        system_content = \"日本語で回答してください。\"\n",
    "        if session[\"pdf_text\"]:\n",
    "            # コンテキスト長に収まるよう、PDF テキストを制限する\n",
    "            pdf_excerpt = session[\"pdf_text\"][:3000]\n",
    "            system_content += (\n",
    "                \"\\n\\n以下はアップロードされた PDF の内容です。\"\n",
    "                \"この内容に基づいて回答してください:\\n\\n\"\n",
    "                f\"{pdf_excerpt}\"\n",
    "            )\n",
    "        messages.append(SystemMessage(content=system_content))\n",
    "        session[\"first_message\"] = False\n",
    "\n",
    "    messages.append(HumanMessage(content=message))\n",
    "\n",
    "    response = app.invoke({\"messages\": messages}, config=config)\n",
    "    bot_reply = response[\"messages\"][-1].content\n",
    "\n",
    "    # Gradio の messages フォーマット（role/content 辞書）で履歴に追加\n",
    "    chat_history.append({\"role\": \"user\", \"content\": message})\n",
    "    chat_history.append({\"role\": \"assistant\", \"content\": bot_reply})\n",
    "    return \"\", chat_history\n",
    "\n",
    "\n",
    "# --- Gradio UI の構築 ---\n",
    "with gr.Blocks(title=\"PDF チャットアシスタント\") as demo:\n",
    "    gr.Markdown(\"### PDF チャットアシスタント\")\n",
    "\n",
    "    with gr.Row():\n",
    "        # 左カラム: PDF アップロード + 内容表示\n",
    "        with gr.Column(scale=1):\n",
    "            pdf_input = gr.File(\n",
    "                label=\"PDF ファイルをドラッグ＆ドロップ\",\n",
    "                file_types=[\".pdf\"],\n",
    "                type=\"filepath\",\n",
    "            )\n",
    "            pdf_display = gr.Textbox(\n",
    "                label=\"PDF 内容（先頭1000行）\",\n",
    "                lines=25,\n",
    "                max_lines=25,\n",
    "                interactive=False,\n",
    "            )\n",
    "\n",
    "        # 右カラム: チャットインターフェース\n",
    "        with gr.Column(scale=1):\n",
    "            chatbot = gr.Chatbot(\n",
    "                label=\"AI チャット\",\n",
    "                height=400,\n",
    "            )\n",
    "            msg_input = gr.Textbox(\n",
    "                label=\"メッセージを入力\",\n",
    "                placeholder=\"質問を入力してください...\",\n",
    "                lines=2,\n",
    "            )\n",
    "            send_btn = gr.Button(\"送信\", variant=\"primary\")\n",
    "\n",
    "    # イベントハンドラの設定\n",
    "    pdf_input.change(\n",
    "        on_pdf_upload,\n",
    "        inputs=pdf_input,\n",
    "        outputs=[pdf_display, chatbot],\n",
    "    )\n",
    "    send_btn.click(\n",
    "        respond,\n",
    "        inputs=[msg_input, chatbot],\n",
    "        outputs=[msg_input, chatbot],\n",
    "    )\n",
    "    msg_input.submit(\n",
    "        respond,\n",
    "        inputs=[msg_input, chatbot],\n",
    "        outputs=[msg_input, chatbot],\n",
    "    )\n",
    "\n",
    "# Colab 環境では share=True でパブリック URL を生成する\n",
    "demo.launch(share=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demo.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
