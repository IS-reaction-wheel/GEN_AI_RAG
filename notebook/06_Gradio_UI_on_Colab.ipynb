{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 06 Colab 環境での Gradio UI の実装\n",
    "#### 機能\n",
    "- Google Colab 環境で Gradio を立ち上げする。\n",
    "- Gradio UI に PDFファイルをドラッグアンドドロップで入力する。\n",
    "- PDF ファイルが入力されたら、markdown に変換・前処理し、PDF の内容の先頭1000行を表示する。\n",
    "- PDF の内容表示は、画面をスクロールして、表示内容の全文を確認できるようにする。\n",
    "- Gradio UI にチャットインターフェース（ユーザ入力 / AI 回答）を設ける。\n",
    "- AI チャットは、マルチターン対応、ストリーミング応答を行う。\n",
    "- ユーザ入力 / AI 回答とも、画面をスクロールして、会話履歴含めた全文を確認できるようにする。\n",
    "- **会話クリアボタン**: PDF を保持したまま会話履歴をリセットし、新しいスレッドで再質問できる。\n",
    "- **システムプロンプト設定**: アコーディオン内で自由に編集可能。デフォルトは「日本語で回答してください。」\n",
    "- **Temperature スライダー**: LLM の応答の正確さ/創造性を 0.0〜1.0 で調整できる。\n",
    "- **生成停止ボタン**: ストリーミング中の応答を途中で中断できる。\n",
    "\n",
    "#### 環境・要件\n",
    "- Google Colab に Ollama をセットアップ\n",
    "- ChatOllama で LLM に接続\n",
    "- LangGraph によるLLMのメモリ管理\n",
    "- LangGraph/LangChain によるチャット\n",
    "- データの前処理\n",
    "  - markdown に変換（MarkItDown を使用）\n",
    "  - Unicode正規化 (NFKC), 1文字行ブロックの除去, 空行圧縮\n",
    "\n",
    "**この notebook で使用しているテストデータ**\n",
    "> [井澤克彦, 市川信一郎, 高速回転ホイール: 高速回転ホイール開発を通しての知見, 宇宙航空研究開発機構研究開発報告, 2008](https://jaxa.repo.nii.ac.jp/records/2149)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**必要なライブラリをインストール**\n",
    "- 1行にまとめることで pip が全パッケージの依存関係を一括解決する。\n",
    "- 分割すると後勝ちで依存関係が壊れるリスクがある。\n",
    "- NOTE: Colab では uv ではなく pip を使う。\n",
    "> uv は依存解決の過程で numpy 等をアップグレードし、プリインストール済みの scipy 等を壊すため。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Google Colab に必要なライブラリをインストールする。\n",
    "# 1行にまとめることで pip が全パッケージの依存関係を一括解決する。\n",
    "# NOTE: Colab では uv ではなく pip を使う。uv は依存解決の過程で\n",
    "#       numpy 等をアップグレードし、プリインストール済みの scipy 等を壊すため。\n",
    "# NOTE: langchain 関連は 1.x 系に明示的に指定する。\n",
    "#       Colab プリインストールの 0.3.x が残ると langchain-mcp-adapters が動作しない。\n",
    "# Pythonのリストとして定義することで、Pylanceの警告を防ぎ、可読性を高める。\n",
    "\n",
    "# fmt: off\n",
    "pkgs = [\n",
    "    \"ollama\", \"langchain-ollama\",\n",
    "    \"langchain>=1.2.8\", \"langchain-core>=1.2.8\", \"langgraph>=1.0.7\",\n",
    "    \"markitdown[all]\", \"gradio>=6.0\",\n",
    "]\n",
    "# fmt: on\n",
    "\n",
    "# リストを結合して pip に渡す\n",
    "# magic command内で {変数} を使うと展開される機能を利用\n",
    "%pip install -U -q {\" \".join(pkgs)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Google Colab に Ollama をセットアップ**\n",
    "- Ollama のインストール, 起動, モデルのダウンロードを行う。\n",
    "- 詳細は [01_connect_oss_llm.ipynb](01_connect_oss_llm.ipynb) を参照。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selecting previously unselected package zstd.\n",
      "(Reading database ... 121852 files and directories currently installed.)\n",
      "Preparing to unpack .../zstd_1.4.8+dfsg-3build1_amd64.deb ...\n",
      "Unpacking zstd (1.4.8+dfsg-3build1) ...\n",
      "Setting up zstd (1.4.8+dfsg-3build1) ...\n",
      "Processing triggers for man-db (2.10.2-1) ...\n",
      ">>> Installing ollama to /usr/local\n",
      ">>> Downloading ollama-linux-amd64.tar.zst\n",
      "######################################################################## 100.0%\n",
      ">>> Creating ollama user...\n",
      ">>> Adding ollama user to video group...\n",
      ">>> Adding current user to ollama group...\n",
      ">>> Creating ollama systemd service...\n",
      "\u001b[1m\u001b[31mWARNING:\u001b[m systemd is not running\n",
      "\u001b[1m\u001b[31mWARNING:\u001b[m Unable to detect NVIDIA/AMD GPU. Install lspci or lshw to automatically detect and install GPU dependencies.\n",
      ">>> The Ollama API is now available at 127.0.0.1:11434.\n",
      ">>> Install complete. Run \"ollama\" from the command line.\n",
      "success                                                     \n",
      "gpt-oss:20b: Done!\n",
      "  Model\n",
      "    architecture        gptoss    \n",
      "    parameters          20.9B     \n",
      "    context length      131072    \n",
      "    embedding length    2880      \n",
      "    quantization        MXFP4     \n",
      "\n",
      "  Capabilities\n",
      "    completion    \n",
      "    tools         \n",
      "    thinking      \n",
      "\n",
      "  Parameters\n",
      "    temperature    1    \n",
      "\n",
      "  License\n",
      "    Apache License               \n",
      "    Version 2.0, January 2004    \n",
      "    ...                          \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Ollama のインストール・起動・モデルのダウンロード\n",
    "# 詳細は 01_connect_oss_llm.ipynb を参照\n",
    "import subprocess\n",
    "import time\n",
    "import ollama  # type: ignore\n",
    "\n",
    "!apt-get install -y -qq zstd\n",
    "!curl -fsSL https://ollama.com/install.sh | sh\n",
    "\n",
    "process = subprocess.Popen(\n",
    "    [\"ollama\", \"serve\"],\n",
    "    stdout=subprocess.DEVNULL,\n",
    "    stderr=subprocess.DEVNULL,\n",
    ")\n",
    "time.sleep(5)\n",
    "\n",
    "\n",
    "def ollama_pull(model: str) -> None:\n",
    "    \"\"\"Ollama モデルをダウンロードし、進捗をインライン表示する。\n",
    "\n",
    "    NOTE: ollama pull のプログレスバーは Colab で文字化けするため、\n",
    "          Python API 経由でステータスのみ表示する。\n",
    "    \"\"\"\n",
    "    for progress in ollama.pull(model, stream=True):\n",
    "        status = progress.get(\"status\", \"\")\n",
    "        total = progress.get(\"total\") or 0\n",
    "        completed = progress.get(\"completed\") or 0\n",
    "        if total:\n",
    "            line = f\"{status}: {completed / total:.0%}\"\n",
    "        else:\n",
    "            line = status\n",
    "        print(f\"\\r{line:<60}\", end=\"\", flush=True)\n",
    "    print(f\"\\n{model}: Done!\")\n",
    "\n",
    "\n",
    "# AI エージェントにはツールコール対応モデルが必要。\n",
    "model_name = \"gpt-oss:20b\"\n",
    "ollama_pull(model_name)\n",
    "!ollama show {model_name}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ChatOllama で LLM に接続**\n",
    "- 詳細は [01_connect_oss_llm.ipynb](01_connect_oss_llm.ipynb) を参照。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ChatOllama で LLM に接続する。\n",
    "from langchain_ollama import ChatOllama  # type: ignore\n",
    "\n",
    "llm = ChatOllama(\n",
    "    model=\"gpt-oss:20b\",\n",
    "    num_ctx=16384,\n",
    "    num_predict=-1,\n",
    "    temperature=0.8,\n",
    "    top_k=40,\n",
    "    top_p=0.9,\n",
    "    repeat_penalty=1.1,\n",
    "    reasoning=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LangGraph によるメモリ付きチャットグラフの構築\n",
    "**LangGraph によるLLMのメモリ管理**\n",
    "- chat_node で、LLM を呼び出した際の処理を設定。\n",
    "> 今回は人間のプロンプトを LLM に入力して、LLM から応答を取得する処理。\n",
    "- LangGraph のグラフを構築し、ノードとエッジを設定し、グラフを compile する。\n",
    "> 今回は単純だが、各処理をつなげて自動化させる場合に効果を発揮する。\n",
    "- InMemorySaver を使用して、LLM との会話をメモリに保存させる。\n",
    "> LLM とのチャット履歴が保持されるようになる。\n",
    "- 作成したグラフを図示する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# LangGraph によるメモリ付きチャットグラフの構築\n",
    "# InMemorySaver を使って、スレッド（thread_id）ごとに会話履歴を保持する\n",
    "from langgraph.graph import StateGraph, MessagesState, START, END  # type: ignore\n",
    "from langgraph.checkpoint.memory import InMemorySaver  # type: ignore\n",
    "from langchain_core.runnables import RunnableConfig  # type: ignore\n",
    "from IPython.display import Image, display\n",
    "\n",
    "\n",
    "def _make_llm(temperature: float) -> ChatOllama:\n",
    "    \"\"\"指定した temperature で ChatOllama インスタンスを生成する。\n",
    "    temperature 以外のパラメータはベースの llm から引き継ぐ。\n",
    "\n",
    "    Args:\n",
    "        temperature: LLM の応答のランダム性（0.0=正確、1.0=創造的）。\n",
    "\n",
    "    Returns:\n",
    "        ChatOllama: 指定 temperature で構成された LLM インスタンス。\n",
    "    \"\"\"\n",
    "    return ChatOllama(\n",
    "        model=llm.model,\n",
    "        num_ctx=llm.num_ctx,\n",
    "        num_predict=llm.num_predict,\n",
    "        temperature=temperature,\n",
    "        top_k=llm.top_k,\n",
    "        top_p=llm.top_p,\n",
    "        repeat_penalty=llm.repeat_penalty,\n",
    "        reasoning=llm.reasoning,\n",
    "    )\n",
    "\n",
    "\n",
    "def chat_node(state: MessagesState, config: RunnableConfig) -> dict:\n",
    "    \"\"\"LangGraph チャットノード。会話履歴を LLM に送信し、応答を返す。\n",
    "\n",
    "    config 経由で temperature を受け取り、動的に LLM インスタンスを生成する。\n",
    "    これによりグラフを毎回再構築せずに temperature を切り替えられる。\n",
    "\n",
    "    Args:\n",
    "        state: LangGraph の MessagesState。\"messages\" キーに会話履歴を持つ。\n",
    "        config: LangGraph の RunnableConfig。config[\"configurable\"][\"temperature\"]\n",
    "                で LLM の temperature を指定する（デフォルト: llm.temperature）。\n",
    "\n",
    "    Returns:\n",
    "        dict: {\"messages\": [response]} 形式。LLM の応答メッセージを含む。\n",
    "    \"\"\"\n",
    "    temp = config[\"configurable\"].get(\"temperature\", llm.temperature)\n",
    "    response = _make_llm(temp).invoke(state[\"messages\"])\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "\n",
    "# グラフの構築\n",
    "graph = StateGraph(MessagesState)\n",
    "graph.add_node(\"chat\", chat_node)\n",
    "graph.add_edge(START, \"chat\")\n",
    "graph.add_edge(\"chat\", END)\n",
    "\n",
    "# InMemorySaver：スレッドごとに会話履歴をインメモリで保持\n",
    "memory = InMemorySaver()\n",
    "app = graph.compile(checkpointer=memory)\n",
    "\n",
    "# 構築したグラフを図示\n",
    "display(Image(app.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**データの前処理関数の定義**\n",
    "- MarkItDown で PDF → markdown に変換する。\n",
    "- 前処理: Unicode正規化 (NFKC), 1文字行ブロックの除去, 空行圧縮を行う。\n",
    "- 詳細は [04_AI_Embedding_RAG.ipynb](04_AI_Embedding_RAG.ipynb) を参照。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "process_pdf 関数を定義しました。\n"
     ]
    }
   ],
   "source": [
    "# PDF → markdown 変換 + 前処理（Unicode正規化, ゴミ除去, 空行圧縮）\n",
    "# 詳細は 04_AI_Embedding_RAG.ipynb を参照\n",
    "import re\n",
    "import unicodedata\n",
    "from markitdown import MarkItDown  # type: ignore\n",
    "\n",
    "\n",
    "def clean_pdf_text(text: str) -> str:\n",
    "    \"\"\"PDF 抽出テキストの汎用クリーニングを行う。\n",
    "\n",
    "    1. 1文字行が 3 行以上連続するブロックを除去（図表・縦書き由来のゴミ）\n",
    "    2. 連続する空行を 2 つまでに圧縮\n",
    "    \"\"\"\n",
    "    text = re.sub(\n",
    "        r\"(^[^\\S\\n]*\\S[^\\S\\n]*$\\n?){3,}\",\n",
    "        \"\\n\",\n",
    "        text,\n",
    "        flags=re.MULTILINE,\n",
    "    )\n",
    "    text = re.sub(r\"\\n{3,}\", \"\\n\\n\", text)\n",
    "    return text.strip()\n",
    "\n",
    "\n",
    "def process_pdf(filepath: str) -> str:\n",
    "    \"\"\"PDF ファイルを markdown に変換し、前処理を行う。\"\"\"\n",
    "    md = MarkItDown()\n",
    "    result = md.convert(filepath)\n",
    "    text = result.text_content\n",
    "\n",
    "    # Unicode 正規化 (NFKC)\n",
    "    text = unicodedata.normalize(\"NFKC\", text)\n",
    "\n",
    "    # クリーニング\n",
    "    text = clean_pdf_text(text)\n",
    "    return text\n",
    "\n",
    "\n",
    "print(\"process_pdf 関数を定義しました。\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Gradio UI の実装**\n",
    "- 左カラム: PDF ファイルのドラッグ＆ドロップ入力と、変換後テキストの表示（先頭1000行、スクロール可能）\n",
    "- 右カラム: マルチターン対応のチャットインターフェース（会話履歴スクロール可能）\n",
    "  - システムプロンプト設定（アコーディオン内、デフォルト非表示）\n",
    "  - Temperature スライダー（0.0〜1.0）で応答の正確さ/創造性を調整\n",
    "  - 送信 / 生成停止 / 会話クリア ボタンを配置\n",
    "- PDF をアップロードすると、新しい会話スレッドが開始される。\n",
    "- 「会話をクリア」で PDF を保持したまま新スレッドで再質問可能。\n",
    "- 「生成を停止」でストリーミング中の応答を中断可能。\n",
    "- `gr.State()` でユーザー（ブラウザタブ）ごとにセッション状態を管理する。\n",
    "- `share=True` で Colab 環境からでもパブリック URL でアクセス可能。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Gradio の概要**\n",
    "\n",
    "Blocks（UI（テキストボックスなど））, State（Gradio アプリのメモリ）, イベント（トリガー）, ユーザー定義関数（ロジック）から構成される。\n",
    "1. **Blocks** ：アプリ全体の「レイアウト」を定義する。\n",
    "> Row（横並び）や Column（縦並び）を使って、ボタン、テキストボックス、チャット窓などのコンポーネントを配置します。\n",
    "2. **State** ：Gradio を起動すると立ち上がるブラウザのタブごとに、独立した「メモリ」を保持する。State は、画面には表示されない「ユーザーごとの変数」（ユーザが定義する必要あり）\n",
    "> 今回のコードでは、PDFの全文データや、会話を識別するための thread_id を保存するために使用し、複数ユーザーが同時にアクセスしても会話が混ざらないようになっている。\n",
    "3. **イベント** ：「何をした時に、何が起きるか」という動作のトリガーを定義します。\n",
    "> 「ファイルが変更された（.change）」「ボタンが押された（.click）」「Enterキーが押された（.submit）」といった操作を検知する。\n",
    "4. **ユーザー定義関数** ：イベントによって呼び出される、**実際の処理（Pythonコード）**。\n",
    "> Gradioのイベント設定によって、「画面の入力値（inputs）」と「現在のセッション状態（state）」を受け取り、処理結果を return または yield することで、「画面の表示（outputs）」と「セッション状態（state）」を最新の状態に更新する。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**各実装の解説**\n",
    "\n",
    "<u>`on_pdf_upload`, `on_clear_chat`</u>\n",
    "- `filepath` ：Gradio のイベントボックス（`.File`）にファイルを入力すると、自動で作成される。今回は Colab 上で動作させており、Colab の一時フォルダとなる。\n",
    "- Gradio 内での新しいセッションの作成。`.copy()` で作成し、次の 2行で初期化している。セッションの id は、番号が一意になるように、`uuid4()` で乱数によって生成している。\n",
    "  - `new_state = state.copy()`\n",
    "  - `new_state[\"pdf_text\"] = text`\n",
    "  - `new_state[\"thread_id\"] = str(uuid.uuid4())` # uuid4()：乱数で一意になる id を生成\n",
    "- `return display_text, [], new_state` ：`[]` はチャット履歴を空で返している。\n",
    "\n",
    "<u>`_make_llm` / `chat_node`</u>\n",
    "> コードは、**LangGraph によるメモリ付きチャットグラフの構築** を参照。\n",
    "\n",
    "> UI に、LLM の `temperature` パラメータを調整できるスライダーを設けている。`chat_node` は `config[\"configurable\"][\"temperature\"]` から値を取得し、`chat_node` 内の動作として、`_make_llm` で LLM インスタンスを生成する。グラフは一度だけ構築し、`temperature` は `config` 経由で、AI とチャットを行うたびに、毎回渡す。（AI の記憶などは、別変数で管理しているので、チャットの会話履歴などには影響しない。）\n",
    "\n",
    "<u>`respond`</u>\n",
    "- `config = {\"configurable\": {\"thread_id\": ..., \"temperature\": ...}}` ：\n",
    "> `thread_id` で LLM の記憶を管理し、`temperature` でノード内の LLM 生成パラメータを制御する。グラフ自体は **LangGraph によるメモリ付きチャットグラフの構築** で構築済みの `app` をそのまま使う。\n",
    "- `chat_history` ：Gradio の Chatbot 形式のデータで、ユーザ側 / AI の会話の履歴情報。システムプロンプトは含まれない。（要は、 Gradio UI の画面で見られる情報。）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
      "* Running on public URL: https://805b4a20e3ac21f118.gradio.live\n",
      "\n",
      "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://805b4a20e3ac21f118.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Gradio UI: PDF アップロード + AI チャット（ストリーミング応答）\n",
    "import uuid\n",
    "import gradio as gr  # type: ignore\n",
    "from langchain_core.messages import HumanMessage, SystemMessage  # type: ignore\n",
    "\n",
    "\n",
    "def on_pdf_upload(filepath: str, state: dict) -> tuple[str, list, dict]:\n",
    "    \"\"\"PDF アップロード時: markdown 変換・前処理し、先頭1000行を返す。\n",
    "    同時にチャット履歴をリセットし、新しい会話スレッドを開始する。\n",
    "\n",
    "    Args:\n",
    "        filepath: Gradio の File コンポーネントが生成した一時ファイルパス。\n",
    "        state: セッション状態。\"pdf_text\" と \"thread_id\" を含む辞書。\n",
    "\n",
    "    Returns:\n",
    "        tuple[str, list, dict]:\n",
    "            - display_text: PDF テキストの先頭1000行（表示用）。\n",
    "            - chat_history: 空リスト（チャット履歴のリセット）。\n",
    "            - new_state: 更新されたセッション状態。\n",
    "    \"\"\"\n",
    "    if filepath is None:\n",
    "        return \"PDF ファイルをドラッグ＆ドロップしてください。\", [], state\n",
    "\n",
    "    text = process_pdf(filepath)\n",
    "\n",
    "    # State の更新（新しいスレッド ID を発行）\n",
    "    new_state = state.copy()\n",
    "    new_state[\"pdf_text\"] = text\n",
    "    new_state[\"thread_id\"] = str(uuid.uuid4())\n",
    "\n",
    "    # 先頭1000行を表示用に切り出す\n",
    "    lines = text.split(\"\\n\")[:1000]\n",
    "    display_text = \"\\n\".join(lines)\n",
    "    if len(text.split(\"\\n\")) > 1000:\n",
    "        display_text += \"\\n\\n... (1000行以降は省略)\"\n",
    "\n",
    "    return display_text, [], new_state  # チャット履歴もリセット\n",
    "\n",
    "\n",
    "def on_clear_chat(state: dict) -> tuple[list, dict]:\n",
    "    \"\"\"会話履歴をクリアし、新しいスレッド ID を発行する。\n",
    "    PDF テキストはそのまま保持するため、同じ文書で話題を変えられる。\n",
    "\n",
    "    Args:\n",
    "        state: セッション状態。\"pdf_text\" と \"thread_id\" を含む辞書。\n",
    "\n",
    "    Returns:\n",
    "        tuple[list, dict]:\n",
    "            - chat_history: 空リスト（チャット履歴のリセット）。\n",
    "            - new_state: 新しい thread_id を持つ更新済みセッション状態。\n",
    "    \"\"\"\n",
    "    new_state = state.copy()\n",
    "    new_state[\"thread_id\"] = str(uuid.uuid4())\n",
    "    return [], new_state\n",
    "\n",
    "\n",
    "def respond(\n",
    "    message: str,\n",
    "    chat_history: list,\n",
    "    state: dict,\n",
    "    system_prompt: str,\n",
    "    temperature: float,\n",
    "):\n",
    "    \"\"\"ユーザのメッセージを LLM に送信し、ストリーミングで応答を返す。\n",
    "\n",
    "    Args:\n",
    "        message: ユーザが入力したメッセージ文字列。\n",
    "        chat_history: Gradio Chatbot 形式の会話履歴リスト。\n",
    "        state: セッション状態。\"pdf_text\" と \"thread_id\" を含む辞書。\n",
    "        system_prompt: LLM に与えるシステムプロンプト。\n",
    "        temperature: LLM の応答のランダム性（0.0〜1.0）。\n",
    "\n",
    "    Yields:\n",
    "        tuple[str, list, dict]:\n",
    "            - msg_input: 空文字列（入力欄のクリア）。\n",
    "            - chat_history: 応答トークンが追記された会話履歴。\n",
    "            - state: セッション状態（変更なし）。\n",
    "    \"\"\"\n",
    "\n",
    "    # 空入力の防止\n",
    "    if not message.strip():\n",
    "        yield \"\", chat_history, state\n",
    "        return\n",
    "\n",
    "    # LLM の temperature パラメータとメモリ管理の設定\n",
    "    config = {\n",
    "        \"configurable\": {\n",
    "            \"thread_id\": state[\"thread_id\"],\n",
    "            \"temperature\": temperature,\n",
    "        }\n",
    "    }\n",
    "    messages = []\n",
    "\n",
    "    # 会話の最初（履歴が空）ならシステムプロンプトを設定する\n",
    "    if not chat_history:\n",
    "        system_content = system_prompt\n",
    "        if state[\"pdf_text\"]:\n",
    "            # num_ctx=16384 に収まるよう、PDF テキストを制限する\n",
    "            pdf_excerpt = state[\"pdf_text\"][:10000]\n",
    "            system_content += (\n",
    "                \"\\n\\n以下はアップロードされた PDF の内容です。\"\n",
    "                \"この内容に基づいて回答してください:\\n\\n\"\n",
    "                f\"{pdf_excerpt}\"\n",
    "            )\n",
    "        messages.append(SystemMessage(content=system_content))\n",
    "\n",
    "    messages.append(HumanMessage(content=message))\n",
    "\n",
    "    # ユーザーメッセージと空のアシスタントメッセージを追加\n",
    "    chat_history = chat_history + [\n",
    "        {\"role\": \"user\", \"content\": message},\n",
    "        {\"role\": \"assistant\", \"content\": \"\"},\n",
    "    ]\n",
    "\n",
    "    # stream_mode=\"messages\" でトークン単位のストリーミング\n",
    "    bot_reply = \"\"\n",
    "    for chunk, metadata in app.stream(\n",
    "        {\"messages\": messages}, config=config, stream_mode=\"messages\"\n",
    "    ):\n",
    "        if chunk.content:\n",
    "            bot_reply += chunk.content\n",
    "            chat_history[-1] = {\"role\": \"assistant\", \"content\": bot_reply}\n",
    "            yield \"\", chat_history, state\n",
    "\n",
    "\n",
    "# --- Gradio UI の構築 ---\n",
    "with gr.Blocks(title=\"PDF チャットアシスタント\") as demo:\n",
    "    gr.Markdown(\"### PDF チャットアシスタント\")\n",
    "\n",
    "    # ユーザー（ブラウザタブ）ごとのセッション状態を保持する\n",
    "    session_state = gr.State(\n",
    "        {\n",
    "            \"pdf_text\": \"\",\n",
    "            \"thread_id\": str(uuid.uuid4()),\n",
    "        }\n",
    "    )\n",
    "\n",
    "    with gr.Row():\n",
    "        # 左カラム: PDF アップロード + 内容表示\n",
    "        with gr.Column(scale=1):\n",
    "            pdf_input = gr.File(\n",
    "                label=\"PDF ファイルをドラッグ＆ドロップ\",\n",
    "                file_types=[\".pdf\"],\n",
    "                type=\"filepath\",\n",
    "            )\n",
    "            pdf_display = gr.Textbox(\n",
    "                label=\"PDF 内容（先頭1000行）\",\n",
    "                lines=25,\n",
    "                max_lines=25,\n",
    "                interactive=False,\n",
    "            )\n",
    "\n",
    "        # 右カラム: チャットインターフェース\n",
    "        with gr.Column(scale=1):\n",
    "            chatbot = gr.Chatbot(\n",
    "                label=\"AI チャット\",\n",
    "                height=400,\n",
    "            )\n",
    "\n",
    "            # システムプロンプト設定（アコーディオン）\n",
    "            with gr.Accordion(\"システムプロンプト設定 (任意)\", open=False):\n",
    "                system_prompt_input = gr.Textbox(\n",
    "                    label=\"システムプロンプト\",\n",
    "                    value=\"日本語で回答してください。\",\n",
    "                    lines=2,\n",
    "                )\n",
    "\n",
    "            # Temperature スライダー（初期値はベース LLM の設定に合わせる）\n",
    "            temp_slider = gr.Slider(\n",
    "                minimum=0.0,\n",
    "                maximum=1.0,\n",
    "                value=llm.temperature,\n",
    "                step=0.1,\n",
    "                label=\"Temperature (低いほど正確、高いほど創造的)\",\n",
    "            )\n",
    "\n",
    "            msg_input = gr.Textbox(\n",
    "                label=\"メッセージを入力\",\n",
    "                placeholder=\"質問を入力してください...\",\n",
    "                lines=2,\n",
    "            )\n",
    "            with gr.Row():\n",
    "                send_btn = gr.Button(\"送信\", variant=\"primary\")\n",
    "                stop_btn = gr.Button(\"生成を停止\", variant=\"stop\")\n",
    "                clear_btn = gr.Button(\"会話をクリア\", variant=\"secondary\")\n",
    "\n",
    "    # --- イベントハンドラの設定 ---\n",
    "\n",
    "    # PDF アップロード\n",
    "    pdf_input.change(\n",
    "        on_pdf_upload,\n",
    "        inputs=[pdf_input, session_state],\n",
    "        outputs=[pdf_display, chatbot, session_state],\n",
    "    )\n",
    "\n",
    "    # 送信（ストリーミング応答）— イベントを変数に保持して停止ボタンに渡す\n",
    "    submit_args = dict(\n",
    "        fn=respond,\n",
    "        inputs=[msg_input, chatbot, session_state, system_prompt_input, temp_slider],\n",
    "        outputs=[msg_input, chatbot, session_state],\n",
    "    )\n",
    "    submit_event_click = send_btn.click(**submit_args)\n",
    "    submit_event_enter = msg_input.submit(**submit_args)\n",
    "\n",
    "    # 生成停止ボタン\n",
    "    stop_btn.click(\n",
    "        fn=None,\n",
    "        inputs=None,\n",
    "        outputs=None,\n",
    "        cancels=[submit_event_click, submit_event_enter],\n",
    "    )\n",
    "\n",
    "    # 会話クリアボタン\n",
    "    clear_btn.click(\n",
    "        on_clear_chat,\n",
    "        inputs=[session_state],\n",
    "        outputs=[chatbot, session_state],\n",
    "    )\n",
    "\n",
    "# Colab 環境では share=True でパブリック URL を生成する\n",
    "demo.launch(share=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closing server running on port: 7860\n"
     ]
    }
   ],
   "source": [
    "demo.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Gradio UI の画面](https://github.com/IS-reaction-wheel/GEN_AI_RAG/raw/main/images/notebook_06.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
