{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 02 LangGraph/LangChain によるLLMチャット（Ollama）\n",
    "- Google Colab に Ollama をセットアップ\n",
    "- ChatOllama で LLM に接続\n",
    "- LangGraph によるLLMのメモリ管理\n",
    "- LangGraph/LangChain によるチャット\n",
    "- MCPサーバ（ddg-search）によるweb検索（今回（notebook 02）はユーザがMCPサーバを実行）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Google Colab に Ollama をセットアップ**\n",
    "- Ollama のインストール, 起動, モデルのダウンロードを行う。\n",
    "- 詳細は [01_connect_oss_llm.ipynb](01_connect_oss_llm.ipynb) を参照。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selecting previously unselected package zstd.\n",
      "(Reading database ... 121689 files and directories currently installed.)\n",
      "Preparing to unpack .../zstd_1.4.8+dfsg-3build1_amd64.deb ...\n",
      "Unpacking zstd (1.4.8+dfsg-3build1) ...\n",
      "Setting up zstd (1.4.8+dfsg-3build1) ...\n",
      "Processing triggers for man-db (2.10.2-1) ...\n",
      ">>> Installing ollama to /usr/local\n",
      ">>> Downloading ollama-linux-amd64.tar.zst\n",
      "######################################################################## 100.0%\n",
      ">>> Creating ollama user...\n",
      ">>> Adding ollama user to video group...\n",
      ">>> Adding current user to ollama group...\n",
      ">>> Creating ollama systemd service...\n",
      "\u001b[1m\u001b[31mWARNING:\u001b[m systemd is not running\n",
      "\u001b[1m\u001b[31mWARNING:\u001b[m Unable to detect NVIDIA/AMD GPU. Install lspci or lshw to automatically detect and install GPU dependencies.\n",
      ">>> The Ollama API is now available at 127.0.0.1:11434.\n",
      ">>> Install complete. Run \"ollama\" from the command line.\n",
      "Done!\n",
      "  Model\n",
      "    architecture        gemma3     \n",
      "    parameters          999.89M    \n",
      "    context length      32768      \n",
      "    embedding length    1152       \n",
      "    quantization        Q4_0       \n",
      "\n",
      "  Capabilities\n",
      "    completion    \n",
      "\n",
      "  Parameters\n",
      "    stop           \"<end_of_turn>\"    \n",
      "    temperature    1                  \n",
      "    top_k          64                 \n",
      "    top_p          0.95               \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Ollama のインストール・起動・モデルのダウンロード\n",
    "# 詳細は 01_connect_oss_llm.ipynb を参照\n",
    "import subprocess\n",
    "import time\n",
    "\n",
    "!apt-get install -y -qq zstd\n",
    "!curl -fsSL https://ollama.com/install.sh | sh\n",
    "\n",
    "process = subprocess.Popen(\n",
    "    [\"ollama\", \"serve\"],\n",
    "    stdout=subprocess.DEVNULL,\n",
    "    stderr=subprocess.DEVNULL,\n",
    ")\n",
    "time.sleep(5)\n",
    "\n",
    "!ollama pull gemma3:1b-it-qat > /dev/null 2>&1 && echo \"Done!\"\n",
    "!ollama show gemma3:1b-it-qat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**必要なライブラリをインストール**\n",
    "- 1行にまとめることで pip が全パッケージの依存関係を一括解決する。\n",
    "- 分割すると後からインストールしたパッケージが優先され、依存関係が壊れるリスクがある。\n",
    "- NOTE: Colab では uv ではなく pip を使う。\n",
    "> uv は依存解決の過程で numpy 等をアップグレードし、プリインストール済みの scipy 等を壊すため。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchain-ollama\n",
      "  Downloading langchain_ollama-1.0.1-py3-none-any.whl.metadata (2.5 kB)\n",
      "Requirement already satisfied: langgraph in /usr/local/lib/python3.12/dist-packages (1.0.7)\n",
      "Collecting langgraph\n",
      "  Downloading langgraph-1.0.8-py3-none-any.whl.metadata (7.4 kB)\n",
      "Collecting duckduckgo-mcp-server\n",
      "  Downloading duckduckgo_mcp_server-0.1.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: mcp in /usr/local/lib/python3.12/dist-packages (1.26.0)\n",
      "Requirement already satisfied: langchain-core<2.0.0,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langchain-ollama) (1.2.8)\n",
      "Collecting ollama<1.0.0,>=0.6.0 (from langchain-ollama)\n",
      "  Downloading ollama-0.6.1-py3-none-any.whl.metadata (4.3 kB)\n",
      "Requirement already satisfied: langgraph-checkpoint<5.0.0,>=2.1.0 in /usr/local/lib/python3.12/dist-packages (from langgraph) (4.0.0)\n",
      "Requirement already satisfied: langgraph-prebuilt<1.1.0,>=1.0.7 in /usr/local/lib/python3.12/dist-packages (from langgraph) (1.0.7)\n",
      "Requirement already satisfied: langgraph-sdk<0.4.0,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from langgraph) (0.3.3)\n",
      "Requirement already satisfied: pydantic>=2.7.4 in /usr/local/lib/python3.12/dist-packages (from langgraph) (2.12.3)\n",
      "Requirement already satisfied: xxhash>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from langgraph) (3.6.0)\n",
      "Requirement already satisfied: beautifulsoup4>=4.13.3 in /usr/local/lib/python3.12/dist-packages (from duckduckgo-mcp-server) (4.13.5)\n",
      "Requirement already satisfied: httpx>=0.28.1 in /usr/local/lib/python3.12/dist-packages (from duckduckgo-mcp-server) (0.28.1)\n",
      "Requirement already satisfied: anyio>=4.5 in /usr/local/lib/python3.12/dist-packages (from mcp) (4.12.1)\n",
      "Requirement already satisfied: httpx-sse>=0.4 in /usr/local/lib/python3.12/dist-packages (from mcp) (0.4.3)\n",
      "Requirement already satisfied: jsonschema>=4.20.0 in /usr/local/lib/python3.12/dist-packages (from mcp) (4.26.0)\n",
      "Requirement already satisfied: pydantic-settings>=2.5.2 in /usr/local/lib/python3.12/dist-packages (from mcp) (2.12.0)\n",
      "Requirement already satisfied: pyjwt>=2.10.1 in /usr/local/lib/python3.12/dist-packages (from pyjwt[crypto]>=2.10.1->mcp) (2.11.0)\n",
      "Requirement already satisfied: python-multipart>=0.0.9 in /usr/local/lib/python3.12/dist-packages (from mcp) (0.0.22)\n",
      "Requirement already satisfied: sse-starlette>=1.6.1 in /usr/local/lib/python3.12/dist-packages (from mcp) (3.2.0)\n",
      "Requirement already satisfied: starlette>=0.27 in /usr/local/lib/python3.12/dist-packages (from mcp) (0.50.0)\n",
      "Requirement already satisfied: typing-extensions>=4.9.0 in /usr/local/lib/python3.12/dist-packages (from mcp) (4.15.0)\n",
      "Requirement already satisfied: typing-inspection>=0.4.1 in /usr/local/lib/python3.12/dist-packages (from mcp) (0.4.2)\n",
      "Requirement already satisfied: uvicorn>=0.31.1 in /usr/local/lib/python3.12/dist-packages (from mcp) (0.40.0)\n",
      "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio>=4.5->mcp) (3.11)\n",
      "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4>=4.13.3->duckduckgo-mcp-server) (2.8.3)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx>=0.28.1->duckduckgo-mcp-server) (2026.1.4)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx>=0.28.1->duckduckgo-mcp-server) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx>=0.28.1->duckduckgo-mcp-server) (0.16.0)\n",
      "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.20.0->mcp) (25.4.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.20.0->mcp) (2025.9.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.20.0->mcp) (0.37.0)\n",
      "Requirement already satisfied: rpds-py>=0.25.0 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.20.0->mcp) (0.30.0)\n",
      "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.0.0->langchain-ollama) (1.33)\n",
      "Requirement already satisfied: langsmith<1.0.0,>=0.3.45 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.0.0->langchain-ollama) (0.6.8)\n",
      "Requirement already satisfied: packaging>=23.2.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.0.0->langchain-ollama) (26.0)\n",
      "Requirement already satisfied: pyyaml<7.0.0,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.0.0->langchain-ollama) (6.0.3)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.0.0->langchain-ollama) (9.1.2)\n",
      "Requirement already satisfied: uuid-utils<1.0,>=0.12.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.0.0->langchain-ollama) (0.14.0)\n",
      "Requirement already satisfied: ormsgpack>=1.12.0 in /usr/local/lib/python3.12/dist-packages (from langgraph-checkpoint<5.0.0,>=2.1.0->langgraph) (1.12.2)\n",
      "Requirement already satisfied: orjson>=3.10.1 in /usr/local/lib/python3.12/dist-packages (from langgraph-sdk<0.4.0,>=0.3.0->langgraph) (3.11.7)\n",
      "Requirement already satisfied: python-dotenv>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from mcp[cli]>=1.3.0->duckduckgo-mcp-server) (1.2.1)\n",
      "Requirement already satisfied: typer>=0.16.0 in /usr/local/lib/python3.12/dist-packages (from mcp[cli]>=1.3.0->duckduckgo-mcp-server) (0.21.1)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.7.4->langgraph) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.7.4->langgraph) (2.41.4)\n",
      "Requirement already satisfied: cryptography>=3.4.0 in /usr/local/lib/python3.12/dist-packages (from pyjwt[crypto]>=2.10.1->mcp) (43.0.3)\n",
      "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.12/dist-packages (from uvicorn>=0.31.1->mcp) (8.3.1)\n",
      "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.12/dist-packages (from cryptography>=3.4.0->pyjwt[crypto]>=2.10.1->mcp) (2.0.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<2.0.0,>=1.0.0->langchain-ollama) (3.0.0)\n",
      "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain-ollama) (1.0.0)\n",
      "Requirement already satisfied: requests>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain-ollama) (2.32.4)\n",
      "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain-ollama) (0.25.0)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from typer>=0.16.0->mcp[cli]>=1.3.0->duckduckgo-mcp-server) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.12/dist-packages (from typer>=0.16.0->mcp[cli]>=1.3.0->duckduckgo-mcp-server) (13.9.4)\n",
      "Requirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from cffi>=1.12->cryptography>=3.4.0->pyjwt[crypto]>=2.10.1->mcp) (3.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.0.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain-ollama) (3.4.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.0.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain-ollama) (2.5.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->typer>=0.16.0->mcp[cli]>=1.3.0->duckduckgo-mcp-server) (4.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->typer>=0.16.0->mcp[cli]>=1.3.0->duckduckgo-mcp-server) (2.19.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer>=0.16.0->mcp[cli]>=1.3.0->duckduckgo-mcp-server) (0.1.2)\n",
      "Downloading langchain_ollama-1.0.1-py3-none-any.whl (29 kB)\n",
      "Downloading langgraph-1.0.8-py3-none-any.whl (158 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m158.1/158.1 kB\u001b[0m \u001b[31m17.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading duckduckgo_mcp_server-0.1.1-py3-none-any.whl (6.5 kB)\n",
      "Downloading ollama-0.6.1-py3-none-any.whl (14 kB)\n",
      "Installing collected packages: ollama, langchain-ollama, duckduckgo-mcp-server, langgraph\n",
      "  Attempting uninstall: langgraph\n",
      "    Found existing installation: langgraph 1.0.7\n",
      "    Uninstalling langgraph-1.0.7:\n",
      "      Successfully uninstalled langgraph-1.0.7\n",
      "Successfully installed duckduckgo-mcp-server-0.1.1 langchain-ollama-1.0.1 langgraph-1.0.8 ollama-0.6.1\n"
     ]
    }
   ],
   "source": [
    "# Google Colab に必要なライブラリをインストールする。\n",
    "# 1行にまとめることで pip が全パッケージの依存関係を一括解決する。\n",
    "# NOTE: Colab では uv ではなく pip を使う。uv は依存解決の過程で\n",
    "#       numpy 等をアップグレードし、プリインストール済みの scipy 等を壊すため。\n",
    "%pip install -U langchain-ollama langgraph \\\n",
    "     duckduckgo-mcp-server mcp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ChatOllama で LLM に接続**\n",
    "- 詳細は [01_connect_oss_llm.ipynb](01_connect_oss_llm.ipynb) を参照。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ChatOllama で LLM に接続する。\n",
    "from langchain_ollama import ChatOllama  # type: ignore\n",
    "\n",
    "llm = ChatOllama(\n",
    "    model=\"gemma3:1b-it-qat\",\n",
    "    num_ctx=4096,\n",
    "    num_predict=-1,\n",
    "    temperature=0.8,\n",
    "    top_k=40,\n",
    "    top_p=0.9,\n",
    "    repeat_penalty=1.1,\n",
    "    reasoning=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**LangGraph によるLLMのメモリ管理**\n",
    "- chat_node で、LLM を呼び出した際の処理を設定。\n",
    "> 今回は人間のプロンプトを LLM に入力して、LLM から応答を取得する処理。\n",
    "- LangGraph のグラフを構築し、ノードとエッジを設定し、グラフを compile する。\n",
    "> 今回は単純だが、各処理をつなげて自動化させる場合に効果を発揮する。\n",
    "- InMemorySaver を使用して、LLM との会話をメモリに保存させる。\n",
    "> LLM とのチャット履歴が保持されるようになる。\n",
    "- 作成したグラフを図示する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# LangGraph によるメモリ付きチャットグラフの構築\n",
    "# InMemorySaver を使って、スレッド（thread_id）ごとに会話履歴を保持する\n",
    "from langgraph.graph import StateGraph, MessagesState, START, END  # type: ignore\n",
    "from langgraph.checkpoint.memory import InMemorySaver  # type: ignore\n",
    "from IPython.display import Image, display\n",
    "\n",
    "\n",
    "# チャットノード：現在の会話履歴を LLM に送信し、返答を生成\n",
    "def chat_node(state: MessagesState):\n",
    "    response = llm.invoke(state[\"messages\"])\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "\n",
    "# グラフの構築\n",
    "graph = StateGraph(MessagesState)\n",
    "graph.add_node(\"chat\", chat_node)\n",
    "graph.add_edge(START, \"chat\")\n",
    "graph.add_edge(\"chat\", END)\n",
    "\n",
    "# InMemorySaver：スレッドごとに会話履歴をインメモリで保持\n",
    "memory = InMemorySaver()\n",
    "app = graph.compile(checkpointer=memory)\n",
    "\n",
    "# 構築したグラフを図示\n",
    "display(Image(app.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**LangGraph/LangChain によるチャット**\n",
    "- SystemMessage でシステムプロンプトを、HumanMessage でユーザのプロンプトを設定する。\n",
    "- config で指定した thread_id ごとに会話履歴が保持される。\n",
    "- 2回目の質問で前の会話を踏まえた回答が返れば、メモリが正常に動作している証拠。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 1回目の返答 ===\n",
      "日本で一番高い山は、**富士山**です！\n",
      "\n",
      "富士山は、標高930.4m で、日本最高峰です。\n",
      "\n",
      "何か他に知りたいことはありますか？\n",
      "\n",
      "=== 2回目の返答 ===\n",
      "あなたの名前はジョンです。\n",
      "\n",
      "富士山は標高930.4mです。\n"
     ]
    }
   ],
   "source": [
    "# マルチターン会話のテスト\n",
    "# 同じ thread_id で LLM を複数回呼び出すと、会話履歴が自動的に保持される\n",
    "from langchain_core.messages import HumanMessage, SystemMessage  # type: ignore\n",
    "\n",
    "config = {\"configurable\": {\"thread_id\": \"thread_1\"}}\n",
    "\n",
    "# 1回目の質問（システムプロンプト付き）\n",
    "response1 = app.invoke(\n",
    "    {\n",
    "        \"messages\": [\n",
    "            SystemMessage(content=\"日本語で回答してください。\"),\n",
    "            HumanMessage(content=\"私はジョンです。日本で一番高い山は？\"),\n",
    "        ]\n",
    "    },\n",
    "    config=config,\n",
    ")\n",
    "print(\"=== 1回目の返答 ===\")\n",
    "print(response1[\"messages\"][-1].content)\n",
    "\n",
    "# 2回目の質問（前の会話を踏まえて回答できれば、メモリが動作している証拠）\n",
    "response2 = app.invoke(\n",
    "    {\n",
    "        \"messages\": [\n",
    "            HumanMessage(\n",
    "                content=\"私の名前は何ですか？先ほど答えた山の高さは何メートルですか？\"\n",
    "            ),\n",
    "        ]\n",
    "    },\n",
    "    config=config,\n",
    ")\n",
    "print(\"\\n=== 2回目の返答 ===\")\n",
    "print(response2[\"messages\"][-1].content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**MCPサーバ（duckduckgo-mcp-server）によるWeb検索**\n",
    "- MCPサーバ（duckduckgo-mcp-server）を stdio 経由で起動し、検索ツールを取得する。\n",
    "> AIに自律的に検索ツールを使わせるのではなく、今回（notebook 02）はユーザがMCPサーバを実行。\n",
    "- 検索結果をコンテキストとして LLM に渡し、回答を生成させる。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**MCPサーバ設定の補足**\n",
    "\n",
    "github に記載されているMCP接続の設定は以下\n",
    "> \"mcpServers\": {\"ddg-search\": {\"command\": \"uvx\", \"args\": [\"duckduckgo-mcp-server\"]}}\n",
    "\n",
    "今回は、pip install済なので、command=\"duckduckgo-mcp-server\" で直接起動。uvx経由ではないので、args=[] としてよい。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**stdio_client接続の補足**\n",
    "\n",
    "async with stdio_client(server_params) as (read, write):を、次に修正している。\n",
    "> async with stdio_client(server_params, errlog=open(os.devnull, \"w\")) as (read, write):\n",
    "\n",
    "これはColab環境特有の処置で、Colabの内部構造が標準的なstderrに対応していないため。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 利用可能なツール ===\n",
      "  - search: \n",
      "    Search DuckDuckGo and return formatted results.\n",
      "\n",
      "    Args:\n",
      "        query: The search query string\n",
      "        max_results: Maximum number of results to return (default: 10)\n",
      "        ctx: MCP context for logging\n",
      "    \n",
      "  - fetch_content: \n",
      "    Fetch and parse content from a webpage URL.\n",
      "\n",
      "    Args:\n",
      "        url: The webpage URL to fetch content from\n",
      "        ctx: MCP context for logging\n",
      "    \n",
      "\n",
      "=== 検索結果 ===\n",
      "Found 3 search results:\n",
      "\n",
      "1. 人口推計（2025年（令和7年）8月確定値、2026年（令和8年）1月概算値） （2026年1月20日公表）\n",
      "   URL: https://www.stat.go.jp/data/jinsui/new.html\n",
      "   Summary: 2026年（令和8年）1月報 （2025年（令和7年）8月確定値、2026年（令和8年）1月概算値）（PDF：346KB） 統計表 統計表 （政府統計の総合窓口「e-Stat」 掲載の統計表） ※ その他の推計結果 各月1日現在人口 「全国：年齢（5歳階級）、男女別人口」 各年10月1日現在人口 「全国：年齢（各歳）、男女別 ...\n",
      "\n",
      "2. 日本の人口・男女年齢階級別2025年 - Memorva メモルヴァ\n",
      "   URL: https://memorva.jp/ranking/japan/soumu_population_age.php\n",
      "   Summary: 2025年8月6日、総務省が発表した「住民基本台帳に基づく人口、人口動態及び世帯数」によると、2025年（令和7年）1月1日現在の日本の総人口は1億2,433万690人だった。 前年から55万4,485人減少した。\n",
      "\n",
      "3. 2025年の日本人の人口は1億2065万3227人 前年より90万減 | ツギノジダイ\n",
      "   URL: https://smbiz.asahi.com/article/15986172\n",
      "   Summary: 総務省が発表した2025年1月1日時点の住民基本台帳によると、総人口は1億2433万690人で前年より55万4485人 (0.44%)減少しました。 日本人は1億2065万3227人で0.75%減少し、16年連続の減少となりました。 一方、外国人は367万7463人で10.65%増加し、最多を更新しました。\n",
      "\n",
      "\n",
      "=== LLMの回答 ===\n",
      "2025年日本人の総人口は、1億2433万690人でした。\n"
     ]
    }
   ],
   "source": [
    "# MCPサーバ（duckduckgo-mcp-server）に接続して検索し、結果をLLMに渡す\n",
    "# Colab環境ではasyncioのインポートなしで非同期処理（async/await）が使用可能\n",
    "# 通常のPython環境では、asyncioのインポートが必要\n",
    "import os\n",
    "from mcp import ClientSession, StdioServerParameters  # type: ignore\n",
    "from mcp.client.stdio import stdio_client  # type: ignore\n",
    "from langchain_core.messages import HumanMessage, SystemMessage  # type: ignore\n",
    "\n",
    "# MCPサーバへの接続パラメータ（stdio 経由で起動）\n",
    "server_params = StdioServerParameters(\n",
    "    command=\"duckduckgo-mcp-server\",\n",
    "    args=[],\n",
    ")\n",
    "\n",
    "query = \"2025年 日本の総人口\"\n",
    "\n",
    "# MCPサーバに接続し、検索ツールを取得して実行\n",
    "# NOTE: Colab の stderr は fileno() 未対応のため、errlog をファイルに迂回する\n",
    "async with stdio_client(server_params, errlog=open(os.devnull, \"w\")) as (read, write):\n",
    "    async with ClientSession(read, write) as session:\n",
    "        await session.initialize()\n",
    "\n",
    "        # MCPサーバが提供するツール一覧を表示\n",
    "        tools = await session.list_tools()\n",
    "        print(\"=== 利用可能なツール ===\")\n",
    "        for t in tools.tools:\n",
    "            print(f\"  - {t.name}: {t.description}\")\n",
    "\n",
    "        # 検索ツールを実行（max_results で検索結果数を制限）\n",
    "        result = await session.call_tool(\"search\", {\"query\": query, \"max_results\": 3})\n",
    "\n",
    "# 検索結果を取得\n",
    "search_results = result.content[0].text\n",
    "print(\"\\n=== 検索結果 ===\")\n",
    "print(search_results)\n",
    "\n",
    "# 検索結果をコンテキストとしてLLMに質問\n",
    "config = {\"configurable\": {\"thread_id\": \"thread_search\"}}\n",
    "response = app.invoke(\n",
    "    {\n",
    "        \"messages\": [\n",
    "            SystemMessage(\n",
    "                content=\"以下の検索結果をもとに、ユーザの質問に日本語で簡潔に回答してください。\"\n",
    "            ),\n",
    "            HumanMessage(content=f\"質問: {query}\\n\\n検索結果:\\n{search_results}\"),\n",
    "        ]\n",
    "    },\n",
    "    config=config,\n",
    ")\n",
    "print(\"\\n=== LLMの回答 ===\")\n",
    "print(response[\"messages\"][-1].content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
