{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c894267f",
   "metadata": {},
   "source": [
    "### 01 Google Colab から OSS LLM への接続\n",
    "- OPTION：VS Code から Google Colab の GPUサーバに接続\n",
    "- LLMに接続（Hugging Faceのログインなし）\n",
    "- LLMに接続（Hugging Faceのログイン<u>**あり**</u>）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2072c39",
   "metadata": {},
   "source": [
    "**OPTION：VS Code から Google Colab の GPUサーバに接続する手順**\n",
    "1. カーネルでcolabを選択\n",
    "2. New Colab Serverを選択し、GPUを選択, GPUの種類（T4など）を選択\n",
    "3. !nvidia-smiでGPUが選択されているか確認"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f4b2cfd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Feb  5 11:05:08 2026       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
      "| N/A   44C    P8              9W /   70W |       0MiB /  15360MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "# !nvidia-smiでGPUが選択されているか確認\n",
    "# コマンドの「!」は、python ではなく、OS のコマンドという意味（シェルエスケープ）\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "221ceebd",
   "metadata": {},
   "source": [
    "**LLMに接続（Hugging Faceのログインなし）**\n",
    "1. Google Colabに必要なライブラリをインストールする。\n",
    "2. torchのGPUの使用可否、Hugging Faceのログイン状況を確認する。\n",
    "3. transformersを使ってLLMモデルをHugging Faceから読み込みする。\n",
    "4. モデルを読み込めているか確認する。確認後、一旦、モデルを削除。\n",
    "5. transformersを使ってLLMモデルを<u>**量子化して**</u>読み込みする。\n",
    "6. 読み込んだモデルのメモリ消費量が減っているか確認する。確認後、一旦、モデルを削除。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "432718be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (5.0.0)\n",
      "Requirement already satisfied: accelerate in /usr/local/lib/python3.12/dist-packages (1.12.0)\n",
      "Collecting bitsandbytes\n",
      "  Downloading bitsandbytes-0.49.1-py3-none-manylinux_2_24_x86_64.whl.metadata (10 kB)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.20.3)\n",
      "Requirement already satisfied: huggingface-hub<2.0,>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (1.3.4)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.0.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2025.11.3)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.2)\n",
      "Requirement already satisfied: typer-slim in /usr/local/lib/python3.12/dist-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.7.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from accelerate) (5.9.5)\n",
      "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from accelerate) (2.9.0+cu126)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=1.3.0->transformers) (2025.3.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=1.3.0->transformers) (1.2.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=1.3.0->transformers) (0.28.1)\n",
      "Requirement already satisfied: shellingham in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=1.3.0->transformers) (1.5.4)\n",
      "Requirement already satisfied: typing-extensions>=4.1.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=1.3.0->transformers) (4.15.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (75.2.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (3.6.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.80)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (11.3.0.4)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (10.3.7.77)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (11.7.1.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.5.4.2)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (2.27.5)\n",
      "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (3.3.20)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.77)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.85)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (1.11.1.6)\n",
      "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (3.5.0)\n",
      "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from typer-slim->transformers) (8.3.1)\n",
      "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers) (4.12.1)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers) (2026.1.4)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers) (1.0.9)\n",
      "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers) (3.11)\n",
      "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers) (0.16.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=2.0.0->accelerate) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.3)\n",
      "Downloading bitsandbytes-0.49.1-py3-none-manylinux_2_24_x86_64.whl (59.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.1/59.1 MB\u001b[0m \u001b[31m19.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: bitsandbytes\n",
      "Successfully installed bitsandbytes-0.49.1\n"
     ]
    }
   ],
   "source": [
    "# Google Colabに必要なライブラリをインストールする。\n",
    "# transformers accelerateはLLMの読み込み、bitsandbytesは量子化に必要\n",
    "# -U は最新版をインストールするという指示\n",
    "%pip install -U transformers accelerate bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ca0501b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "# torchでcudaが使用可能か確認する。\n",
    "import torch  # type: ignore\n",
    "\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "30675bdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:104: UserWarning: \n",
      "Error while fetching `HF_TOKEN` secret value from your vault: 'Requesting secret HF_TOKEN timed out. Secrets can only be fetched when running from the Colab UI.'.\n",
      "You are not authenticated with the Hugging Face Hub in this notebook.\n",
      "If the error persists, please let us know by opening an issue on GitHub (https://github.com/huggingface/huggingface_hub/issues/new).\n",
      "  warnings.warn(\n",
      "Not logged in!\n",
      "WARNING:huggingface_hub._login:Not logged in!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ログインしていません。\n"
     ]
    }
   ],
   "source": [
    "# Hugging Faceにはログインしていない状態です。\n",
    "import os\n",
    "from huggingface_hub import logout, whoami  # type: ignore\n",
    "\n",
    "# もし既にHugging Faceにログインしている場合は、一旦ログアウトする\n",
    "logout()  # トークンキャッシュを削除\n",
    "if \"HF_TOKEN\" in os.environ:\n",
    "    del os.environ[\"HF_TOKEN\"]  # セッション内の環境変数も削除\n",
    "\n",
    "try:\n",
    "    user_info = whoami()\n",
    "    print(f\"ログイン成功: {user_info['name']} としてアクセス中\")\n",
    "except Exception:\n",
    "    print(\"ログインしていません。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "314db8f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea522911afa5401fb086569294d73280",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/902 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d897aff6e9854dd29afb15b74465319a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae3f64e4e5f7429dbbd26100dc38eb8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/33.4M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d863cacf67d4523aa5bffbd19d01f61",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/35.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "710c879c2a5841499d7ffde4f0bfe8a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/670 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9fb048f0f44413f95b409eb0158efbd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "chat_template.jinja: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n",
      "WARNING:huggingface_hub.utils._http:Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58b143305334403d96e901d706f7af93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/2.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3bb70c95ed540d7b5f473eceb494500",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/340 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ab23c09bf3e4822b9fdd6ddbe53afad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/233 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "モデルのメモリ消費量: 1.86 GB\n"
     ]
    }
   ],
   "source": [
    "# transformersを使ってLLMモデルをHugging Faceから読み込みする。\n",
    "# モデル読み込み後、モデルの大きさを出力させる。\n",
    "import torch  # type: ignore\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer  # type: ignore\n",
    "\n",
    "model_name = \"unsloth/gemma-3-1b-it\"  # Hugging Faceのモデルの名称\n",
    "\n",
    "# トークナイザ：テキストをLLMが読み込みできる形式に変換する\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# LLM モデルの読み込み\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,  # Hugging Faceから読み込みするモデルの名称\n",
    "    dtype=torch.bfloat16,  # 読み込みするモデルのデータ型\n",
    "    device_map=\"auto\",  # 読み込んだモデルを CPU / GPUに自動で割り当てする指示\n",
    ")\n",
    "\n",
    "# モデルのメモリ消費量（バイト）を取得し、GBに変換して表示\n",
    "memory_footprint_bytes = model.get_memory_footprint()\n",
    "memory_footprint_gb = memory_footprint_bytes / (1024 * 1024 * 1024)\n",
    "print(f\"モデルのメモリ消費量: {memory_footprint_gb:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0eb3be05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'generated_text': [{'role': 'user', 'content': '日本で一番高い山は？日本語で回答して下さい。'},\n",
       "   {'role': 'assistant', 'content': '日本の最高峰である富士山です。\\n'}]}]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 読み込んだLLMの動作を確認する。\n",
    "# 実際にLLMに質問し、会話を生成する。\n",
    "from transformers import GenerationConfig, pipeline  # type: ignore\n",
    "\n",
    "# LLMに生成させる文章の計算条件\n",
    "# LM Studio（ローカルLLMを取り扱うデスクトップアプリ）のデフォルト値を流用\n",
    "# LLMモデル内部にもgeneration_configが設定されており、設定しなくても動作するが、明示的に設定している。\n",
    "generation_config = GenerationConfig(\n",
    "    do_sample=True,  # Trueにしないと後段の設定が効かない。（Falseだとgreedy法）\n",
    "    temperature=0.8,  # LLMが生成する文章のランダムさのパラメータ（値が大きい:多様、小さい:決定的）\n",
    "    top_k=40,  # LLMが生成するランダムな文章の妥当性を向上させるパラメータ（値が小さい:候補が絞られる）\n",
    "    top_p=0.9,  # LLMが生成するランダムな文章の妥当性を向上させるパラメータ（値が小さい:候補が絞られる）\n",
    "    repetition_penalty=1.1,  # LLMが不要に繰り返し同じ出力をすることを抑制させるパラメータ（1.0=なし、値が大きい:抑制強い）\n",
    "    max_new_tokens=512,  # LLMが生成する文章（トークン数）の上限（≠LM Studio）\n",
    ")\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "# LLMに入力するプロンプト\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"日本で一番高い山は？日本語で回答して下さい。\"},\n",
    "]\n",
    "\n",
    "# LLMにプロンプトを送信し、会話を生成\n",
    "pipe(messages, generation_config=generation_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6df3afe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 読み込んだLLMを削除し、メモリを解放する。\n",
    "import gc\n",
    "import torch  # type: ignore\n",
    "\n",
    "del model\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b8a71d4",
   "metadata": {},
   "source": [
    "LLM モデルを**量子化**して読み込みさせる場合。**BitsAndBytesConfigはGPU動作が前提**のため、CPUで動作させると非常に遅いので注意。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "198698ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34e6645519004473ace71f8ee1af5236",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/340 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "モデルのメモリ消費量: 0.89 GB\n"
     ]
    }
   ],
   "source": [
    "# transformersを使ってLLMモデルを**量子化して**読み込みする。\n",
    "# モデル読み込み後、モデルの大きさを出力させる。\n",
    "# **BitsAndBytesConfigはGPU動作が前提**のため、CPUで動作させると非常に遅いので注意。\n",
    "import torch  # type: ignore\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig  # type: ignore\n",
    "\n",
    "model_name = \"unsloth/gemma-3-1b-it\"\n",
    "\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,  # 4 ビットに量子化された形式で読み込むように指定\n",
    "    bnb_4bit_quant_type=\"nf4\",  # 4 ビット量子化のデータ型として NF4 を指定\n",
    "    bnb_4bit_use_double_quant=True,  # 二重量子化の指定\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,  # 計算時のデータ型を指定\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=quantization_config,\n",
    "    dtype=torch.bfloat16,  # bnb_4bit_compute_dtypeと一致させる必要がある\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "# モデルのメモリ消費量（バイト）を取得し、GBに変換して表示\n",
    "memory_footprint_bytes = model.get_memory_footprint()\n",
    "memory_footprint_gb = memory_footprint_bytes / (1024 * 1024 * 1024)\n",
    "print(f\"モデルのメモリ消費量: {memory_footprint_gb:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b1c065e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'generated_text': [{'role': 'user', 'content': '日本で一番高い山は？日本語で回答して下さい。'},\n",
       "   {'role': 'assistant',\n",
       "    'content': '日本の最高峰である富士山です。\\n\\n*   **標高:** 3,776.28m (12,389ft)\\n*   **場所:** 長野県御洲町\\n*   **特徴:** 日本第三の山であり、多くの登山者が訪れる人気の山です。\\n\\nより詳しい情報については、以下のサイトもご参照ください。\\n\\n*   **富士山登山 - Wikipedia:** [https://ja.wikipedia.org/wiki/富士山](https://ja.wikipedia.org/wiki/富士山)\\n*   **富士山の高さと歴史 - 日本登山:** [https://www.j-mountain.jp/fujisan/](https://www.j-mountain.jp/fujisan/)'}]}]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# **BitsAndBytesConfigはGPU動作が前提**のため、CPUで動作させると非常に遅いので注意。\n",
    "# 読み込んだLLMの動作を確認する。\n",
    "# 実際にLLMに質問し、会話を生成する。\n",
    "from transformers import GenerationConfig, pipeline  # type: ignore\n",
    "\n",
    "# LLMに生成させる文章の計算条件\n",
    "# LM Studio（ローカルLLMを取り扱うデスクトップアプリ）のデフォルト値を流用\n",
    "# LLMモデル内部にもgeneration_configが設定されており、設定しなくても動作するが、明示的に設定している。\n",
    "generation_config = GenerationConfig(\n",
    "    do_sample=True,  # Trueにしないと後段の設定が効かない。（Falseだとgreedy法）\n",
    "    temperature=0.8,  # LLMが生成する文章のランダムさのパラメータ（値が大きい:多様、小さい:決定的）\n",
    "    top_k=40,  # LLMが生成するランダムな文章の妥当性を向上させるパラメータ（値が小さい:候補が絞られる）\n",
    "    top_p=0.9,  # LLMが生成するランダムな文章の妥当性を向上させるパラメータ（値が小さい:候補が絞られる）\n",
    "    repetition_penalty=1.1,  # LLMが不要に繰り返し同じ出力をすることを抑制させるパラメータ（1.0=なし、値が大きい:抑制強い）\n",
    "    max_new_tokens=512,  # LLMが生成する文章（トークン数）の上限（≠LM Studio）\n",
    ")\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "# LLMに入力するプロンプト\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"日本で一番高い山は？日本語で回答して下さい。\"},\n",
    "]\n",
    "\n",
    "# LLMにプロンプトを送信し、会話を生成\n",
    "pipe(messages, generation_config=generation_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a451468d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 読み込んだLLMを削除し、メモリを解放する。\n",
    "import gc\n",
    "import torch  # type: ignore\n",
    "\n",
    "del model\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1bf9613",
   "metadata": {},
   "source": [
    "**LLMに接続（Hugging Faceのログイン<u>あり</u>）**\n",
    "1. Google Colabに必要なライブラリをインストールする。torchのGPUの使用可否も確認。\n",
    "2. Hugging FaceのAPIキーを設定する。\n",
    "    - ケース1：Google Drive をマウントして、.envファイルから読み込み。（VS Code経由のため、Colab拡張機能の設定が必要）\n",
    "    - ケース2：ダイアログを表示させ、直接APIキーを入力する。\n",
    "3. transformersを使って<u>**認証が必要な**</u>LLMモデルをHugging Faceから読み込みする。\n",
    "4. モデルを読み込めているか確認する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "809d6ca2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (5.0.0)\n",
      "Requirement already satisfied: accelerate in /usr/local/lib/python3.12/dist-packages (1.12.0)\n",
      "Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.12/dist-packages (0.49.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.20.3)\n",
      "Requirement already satisfied: huggingface-hub<2.0,>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (1.3.4)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.0.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2025.11.3)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.2)\n",
      "Requirement already satisfied: typer-slim in /usr/local/lib/python3.12/dist-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.7.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from accelerate) (5.9.5)\n",
      "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from accelerate) (2.9.0+cu126)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=1.3.0->transformers) (2025.3.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=1.3.0->transformers) (1.2.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=1.3.0->transformers) (0.28.1)\n",
      "Requirement already satisfied: shellingham in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=1.3.0->transformers) (1.5.4)\n",
      "Requirement already satisfied: typing-extensions>=4.1.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=1.3.0->transformers) (4.15.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (75.2.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (3.6.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.80)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (11.3.0.4)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (10.3.7.77)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (11.7.1.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.5.4.2)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (2.27.5)\n",
      "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (3.3.20)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.77)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.85)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (1.11.1.6)\n",
      "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (3.5.0)\n",
      "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from typer-slim->transformers) (8.3.1)\n",
      "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers) (4.12.1)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers) (2026.1.4)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers) (1.0.9)\n",
      "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers) (3.11)\n",
      "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers) (0.16.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=2.0.0->accelerate) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.3)\n"
     ]
    }
   ],
   "source": [
    "# Google Colabに必要なライブラリをインストールする。\n",
    "# transformers accelerateはLLMの読み込み、bitsandbytesは量子化に必要\n",
    "# -U は最新版をインストールするという指示\n",
    "%pip install -U transformers accelerate bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3229bc1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "# torchでcudaが使用可能か確認する。\n",
    "import torch  # type: ignore\n",
    "\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be53f449",
   "metadata": {},
   "source": [
    "**Hugging FaceのAPIキーを設定する。**\n",
    "- ケース1：Google Drive をマウントして、.envファイルから読み込み。（VS Code経由のため、Colab拡張機能の設定が必要）\n",
    "    > コード実行時は、webブラウザが開き、Google Drive接続の認証が必要になる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bc52c691",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "# Google Driveのマウント（VS Code経由のため、Colab拡張機能の設定が必要）\n",
    "# コード実行時は、webブラウザが開き、Google Drive接続の認証が必要\n",
    "from google.colab import drive  # type: ignore\n",
    "\n",
    "drive.mount(\"/content/drive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "73bfe88d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "現在の場所: /content/drive/MyDrive/GEN_AI_RAG\n",
      "ファイル一覧:\n",
      ".git\n",
      "app\n",
      "data\n",
      "notebook\n",
      "outputs\n",
      "src\n",
      "scripts\n",
      "tests\n",
      ".python-version\n",
      ".venv\n",
      "pyproject.toml\n",
      "uv.lock\n",
      ".vscode\n",
      ".claude\n",
      ".gitignore\n",
      "CLAUDE.md\n",
      "README.md\n",
      ".env\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ワーキングディレクトリをColabに認識させ、直下にあるファイルを確認\n",
    "# .envファイルを確認することが目的。\n",
    "import os\n",
    "from dotenv import load_dotenv  # type: ignore\n",
    "\n",
    "# ワーキングディレクトリを設定し、そのフォルダへ移動する\n",
    "# ColabカーネルからVS Codeのワーキングディレクトリを読み取れないので、ハードコードしている\n",
    "working_folder = \"/content/drive/MyDrive/GEN_AI_RAG\"\n",
    "os.chdir(working_folder)\n",
    "\n",
    "# 現在の場所を確認\n",
    "print(\"現在の場所:\", os.getcwd())\n",
    "\n",
    "# ファイル一覧を表示\n",
    "print(\"ファイル一覧:\\n\" + \"\\n\".join(os.listdir()))\n",
    "\n",
    "# .envファイル（環境変数が記載されているファイル）を読み込み\n",
    "load_dotenv(\".env\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ccbd2eed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HF_TOKEN は登録されています。 (先頭数文字: hf_...)\n"
     ]
    }
   ],
   "source": [
    "# 環境変数（Hugging Faceのアクセストークン：HF_TOKEN）を読み込めているか確認\n",
    "import os\n",
    "\n",
    "# HF_TOKEN が存在するか確認\n",
    "hf_token = os.environ.get(\"HF_TOKEN\")\n",
    "\n",
    "if hf_token:\n",
    "    print(f\"HF_TOKEN は登録されています。 (先頭数文字: {hf_token[:3]}...)\")\n",
    "else:\n",
    "    print(\"HF_TOKEN は登録されていません。\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "386b5651",
   "metadata": {},
   "source": [
    "**Hugging FaceのAPIキーを設定する。**\n",
    "- ケース2：ダイアログを表示させ、直接APIキーを入力する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "562f739f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "トークンはセットされています。\n"
     ]
    }
   ],
   "source": [
    "# Hugging Faceのアクセストークンを読み込みする。\n",
    "# 入力後、Google Colab内の環境変数として設定する。\n",
    "import os\n",
    "from getpass import getpass\n",
    "\n",
    "# 環境変数に既にあればそれを使う。なければ入力を促す。\n",
    "hf_token = os.getenv(\"HF_TOKEN\") or getpass(\"Enter your Hugging Face token: \")\n",
    "os.environ[\"HF_TOKEN\"] = hf_token\n",
    "\n",
    "if hf_token:\n",
    "    print(\"トークンはセットされています。\")\n",
    "else:\n",
    "    print(\"トークンはセットされていません。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e9874c83",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n",
      "WARNING:huggingface_hub._login:Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ログイン成功: IS-reaction-wheel としてアクセス中\n"
     ]
    }
   ],
   "source": [
    "# Hugging Faceにログインする。\n",
    "from huggingface_hub import login, whoami  # type: ignore\n",
    "\n",
    "# Hugging Faceにログイン（要、Hugging Faceのアクセストークン）\n",
    "login(hf_token)\n",
    "\n",
    "# Hugging Faceにログインできているかを確認\n",
    "try:\n",
    "    user_info = whoami()\n",
    "    print(f\"ログイン成功: {user_info['name']} としてアクセス中\")\n",
    "except Exception:\n",
    "    print(\"ログインしていません。\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1f9ef45",
   "metadata": {},
   "source": [
    "**transformersを使って認証が必要なLLMモデルをHugging Faceから読み込みする。**\n",
    "- Googleのgemma-3-1b-itを使用します。事前にHugging Faceのサイトで、認証を受けることが必要です。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4eba69e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c68d1a1653b4fb5a24ed288c8250bfa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/899 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11738c021a15425d8f53241f038fb6da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/1.16M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d7cc894bc9d43068886bf642bb9e394",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/33.4M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d5bcfdae7a54dce94e1a53295e2f92d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/35.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "535ea3fb18fd4b1eb142568f3172382f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/662 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82f5d70a541d442e81e244c974d6b507",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/2.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c74e3b1227454783807ed367d459aa6f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/340 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c03f4ec782ba42398cb0514be1a87a48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/215 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "モデルのメモリ消費量: 1.86 GB\n"
     ]
    }
   ],
   "source": [
    "# transformersを使ってLLMモデルをHugging Faceから読み込みする。\n",
    "# モデル読み込み後、モデルの大きさを出力させる。\n",
    "import torch  # type: ignore\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer  # type: ignore\n",
    "\n",
    "model_name = \"google/gemma-3-1b-it\"  # Hugging Faceのモデルの名称\n",
    "\n",
    "# トークナイザ：テキストをLLMが読み込みできる形式に変換する\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# LLM モデルの読み込み\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,  # Hugging Faceから読み込みするモデルの名称\n",
    "    dtype=torch.bfloat16,  # 読み込みするモデルのデータ型\n",
    "    device_map=\"auto\",  # 読み込んだモデルを CPU / GPUに自動で割り当てする指示\n",
    ")\n",
    "\n",
    "# モデルのメモリ消費量（バイト）を取得し、GBに変換して表示\n",
    "memory_footprint_bytes = model.get_memory_footprint()\n",
    "memory_footprint_gb = memory_footprint_bytes / (1024 * 1024 * 1024)\n",
    "print(f\"モデルのメモリ消費量: {memory_footprint_gb:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ad5cdc42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'generated_text': [{'role': 'user', 'content': '日本で一番高い山は？日本語で回答して下さい。'},\n",
       "   {'role': 'assistant', 'content': '富士山です。'}]}]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 読み込んだLLMの動作を確認する。\n",
    "# 実際にLLMに質問し、会話を生成する。\n",
    "from transformers import GenerationConfig, pipeline  # type: ignore\n",
    "\n",
    "# LLMに生成させる文章の計算条件\n",
    "# LM Studio（ローカルLLMを取り扱うデスクトップアプリ）のデフォルト値を流用\n",
    "# LLMモデル内部にもgeneration_configが設定されており、設定しなくても動作するが、明示的に設定している。\n",
    "generation_config = GenerationConfig(\n",
    "    do_sample=True,  # Trueにしないと後段の設定が効かない。（Falseだとgreedy法）\n",
    "    temperature=0.8,  # LLMが生成する文章のランダムさのパラメータ（値が大きい:多様、小さい:決定的）\n",
    "    top_k=40,  # LLMが生成するランダムな文章の妥当性を向上させるパラメータ（値が小さい:候補が絞られる）\n",
    "    top_p=0.9,  # LLMが生成するランダムな文章の妥当性を向上させるパラメータ（値が小さい:候補が絞られる）\n",
    "    repetition_penalty=1.1,  # LLMが不要に繰り返し同じ出力をすることを抑制させるパラメータ（1.0=なし、値が大きい:抑制強い）\n",
    "    max_new_tokens=512,  # LLMが生成する文章（トークン数）の上限（≠LM Studio）\n",
    ")\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "# LLMに入力するプロンプト\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"日本で一番高い山は？日本語で回答して下さい。\"},\n",
    "]\n",
    "\n",
    "# LLMにプロンプトを送信し、会話を生成\n",
    "pipe(messages, generation_config=generation_config)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
