{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c894267f",
   "metadata": {},
   "source": [
    "### 01 Google Colab から OSS LLM への接続\n",
    "- OPTION：VS Code から Google Colab の GPUサーバに接続\n",
    "- LLMに接続（Hugging Faceのログインなし）\n",
    "- LLMに接続（Hugging Faceのログイン<u>**あり**</u>）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2072c39",
   "metadata": {},
   "source": [
    "**OPTION：VS Code から Google Colab の GPUサーバに接続する手順**\n",
    "1. カーネルでcolabを選択\n",
    "2. New Colab Serverを選択し、GPUを選択, GPUの種類（T4など）を選択\n",
    "3. !nvidia-smiでGPUが選択されているか確認"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f4b2cfd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: line 1: nvidia-smi: command not found\n"
     ]
    }
   ],
   "source": [
    "# !nvidia-smiでGPUが選択されているか確認\n",
    "# コマンドの「!」は、python ではなく、OS のコマンドという意味（シェルエスケープ）\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "221ceebd",
   "metadata": {},
   "source": [
    "**LLMに接続（Hugging Faceのログインなし）**\n",
    "1. Google Colabに必要なライブラリをインストールする。\n",
    "2. torchのGPUの使用可否、Hugging Faceのログイン状況を確認する。\n",
    "3. transformersを使ってLLMモデルをHugging Faceから読み込みする。\n",
    "4. モデルを読み込めているか確認する。確認後、一旦、モデルを削除。\n",
    "5. transformersを使ってLLMモデルを<u>**量子化して**</u>読み込みする。\n",
    "6. 読み込んだモデルのメモリ消費量が減っているか確認する。確認後、一旦、モデルを削除。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "432718be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (5.0.0)\n",
      "Requirement already satisfied: accelerate in /usr/local/lib/python3.12/dist-packages (1.12.0)\n",
      "Collecting bitsandbytes\n",
      "  Downloading bitsandbytes-0.49.1-py3-none-manylinux_2_24_x86_64.whl.metadata (10 kB)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.20.3)\n",
      "Requirement already satisfied: huggingface-hub<2.0,>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (1.3.4)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.0.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2025.11.3)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.2)\n",
      "Requirement already satisfied: typer-slim in /usr/local/lib/python3.12/dist-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.7.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from accelerate) (5.9.5)\n",
      "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from accelerate) (2.9.0+cpu)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=1.3.0->transformers) (2025.3.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=1.3.0->transformers) (1.2.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=1.3.0->transformers) (0.28.1)\n",
      "Requirement already satisfied: shellingham in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=1.3.0->transformers) (1.5.4)\n",
      "Requirement already satisfied: typing-extensions>=4.1.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=1.3.0->transformers) (4.15.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (75.2.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (3.6.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (3.1.6)\n",
      "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from typer-slim->transformers) (8.3.1)\n",
      "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers) (4.12.1)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers) (2026.1.4)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers) (1.0.9)\n",
      "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers) (3.11)\n",
      "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers) (0.16.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=2.0.0->accelerate) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.3)\n",
      "Downloading bitsandbytes-0.49.1-py3-none-manylinux_2_24_x86_64.whl (59.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.1/59.1 MB\u001b[0m \u001b[31m19.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: bitsandbytes\n",
      "Successfully installed bitsandbytes-0.49.1\n"
     ]
    }
   ],
   "source": [
    "# Google Colabに必要なライブラリをインストールする。\n",
    "# transformers accelerateはLLMの読み込み、bitsandbytesは量子化に必要\n",
    "# -U は最新版をインストールするという指示\n",
    "%pip install -U transformers accelerate bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ca0501b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "# torchでcudaが使用可能か確認する。\n",
    "import torch  # type: ignore\n",
    "\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "30675bdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:104: UserWarning: \n",
      "Error while fetching `HF_TOKEN` secret value from your vault: 'Requesting secret HF_TOKEN timed out. Secrets can only be fetched when running from the Colab UI.'.\n",
      "You are not authenticated with the Hugging Face Hub in this notebook.\n",
      "If the error persists, please let us know by opening an issue on GitHub (https://github.com/huggingface/huggingface_hub/issues/new).\n",
      "  warnings.warn(\n",
      "Not logged in!\n",
      "WARNING:huggingface_hub._login:Not logged in!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ログインしていません。\n"
     ]
    }
   ],
   "source": [
    "# Hugging Faceにはログインしていない状態です。\n",
    "import os\n",
    "from huggingface_hub import logout, whoami  # type: ignore\n",
    "\n",
    "# もし既にHugging Faceにログインしている場合は、一旦ログアウトする\n",
    "logout()  # トークンキャッシュを削除\n",
    "if \"HF_TOKEN\" in os.environ:\n",
    "    del os.environ[\"HF_TOKEN\"]  # セッション内の環境変数も削除\n",
    "\n",
    "try:\n",
    "    user_info = whoami()\n",
    "    print(f\"ログイン成功: {user_info['name']} としてアクセス中\")\n",
    "except Exception:\n",
    "    print(\"ログインしていません。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "314db8f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5c1de198d9d4fe894f975e5ac0bd40d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/902 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40d46968a1294205b7c6b5653d66fe72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb4ae86a836e434da9df625da5d26d00",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/33.4M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ae3743a996d4da7b44d821696f5c1ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/35.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d62c3e4b1cc642e1b21481ae9eddc298",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/670 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53dbd23f373c47b1bdb61d37e13c1955",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "chat_template.jinja: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58bb4d490bc74cc8a596285017ca19a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/2.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73cdd5d1ba8847058cd67436484f8022",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/340 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87cf9068fae940d281e05dfed048e5d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/233 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "モデルのメモリ消費量: 1.86 GB\n"
     ]
    }
   ],
   "source": [
    "# transformersを使ってLLMモデルをHugging Faceから読み込みする。\n",
    "# モデル読み込み後、モデルの大きさを出力させる。\n",
    "import torch  # type: ignore\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer  # type: ignore\n",
    "\n",
    "model_name = \"unsloth/gemma-3-1b-it\"  # Hugging Faceのモデルの名称\n",
    "\n",
    "# トークナイザ：テキストをLLMが読み込みできる形式に変換する\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# LLM モデルの読み込み\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,  # Hugging Faceから読み込みするモデルの名称\n",
    "    dtype=torch.bfloat16,  # 読み込みするモデルのデータ型\n",
    "    device_map=\"auto\",  # 読み込んだモデルを CPU / GPUに自動で割り当てする指示\n",
    ")\n",
    "\n",
    "# モデルのメモリ消費量（バイト）を取得し、GBに変換して表示\n",
    "memory_footprint_bytes = model.get_memory_footprint()\n",
    "memory_footprint_gb = memory_footprint_bytes / (1024 * 1024 * 1024)\n",
    "print(f\"モデルのメモリ消費量: {memory_footprint_gb:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0eb3be05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'generated_text': [{'role': 'user', 'content': '日本で一番高い山は？日本語で回答して下さい。'},\n",
       "   {'role': 'assistant', 'content': '富士山です。'}]}]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 読み込んだLLMの動作を確認する。\n",
    "# 実際にLLMに質問し、会話を生成する。\n",
    "from transformers import GenerationConfig, pipeline  # type: ignore\n",
    "\n",
    "# LLMに生成させる文章の計算条件\n",
    "# LM Studio（ローカルLLMを取り扱うデスクトップアプリ）のデフォルト値を流用\n",
    "# LLMモデル内部にもgeneration_configが設定されており、設定しなくても動作するが、明示的に設定している。\n",
    "generation_config = GenerationConfig(\n",
    "    do_sample=True,  # Trueにしないと後段の設定が効かない。（Falseだとgreedy法）\n",
    "    temperature=0.8,  # LLMが生成する文章のランダムさのパラメータ（値が大きい:多様、小さい:決定的）\n",
    "    top_k=40,  # LLMが生成するランダムな文章の妥当性を向上させるパラメータ（値が小さい:候補が絞られる）\n",
    "    top_p=0.9,  # LLMが生成するランダムな文章の妥当性を向上させるパラメータ（値が小さい:候補が絞られる）\n",
    "    repetition_penalty=1.1,  # LLMが不要に繰り返し同じ出力をすることを抑制させるパラメータ（1.0=なし、値が大きい:抑制強い）\n",
    "    max_new_tokens=512,  # LLMが生成する文章（トークン数）の上限（≠LM Studio）\n",
    ")\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "# LLMに入力するプロンプト\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"日本で一番高い山は？日本語で回答して下さい。\"},\n",
    "]\n",
    "\n",
    "# LLMにプロンプトを送信し、会話を生成\n",
    "pipe(messages, generation_config=generation_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6df3afe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 読み込んだLLMを削除し、メモリを解放する。\n",
    "import gc\n",
    "import torch  # type: ignore\n",
    "\n",
    "del model\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b8a71d4",
   "metadata": {},
   "source": [
    "LLM モデルを**量子化**して読み込みさせる場合。**BitsAndBytesConfigはGPU動作が前提**のため、CPUで動作させると非常に遅いので注意。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "198698ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "559f422eabf646dbb314c4ccc8e33476",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/340 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-3902602389.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m model = AutoModelForCausalLM.from_pretrained(\n\u001b[0m\u001b[1;32m     18\u001b[0m     \u001b[0mmodel_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mquantization_config\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mquantization_config\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/auto/auto_factory.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    370\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mmodel_class\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig_class\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub_configs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"text_config\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    371\u001b[0m                 \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_text_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 372\u001b[0;31m             return model_class.from_pretrained(\n\u001b[0m\u001b[1;32m    373\u001b[0m                 \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mmodel_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mhub_kwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    374\u001b[0m             )\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   4107\u001b[0m             \u001b[0mdownload_kwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdownload_kwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4108\u001b[0m         )\n\u001b[0;32m-> 4109\u001b[0;31m         \u001b[0mload_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_load_pretrained_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheckpoint_files\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mload_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4110\u001b[0m         \u001b[0mload_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_finalize_load_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mload_config\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mload_info\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4111\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Set model in evaluation mode to deactivate Dropout modules by default\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36m_load_pretrained_model\u001b[0;34m(cls, model, state_dict, checkpoint_files, load_config)\u001b[0m\n\u001b[1;32m   4229\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4230\u001b[0m             missing_keys, unexpected_keys, mismatched_keys, disk_offload_index, conversion_errors = (\n\u001b[0;32m-> 4231\u001b[0;31m                 convert_and_load_state_dict_in_model(\n\u001b[0m\u001b[1;32m   4232\u001b[0m                     \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4233\u001b[0m                     \u001b[0mstate_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmerged_state_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/core_model_loading.py\u001b[0m in \u001b[0;36mconvert_and_load_state_dict_in_model\u001b[0;34m(model, state_dict, load_config, tp_plan, dtype_plan, disk_offload_index)\u001b[0m\n\u001b[1;32m   1215\u001b[0m                 \u001b[0mpbar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrefresh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1216\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1217\u001b[0;31m                     realized_value, conversion_errors = mapping.convert(\n\u001b[0m\u001b[1;32m   1218\u001b[0m                         \u001b[0mfirst_param_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1219\u001b[0m                         \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/core_model_loading.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(self, layer_name, model, config, hf_quantizer, missing_keys, conversion_errors)\u001b[0m\n\u001b[1;32m    705\u001b[0m                 \u001b[0mlayer_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconversion_errors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcollected_tensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayer_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquantization_operation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    706\u001b[0m             ):\n\u001b[0;32m--> 707\u001b[0;31m                 collected_tensors = self.quantization_operation.convert(\n\u001b[0m\u001b[1;32m    708\u001b[0m                     \u001b[0mcollected_tensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    709\u001b[0m                     \u001b[0msource_patterns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msource_patterns\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/integrations/bitsandbytes.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(self, input_dict, full_layer_name, model, **kwargs)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0mold_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_parameter_or_buffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfull_layer_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m         \u001b[0mnew_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbnb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mParams4bit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequires_grad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mold_value\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__dict__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m         \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_hf_initialized\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mfull_layer_name\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnew_value\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/bitsandbytes/nn/modules.py\u001b[0m in \u001b[0;36mto\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    344\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    345\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdevice\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"meta\"\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbnb_quantized\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 346\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_quantize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    347\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    348\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquant_state\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/bitsandbytes/nn/modules.py\u001b[0m in \u001b[0;36m_quantize\u001b[0;34m(self, device)\u001b[0m\n\u001b[1;32m    299\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_quantize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m         \u001b[0mw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 301\u001b[0;31m         w_4bit, quant_state = bnb.functional.quantize_4bit(\n\u001b[0m\u001b[1;32m    302\u001b[0m             \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m             \u001b[0mblocksize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblocksize\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/bitsandbytes/functional.py\u001b[0m in \u001b[0;36mquantize_4bit\u001b[0;34m(A, absmax, out, blocksize, compress_statistics, quant_type, quant_storage)\u001b[0m\n\u001b[1;32m    862\u001b[0m     \u001b[0minput_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mA\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    863\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 864\u001b[0;31m     _out, _absmax = torch.ops.bitsandbytes.quantize_4bit.default(\n\u001b[0m\u001b[1;32m    865\u001b[0m         \u001b[0mA\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    866\u001b[0m         \u001b[0mblocksize\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_ops.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    839\u001b[0m     \u001b[0;31m# that are named \"self\". This way, all the aten ops can be called by kwargs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    840\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m/\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0m_P\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0m_P\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0m_T\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 841\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    842\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    843\u001b[0m     \u001b[0;31m# Use positional-only argument to avoid naming collision with aten ops arguments\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_compile.py\u001b[0m in \u001b[0;36minner\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     51\u001b[0m                 \u001b[0mfn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__dynamo_disable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdisable_fn\u001b[0m  \u001b[0;31m# type: ignore[attr-defined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mdisable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minner\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py\u001b[0m in \u001b[0;36m_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1042\u001b[0m                 \u001b[0m_maybe_set_eval_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_callback_from_stance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallback\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1043\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1044\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1045\u001b[0m                 \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1046\u001b[0m                     \u001b[0mset_eval_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/library.py\u001b[0m in \u001b[0;36mfunc_no_dynamo\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    730\u001b[0m             \u001b[0;34m@\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_disable_dynamo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    731\u001b[0m             \u001b[0;32mdef\u001b[0m \u001b[0mfunc_no_dynamo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 732\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    733\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    734\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkeys\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/bitsandbytes/backends/default/ops.py\u001b[0m in \u001b[0;36m_\u001b[0;34m(A, blocksize, quant_type, quant_storage)\u001b[0m\n\u001b[1;32m    249\u001b[0m     \u001b[0;31m# Quantize with the lookup table\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m     \u001b[0mcode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCODE\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mquant_type\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscaled\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscaled\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m     \u001b[0mquantized\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscaled\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mcode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muint8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m     \u001b[0;31m# Pack two quantized values per byte\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# transformersを使ってLLMモデルを**量子化して**読み込みする。\n",
    "# モデル読み込み後、モデルの大きさを出力させる。\n",
    "# **BitsAndBytesConfigはGPU動作が前提**のため、CPUで動作させると非常に遅いので注意。\n",
    "import torch  # type: ignore\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig  # type: ignore\n",
    "\n",
    "model_name = \"unsloth/gemma-3-1b-it\"\n",
    "\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,  # 4 ビットに量子化された形式で読み込むように指定\n",
    "    bnb_4bit_quant_type=\"nf4\",  # 4 ビット量子化のデータ型として NF4 を指定\n",
    "    bnb_4bit_use_double_quant=True,  # 二重量子化の指定\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,  # 計算時のデータ型を指定\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=quantization_config,\n",
    "    dtype=torch.bfloat16,  # bnb_4bit_compute_dtypeと一致させる必要がある\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "# モデルのメモリ消費量（バイト）を取得し、GBに変換して表示\n",
    "memory_footprint_bytes = model.get_memory_footprint()\n",
    "memory_footprint_gb = memory_footprint_bytes / (1024 * 1024 * 1024)\n",
    "print(f\"モデルのメモリ消費量: {memory_footprint_gb:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b1c065e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'generated_text': [{'role': 'user', 'content': '日本で一番高い山は？日本語で回答して下さい。'},\n",
       "   {'role': 'assistant', 'content': '日本で一番高い山は、**富士山**です。\\n\\n標高は430mです。\\n'}]}]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# **BitsAndBytesConfigはGPU動作が前提**のため、CPUで動作させると非常に遅いので注意。\n",
    "# 読み込んだLLMの動作を確認する。\n",
    "# 実際にLLMに質問し、会話を生成する。\n",
    "from transformers import GenerationConfig, pipeline  # type: ignore\n",
    "\n",
    "# LLMに生成させる文章の計算条件\n",
    "# LM Studio（ローカルLLMを取り扱うデスクトップアプリ）のデフォルト値を流用\n",
    "# LLMモデル内部にもgeneration_configが設定されており、設定しなくても動作するが、明示的に設定している。\n",
    "generation_config = GenerationConfig(\n",
    "    do_sample=True,  # Trueにしないと後段の設定が効かない。（Falseだとgreedy法）\n",
    "    temperature=0.8,  # LLMが生成する文章のランダムさのパラメータ（値が大きい:多様、小さい:決定的）\n",
    "    top_k=40,  # LLMが生成するランダムな文章の妥当性を向上させるパラメータ（値が小さい:候補が絞られる）\n",
    "    top_p=0.9,  # LLMが生成するランダムな文章の妥当性を向上させるパラメータ（値が小さい:候補が絞られる）\n",
    "    repetition_penalty=1.1,  # LLMが不要に繰り返し同じ出力をすることを抑制させるパラメータ（1.0=なし、値が大きい:抑制強い）\n",
    "    max_new_tokens=512,  # LLMが生成する文章（トークン数）の上限（≠LM Studio）\n",
    ")\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "# LLMに入力するプロンプト\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"日本で一番高い山は？日本語で回答して下さい。\"},\n",
    "]\n",
    "\n",
    "# LLMにプロンプトを送信し、会話を生成\n",
    "pipe(messages, generation_config=generation_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a451468d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 読み込んだLLMを削除し、メモリを解放する。\n",
    "import gc\n",
    "import torch  # type: ignore\n",
    "\n",
    "del model\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1bf9613",
   "metadata": {},
   "source": [
    "**LLMに接続（Hugging Faceのログイン<u>あり</u>）**\n",
    "1. Google Colabに必要なライブラリをインストールする。torchのGPUの使用可否も確認。\n",
    "2. Hugging FaceのAPIキーを設定する。\n",
    "    - ケース1：Google Drive をマウントして、.envファイルから読み込み。（VS Code経由のため、Colab拡張機能の設定が必要）\n",
    "    - ケース2：ダイアログを表示させ、直接APIキーを入力する。\n",
    "3. transformersを使って<u>**認証が必要な**</u>LLMモデルをHugging Faceから読み込みする。\n",
    "4. モデルを読み込めているか確認する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "809d6ca2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (5.0.0)\n",
      "Requirement already satisfied: accelerate in /usr/local/lib/python3.12/dist-packages (1.12.0)\n",
      "Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.12/dist-packages (0.49.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.20.3)\n",
      "Requirement already satisfied: huggingface-hub<2.0,>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (1.3.4)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.0.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2025.11.3)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.2)\n",
      "Requirement already satisfied: typer-slim in /usr/local/lib/python3.12/dist-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.7.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from accelerate) (5.9.5)\n",
      "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from accelerate) (2.9.0+cpu)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=1.3.0->transformers) (2025.3.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=1.3.0->transformers) (1.2.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=1.3.0->transformers) (0.28.1)\n",
      "Requirement already satisfied: shellingham in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=1.3.0->transformers) (1.5.4)\n",
      "Requirement already satisfied: typing-extensions>=4.1.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=1.3.0->transformers) (4.15.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (75.2.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (3.6.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (3.1.6)\n",
      "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from typer-slim->transformers) (8.3.1)\n",
      "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers) (4.12.1)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers) (2026.1.4)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers) (1.0.9)\n",
      "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers) (3.11)\n",
      "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers) (0.16.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=2.0.0->accelerate) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.3)\n"
     ]
    }
   ],
   "source": [
    "# Google Colabに必要なライブラリをインストールする。\n",
    "# transformers accelerateはLLMの読み込み、bitsandbytesは量子化に必要\n",
    "# -U は最新版をインストールするという指示\n",
    "%pip install -U transformers accelerate bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3229bc1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "# torchでcudaが使用可能か確認する。\n",
    "import torch  # type: ignore\n",
    "\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be53f449",
   "metadata": {},
   "source": [
    "**Hugging FaceのAPIキーを設定する。**\n",
    "- ケース1：Google Drive をマウントして、.envファイルから読み込み。（VS Code経由のため、Colab拡張機能の設定が必要）\n",
    "    > コード実行時は、webブラウザが開き、Google Drive接続の認証が必要になる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc52c691",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "# Google Driveのマウント（VS Code経由のため、Colab拡張機能の設定が必要）\n",
    "# コード実行時は、webブラウザが開き、Google Drive接続の認証が必要\n",
    "from google.colab import drive  # type: ignore\n",
    "\n",
    "drive.mount(\"/content/drive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "73bfe88d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "現在の場所: /content/drive/MyDrive/GEN_AI_RAG\n",
      "ファイル一覧:\n",
      ".git\n",
      "notebook\n",
      "app\n",
      "data\n",
      "outputs\n",
      "src\n",
      "scripts\n",
      "tests\n",
      ".python-version\n",
      ".venv\n",
      "pyproject.toml\n",
      "uv.lock\n",
      ".vscode\n",
      ".claude\n",
      ".gitignore\n",
      "CLAUDE.md\n",
      "README.md\n",
      ".env\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ワーキングディレクトリをColabに認識させ、直下にあるファイルを確認\n",
    "# .envファイルを確認することが目的。\n",
    "import os\n",
    "from dotenv import load_dotenv  # type: ignore\n",
    "\n",
    "# ワーキングディレクトリを設定し、そのフォルダへ移動する\n",
    "# ColabカーネルからVS Codeのワーキングディレクトリを読み取れないので、ハードコードしている\n",
    "working_folder = \"/content/drive/MyDrive/GEN_AI_RAG\"\n",
    "os.chdir(working_folder)\n",
    "\n",
    "# 現在の場所を確認\n",
    "print(\"現在の場所:\", os.getcwd())\n",
    "\n",
    "# ファイル一覧を表示\n",
    "print(\"ファイル一覧:\\n\" + \"\\n\".join(os.listdir()))\n",
    "\n",
    "# .envファイル（環境変数が記載されているファイル）を読み込み\n",
    "load_dotenv(\".env\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ccbd2eed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HF_TOKEN は登録されています。 (先頭数文字: hf_...)\n"
     ]
    }
   ],
   "source": [
    "# 環境変数（Hugging Faceのアクセストークン：HF_TOKEN）を読み込めているか確認\n",
    "import os\n",
    "\n",
    "# HF_TOKEN が存在するか確認\n",
    "hf_token = os.environ.get(\"HF_TOKEN\")\n",
    "\n",
    "if hf_token:\n",
    "    print(f\"HF_TOKEN は登録されています。 (先頭数文字: {hf_token[:3]}...)\")\n",
    "else:\n",
    "    print(\"HF_TOKEN は登録されていません。\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "386b5651",
   "metadata": {},
   "source": [
    "**Hugging FaceのAPIキーを設定する。**\n",
    "- ケース2：ダイアログを表示させ、直接APIキーを入力する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "562f739f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "トークンはセットされています。\n"
     ]
    }
   ],
   "source": [
    "# Hugging Faceのアクセストークンを読み込みする。\n",
    "# 入力後、Google Colab内の環境変数として設定する。\n",
    "import os\n",
    "from getpass import getpass\n",
    "\n",
    "# 環境変数に既にあればそれを使う。なければ入力を促す。\n",
    "hf_token = os.getenv(\"HF_TOKEN\") or getpass(\"Enter your Hugging Face token: \")\n",
    "os.environ[\"HF_TOKEN\"] = hf_token\n",
    "\n",
    "if hf_token:\n",
    "    print(\"トークンはセットされています。\")\n",
    "else:\n",
    "    print(\"トークンはセットされていません。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e9874c83",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n",
      "WARNING:huggingface_hub._login:Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ログイン成功: IS-reaction-wheel としてアクセス中\n"
     ]
    }
   ],
   "source": [
    "# Hugging Faceにログインする。\n",
    "from huggingface_hub import login, whoami  # type: ignore\n",
    "\n",
    "# Hugging Faceにログイン（要、Hugging Faceのアクセストークン）\n",
    "login(hf_token)\n",
    "\n",
    "# Hugging Faceにログインできているかを確認\n",
    "try:\n",
    "    user_info = whoami()\n",
    "    print(f\"ログイン成功: {user_info['name']} としてアクセス中\")\n",
    "except Exception:\n",
    "    print(\"ログインしていません。\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1f9ef45",
   "metadata": {},
   "source": [
    "**transformersを使って認証が必要なLLMモデルをHugging Faceから読み込みする。**\n",
    "- Googleのgemma-3-1b-itを使用します。事前にHugging Faceのサイトで、認証を受けることが必要です。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4eba69e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6bedb5fd6af470bb16fe05f5207eb0f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/899 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "227402614a0e46c2870ec3dc8f2752a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/1.16M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4545dd0f26b4a0d9723d76c991788fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/33.4M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54e72a7768e04a2b8322b0582890d781",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/35.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25af48d32c73493cbd9715569906bbe7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/662 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "968eebf2c81647a6ac6280146f3420e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/2.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8274ce5de25f4a6e934eb892e21ed0c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/340 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "acacf8cfe3304c24b9dd95c756583dc6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/215 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "モデルのメモリ消費量: 1.86 GB\n"
     ]
    }
   ],
   "source": [
    "# transformersを使ってLLMモデルをHugging Faceから読み込みする。\n",
    "# モデル読み込み後、モデルの大きさを出力させる。\n",
    "import torch  # type: ignore\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer  # type: ignore\n",
    "\n",
    "model_name = \"google/gemma-3-1b-it\"  # Hugging Faceのモデルの名称\n",
    "\n",
    "# トークナイザ：テキストをLLMが読み込みできる形式に変換する\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# LLM モデルの読み込み\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,  # Hugging Faceから読み込みするモデルの名称\n",
    "    dtype=torch.bfloat16,  # 読み込みするモデルのデータ型\n",
    "    device_map=\"auto\",  # 読み込んだモデルを CPU / GPUに自動で割り当てする指示\n",
    ")\n",
    "\n",
    "# モデルのメモリ消費量（バイト）を取得し、GBに変換して表示\n",
    "memory_footprint_bytes = model.get_memory_footprint()\n",
    "memory_footprint_gb = memory_footprint_bytes / (1024 * 1024 * 1024)\n",
    "print(f\"モデルのメモリ消費量: {memory_footprint_gb:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ad5cdc42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'generated_text': [{'role': 'user', 'content': '日本で一番高い山は？日本語で回答して下さい。'},\n",
       "   {'role': 'assistant', 'content': '日本の最高峰である富士山です。\\n'}]}]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 読み込んだLLMの動作を確認する。\n",
    "# 実際にLLMに質問し、会話を生成する。\n",
    "from transformers import GenerationConfig, pipeline  # type: ignore\n",
    "\n",
    "# LLMに生成させる文章の計算条件\n",
    "# LM Studio（ローカルLLMを取り扱うデスクトップアプリ）のデフォルト値を流用\n",
    "# LLMモデル内部にもgeneration_configが設定されており、設定しなくても動作するが、明示的に設定している。\n",
    "generation_config = GenerationConfig(\n",
    "    do_sample=True,  # Trueにしないと後段の設定が効かない。（Falseだとgreedy法）\n",
    "    temperature=0.8,  # LLMが生成する文章のランダムさのパラメータ（値が大きい:多様、小さい:決定的）\n",
    "    top_k=40,  # LLMが生成するランダムな文章の妥当性を向上させるパラメータ（値が小さい:候補が絞られる）\n",
    "    top_p=0.9,  # LLMが生成するランダムな文章の妥当性を向上させるパラメータ（値が小さい:候補が絞られる）\n",
    "    repetition_penalty=1.1,  # LLMが不要に繰り返し同じ出力をすることを抑制させるパラメータ（1.0=なし、値が大きい:抑制強い）\n",
    "    max_new_tokens=512,  # LLMが生成する文章（トークン数）の上限（≠LM Studio）\n",
    ")\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "# LLMに入力するプロンプト\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"日本で一番高い山は？日本語で回答して下さい。\"},\n",
    "]\n",
    "\n",
    "# LLMにプロンプトを送信し、会話を生成\n",
    "pipe(messages, generation_config=generation_config)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
