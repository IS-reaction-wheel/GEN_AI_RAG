{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 01 Google Colab から OSS LLM への接続（Ollama）\n",
    "- OPTION：VS Code から Google Colab の GPUサーバに接続\n",
    "- Ollama のインストールと起動（Colab 環境特有の作業）\n",
    "- LLM モデルのダウンロードと動作確認\n",
    "- LangChain（ChatOllama）経由での接続"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**OPTION：VS Code から Google Colab の GPUサーバに接続する手順**\n",
    "1. カーネルでcolabを選択\n",
    "2. New Colab Serverを選択し、GPUを選択, GPUの種類（T4など）を選択\n",
    "3. !nvidia-smiでGPUが選択されているか確認"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat Feb  7 09:58:57 2026       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
      "| N/A   69C    P8             11W /   70W |       0MiB /  15360MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "# !nvidia-smiでGPUが選択されているか確認\n",
    "# コマンドの「!」は、python ではなく、OS のコマンドという意味（シェルエスケープ）\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ollama のインストールと起動（Colab 環境特有の作業）\n",
    "1. Google Colab に Ollama をインストールする。\n",
    "2. Ollama をバックグラウンドで起動する。\n",
    "3. Ollama の動作確認。\n",
    "\n",
    "**Ollama のインストールの補足**\n",
    "- !：Notebook上で Linuxのシステムコマンドを実行するための記号\n",
    "- apt-get install：Linux（ColabのOS）でソフトをインストールするコマンド\n",
    "- -y：インストール中に「よろしいですか？[Y/n]」と聞かれた際、自動的に「Yes（はい）」と答えるオプション\n",
    "- !curl -fsSL https://ollama.com/install.sh | sh\n",
    "> Linux に Ollama をダウンロード/インストールするコマンド（公式サイトより）(https://docs.ollama.com/linux#manual-install)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading package lists... Done\n",
      "Building dependency tree... Done\n",
      "Reading state information... Done\n",
      "The following NEW packages will be installed:\n",
      "  zstd\n",
      "0 upgraded, 1 newly installed, 0 to remove and 41 not upgraded.\n",
      "Need to get 603 kB of archives.\n",
      "After this operation, 1,695 kB of additional disk space will be used.\n",
      "Get:1 http://archive.ubuntu.com/ubuntu jammy/main amd64 zstd amd64 1.4.8+dfsg-3build1 [603 kB]\n",
      "Fetched 603 kB in 1s (487 kB/s)\n",
      "Selecting previously unselected package zstd.\n",
      "(Reading database ... 121689 files and directories currently installed.)\n",
      "Preparing to unpack .../zstd_1.4.8+dfsg-3build1_amd64.deb ...\n",
      "Unpacking zstd (1.4.8+dfsg-3build1) ...\n",
      "Setting up zstd (1.4.8+dfsg-3build1) ...\n",
      "Processing triggers for man-db (2.10.2-1) ...\n",
      ">>> Installing ollama to /usr/local\n",
      ">>> Downloading ollama-linux-amd64.tar.zst\n",
      "######################################################################## 100.0%\n",
      ">>> Creating ollama user...\n",
      ">>> Adding ollama user to video group...\n",
      ">>> Adding current user to ollama group...\n",
      ">>> Creating ollama systemd service...\n",
      "\u001b[1m\u001b[31mWARNING:\u001b[m systemd is not running\n",
      "\u001b[1m\u001b[31mWARNING:\u001b[m Unable to detect NVIDIA/AMD GPU. Install lspci or lshw to automatically detect and install GPU dependencies.\n",
      ">>> The Ollama API is now available at 127.0.0.1:11434.\n",
      ">>> Install complete. Run \"ollama\" from the command line.\n"
     ]
    }
   ],
   "source": [
    "# Ollama をインストールする。\n",
    "# zstd: Ollama のインストールスクリプトがアーカイブ展開に必要とする圧縮ツール\n",
    "# curl でインストールスクリプトをダウンロードし、シェルで実行する。\n",
    "!apt-get install -y zstd\n",
    "!curl -fsSL https://ollama.com/install.sh | sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ollama の起動の補足**\n",
    "- `subprocess.Popen`を使って、Ollamaのサーバーモード（serve）を起動\n",
    "- `Popen`：バックグラウンドで実行させる指示\n",
    "- `stdout, stderr=subprocess.DEVNULL`：Ollama の標準出力, エラー出力先を`subprocess.DEVNULL`（ゴミ箱のような場所）に変更している。（不要な出力を表示しないようにするため。）\n",
    "- `time.sleep`：Ollama サーバを起動後、サーバに接続完了するまで処理を待機させる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ollama サーバを起動しました。（PID: 491）\n"
     ]
    }
   ],
   "source": [
    "# Ollama をバックグラウンドで起動/実行\n",
    "# Ollama の標準出力/エラー出力は DEVNULL に破棄\n",
    "import subprocess\n",
    "import time\n",
    "\n",
    "process = subprocess.Popen(\n",
    "    [\"ollama\", \"serve\"],\n",
    "    stdout=subprocess.DEVNULL,\n",
    "    stderr=subprocess.DEVNULL,\n",
    ")\n",
    "time.sleep(5)  # 起動を待つ\n",
    "print(f\"Ollama サーバを起動しました。（PID: {process.pid}）\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ollama version is 0.15.5\n",
      "NAME    ID    SIZE    MODIFIED \n"
     ]
    }
   ],
   "source": [
    "# Ollama の動作確認。バージョンと稼働状態を表示する。\n",
    "!ollama --version\n",
    "!ollama list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LLM モデルのダウンロードと動作確認\n",
    "1. Ollama で LLM モデルをダウンロード（pull）する。\n",
    "> Ollama の LLM ライブラリ（https://ollama.com/library）\n",
    "    \n",
    "> ライブラリにないものを使用したい場合は、Hugging Face（https://huggingface.co/） から GGUF ファイルをダウンロードし、Ollama の設定ファイル（Modelfile）を作成して、Ollama に取り込みする。本リポジトリでは、この方法は使用しない。\n",
    "2. Ollama 経由で LLM に質問し、動作を確認する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# Ollama で LLM モデルをダウンロードする。\n",
    "# Colab だとプログレスバーが文字化けするので、出力を非表示にする。\n",
    "!ollama pull gemma3:1b-it-qat > /dev/null 2>&1 && echo \"Done!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME                ID              SIZE      MODIFIED      \n",
      "gemma3:1b-it-qat    b491bd3989c6    1.0 GB    2 seconds ago    \n"
     ]
    }
   ],
   "source": [
    "# ダウンロードしたモデルの一覧を確認する。\n",
    "!ollama list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Model\n",
      "    architecture        gemma3     \n",
      "    parameters          999.89M    \n",
      "    context length      32768      \n",
      "    embedding length    1152       \n",
      "    quantization        Q4_0       \n",
      "\n",
      "  Capabilities\n",
      "    completion    \n",
      "\n",
      "  Parameters\n",
      "    stop           \"<end_of_turn>\"    \n",
      "    temperature    1                  \n",
      "    top_k          64                 \n",
      "    top_p          0.95               \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# モデルの詳細を確認する。\n",
    "!ollama show gemma3:1b-it-qat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "日本で一番高い山は**富士山**です。\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Ollama CLI で直接 LLM に質問し、動作を確認する。\n",
    "!ollama run gemma3:1b-it-qat \"日本で一番高い山は？日本語で回答して下さい。\" 2> /dev/null"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LangChain（ChatOllama）経由での接続\n",
    "1. 必要なライブラリをインストールする。\n",
    "2. ChatOllama で LLM に接続し、質問する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchain-ollama\n",
      "  Downloading langchain_ollama-1.0.1-py3-none-any.whl.metadata (2.5 kB)\n",
      "Requirement already satisfied: langchain-core<2.0.0,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langchain-ollama) (1.2.8)\n",
      "Collecting ollama<1.0.0,>=0.6.0 (from langchain-ollama)\n",
      "  Downloading ollama-0.6.1-py3-none-any.whl.metadata (4.3 kB)\n",
      "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.0.0->langchain-ollama) (1.33)\n",
      "Requirement already satisfied: langsmith<1.0.0,>=0.3.45 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.0.0->langchain-ollama) (0.6.8)\n",
      "Requirement already satisfied: packaging>=23.2.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.0.0->langchain-ollama) (26.0)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.0.0->langchain-ollama) (2.12.3)\n",
      "Requirement already satisfied: pyyaml<7.0.0,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.0.0->langchain-ollama) (6.0.3)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.0.0->langchain-ollama) (9.1.2)\n",
      "Requirement already satisfied: typing-extensions<5.0.0,>=4.7.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.0.0->langchain-ollama) (4.15.0)\n",
      "Requirement already satisfied: uuid-utils<1.0,>=0.12.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.0.0->langchain-ollama) (0.14.0)\n",
      "Requirement already satisfied: httpx>=0.27 in /usr/local/lib/python3.12/dist-packages (from ollama<1.0.0,>=0.6.0->langchain-ollama) (0.28.1)\n",
      "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx>=0.27->ollama<1.0.0,>=0.6.0->langchain-ollama) (4.12.1)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx>=0.27->ollama<1.0.0,>=0.6.0->langchain-ollama) (2026.1.4)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx>=0.27->ollama<1.0.0,>=0.6.0->langchain-ollama) (1.0.9)\n",
      "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx>=0.27->ollama<1.0.0,>=0.6.0->langchain-ollama) (3.11)\n",
      "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx>=0.27->ollama<1.0.0,>=0.6.0->langchain-ollama) (0.16.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<2.0.0,>=1.0.0->langchain-ollama) (3.0.0)\n",
      "Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain-ollama) (3.11.7)\n",
      "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain-ollama) (1.0.0)\n",
      "Requirement already satisfied: requests>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain-ollama) (2.32.4)\n",
      "Requirement already satisfied: xxhash>=3.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain-ollama) (3.6.0)\n",
      "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain-ollama) (0.25.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain-core<2.0.0,>=1.0.0->langchain-ollama) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain-core<2.0.0,>=1.0.0->langchain-ollama) (2.41.4)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain-core<2.0.0,>=1.0.0->langchain-ollama) (0.4.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.0.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain-ollama) (3.4.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.0.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain-ollama) (2.5.0)\n",
      "Downloading langchain_ollama-1.0.1-py3-none-any.whl (29 kB)\n",
      "Downloading ollama-0.6.1-py3-none-any.whl (14 kB)\n",
      "Installing collected packages: ollama, langchain-ollama\n",
      "Successfully installed langchain-ollama-1.0.1 ollama-0.6.1\n"
     ]
    }
   ],
   "source": [
    "# LangChain の Ollama 連携ライブラリをインストールする。\n",
    "# -U は最新版をインストールするという指示\n",
    "%pip install -U langchain-ollama"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ChatOllama の補足**\n",
    "> LM Studio（ローカルLLMを扱うデスクトップアプリ）のデフォルト値を流用しています。\n",
    "\n",
    "> ChatOllama のデフォルト値はリンク先を参照。(https://reference.langchain.com/python/integrations/langchain_ollama/?h=#langchain_ollama.ChatOllama.num_predict)\n",
    "\n",
    "| パラメータ | 説明 | 特徴 |\n",
    "| :--- | :--- | :--- |\n",
    "| model | モデル名称 | Ollama の NAME と一致させる |\n",
    "| num_ctx | コンテキスト長 | LLM が扱える文章（入力＋出力）の長さ |\n",
    "| num_predict | 最大出力トークン数 | LLM が出力する文章の長さ（-1で無制限） |\n",
    "| temperature | 回答の多様性 | 0.0 は確定的, 1.0 に近いほど回答がランダムになる |\n",
    "| top_k | 候補語の制限 | LLMが出力する文章のランダムさの調整パラメータ（値が小さい:候補が絞られる） |\n",
    "| top_p | 累積確率の制限 | LLMが出力する文章のランダムさの調整パラメータ（値が小さい:候補が絞られる） |\n",
    "| repeat_penalty | 繰り返し抑制 | 同じ言葉のループを防ぐ。（1.0=なし, 値が大きい:抑制強い） |\n",
    "| reasoning | 思考プロセスの制御 | 推論モデルで <think> タグを扱う設定（None でモデルのデフォルト設定が適用） |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "日本で一番高い山は、**富士山**です。\n"
     ]
    }
   ],
   "source": [
    "# ChatOllama で LLM に接続し、質問する。\n",
    "# ChatOllama は Ollama サーバ経由で LLM を呼び出す LangChain のラッパー。\n",
    "from langchain_ollama import ChatOllama  # type: ignore\n",
    "from langchain_core.messages import HumanMessage  # type: ignore\n",
    "\n",
    "llm = ChatOllama(\n",
    "    model=\"gemma3:1b-it-qat\",\n",
    "    num_ctx=4096,\n",
    "    num_predict=-1,\n",
    "    temperature=0.8,\n",
    "    top_k=40,\n",
    "    top_p=0.9,\n",
    "    repeat_penalty=1.1,\n",
    "    reasoning=None,\n",
    ")\n",
    "\n",
    "# LLM に質問する\n",
    "messages = [HumanMessage(content=\"日本で一番高い山は？日本語で回答して下さい。\")]\n",
    "response = llm.invoke(messages)\n",
    "print(response.content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
