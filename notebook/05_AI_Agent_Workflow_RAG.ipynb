{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 05 AI Agent Workflow + RAG（検索拡張生成）\n",
    "gpt-oss:20bを使用する構成のため、**Colab GPU は L4 を使用すること。**\n",
    "- 必要なライブラリをインストール\n",
    "- Google Colab に Ollama をセットアップ\n",
    "  - LLM モデルは gpt-oss:20b を使用（Ollama）\n",
    "  - Embedding モデルは ruri-v3-310m を使用（Sentence Transformers）\n",
    "  - Reranker モデルは cl-nagoya/ruri-v3-reranker-310m を使用（Sentence Transformers）\n",
    "- JAXA（宇宙航空研究開発機構）のリポジトリからデータをダウンロードして読み込み\n",
    "> [井澤克彦, 市川信一郎, 高速回転ホイール: 高速回転ホイール開発を通しての知見, 宇宙航空研究開発機構研究開発報告, 2008](https://jaxa.repo.nii.ac.jp/records/2149)\n",
    "- データの前処理\n",
    "  - markdown に変換（MarkItDown を使用）\n",
    "  - Unicode正規化 (NFKC), 1文字行ブロックの除去, 空行圧縮\n",
    "  - チャンク分割\n",
    "    - LangChain の SpacyTextSplitter を使用\n",
    "    - spaCy の日本語モデルは、ja_ginza を使用\n",
    "- ベクトルデータベースの構築（ChromaDB, インメモリ）\n",
    "- 検索機能の実装\n",
    "  - キーワード検索 @ BM25（spaCyで形態素解析の前処理が必要）\n",
    "  - Embedding model によるセマンティック検索\n",
    "  - ハイブリッド検索\n",
    "  - Reranker による再順位付け\n",
    "  - 検索機能をLLM の tool として定義\n",
    "- LangGraph による Workflow の実装\n",
    "  1. ユーザの質問を入力。\n",
    "  2. ユーザの質問に回答するためのタスク分割, 作成。\n",
    "  3. tool による検索。\n",
    "  4. tool による検索を終えて回答作成に進むか判断。再調査なら 3 に戻る。\n",
    "  5. ユーザへの回答の作成と提示。\n",
    "- 動作確認"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**必要なライブラリをインストール**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "opentelemetry-exporter-otlp-proto-http 1.38.0 requires opentelemetry-exporter-otlp-proto-common==1.38.0, but you have opentelemetry-exporter-otlp-proto-common 1.39.1 which is incompatible.\n",
      "opentelemetry-exporter-otlp-proto-http 1.38.0 requires opentelemetry-proto==1.38.0, but you have opentelemetry-proto 1.39.1 which is incompatible.\n",
      "opentelemetry-exporter-otlp-proto-http 1.38.0 requires opentelemetry-sdk~=1.38.0, but you have opentelemetry-sdk 1.39.1 which is incompatible.\n",
      "opentelemetry-exporter-gcp-logging 1.11.0a0 requires opentelemetry-sdk<1.39.0,>=1.35.0, but you have opentelemetry-sdk 1.39.1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# Google Colab に必要なライブラリをインストールする。\n",
    "# 1行にまとめることで pip が全パッケージの依存関係を一括解決する。\n",
    "# NOTE: Colab では uv ではなく pip を使う。uv は依存解決の過程で\n",
    "#       numpy 等をアップグレードし、プリインストール済みの scipy 等を壊すため。\n",
    "# NOTE: langchain 関連は 1.x 系に明示的に指定する。\n",
    "#       Colab プリインストールの 0.3.x が残ると langchain-mcp-adapters が動作しない。\n",
    "# Pythonのリストとして定義することで、Pylanceの警告を防ぎ、可読性を高める。\n",
    "\n",
    "# fmt: off\n",
    "pkgs = [\n",
    "    \"ollama\", \"langchain-ollama\",\n",
    "    \"langchain>=1.2.8\", \"langchain-core>=1.2.8\", \"langgraph>=1.0.7\",\n",
    "    \"markitdown[all]\", \"chromadb\", \"langchain-text-splitters>=0.3\",\n",
    "    \"spacy\", \"ginza\", \"ja-ginza\", \"rank-bm25\",\n",
    "    \"sentence-transformers\",\n",
    "]\n",
    "# fmt: on\n",
    "\n",
    "# リストを結合して pip に渡す\n",
    "# magic command内で {変数} を使うと展開される機能を利用\n",
    "%pip install -U -q {\" \".join(pkgs)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Google Colab に Ollama をセットアップ**\n",
    "- 詳細は [01_connect_oss_llm.ipynb](01_connect_oss_llm.ipynb) を参照。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selecting previously unselected package zstd.\n",
      "(Reading database ... 121852 files and directories currently installed.)\n",
      "Preparing to unpack .../zstd_1.4.8+dfsg-3build1_amd64.deb ...\n",
      "Unpacking zstd (1.4.8+dfsg-3build1) ...\n",
      "Setting up zstd (1.4.8+dfsg-3build1) ...\n",
      "Processing triggers for man-db (2.10.2-1) ...\n",
      ">>> Installing ollama to /usr/local\n",
      ">>> Downloading ollama-linux-amd64.tar.zst\n",
      "######################################################################## 100.0%\n",
      ">>> Creating ollama user...\n",
      ">>> Adding ollama user to video group...\n",
      ">>> Adding current user to ollama group...\n",
      ">>> Creating ollama systemd service...\n",
      "\u001b[1m\u001b[31mWARNING:\u001b[m systemd is not running\n",
      "\u001b[1m\u001b[31mWARNING:\u001b[m Unable to detect NVIDIA/AMD GPU. Install lspci or lshw to automatically detect and install GPU dependencies.\n",
      ">>> The Ollama API is now available at 127.0.0.1:11434.\n",
      ">>> Install complete. Run \"ollama\" from the command line.\n",
      "success                                                     \n",
      "gpt-oss:20b: Done!\n",
      "  Model\n",
      "    architecture        gptoss    \n",
      "    parameters          20.9B     \n",
      "    context length      131072    \n",
      "    embedding length    2880      \n",
      "    quantization        MXFP4     \n",
      "\n",
      "  Capabilities\n",
      "    completion    \n",
      "    tools         \n",
      "    thinking      \n",
      "\n",
      "  Parameters\n",
      "    temperature    1    \n",
      "\n",
      "  License\n",
      "    Apache License               \n",
      "    Version 2.0, January 2004    \n",
      "    ...                          \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Ollama のインストール・起動・モデルのダウンロード\n",
    "# 詳細は 01_connect_oss_llm.ipynb を参照\n",
    "import subprocess\n",
    "import time\n",
    "import ollama  # type: ignore\n",
    "\n",
    "!apt-get install -y -qq zstd\n",
    "!curl -fsSL https://ollama.com/install.sh | sh\n",
    "\n",
    "process = subprocess.Popen(\n",
    "    [\"ollama\", \"serve\"],\n",
    "    stdout=subprocess.DEVNULL,\n",
    "    stderr=subprocess.DEVNULL,\n",
    ")\n",
    "time.sleep(5)\n",
    "\n",
    "\n",
    "def ollama_pull(model: str) -> None:\n",
    "    \"\"\"Ollama モデルをダウンロードし、進捗をインライン表示する。\"\"\"\n",
    "    for progress in ollama.pull(model, stream=True):\n",
    "        status = progress.get(\"status\", \"\")\n",
    "        total = progress.get(\"total\") or 0\n",
    "        completed = progress.get(\"completed\") or 0\n",
    "        if total:\n",
    "            line = f\"{status}: {completed / total:.0%}\"\n",
    "        else:\n",
    "            line = status\n",
    "        print(f\"\\r{line:<60}\", end=\"\", flush=True)\n",
    "    print(f\"\\n{model}: Done!\")\n",
    "\n",
    "\n",
    "model_name = \"gpt-oss:20b\"\n",
    "ollama_pull(model_name)\n",
    "!ollama show {model_name}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ChatOllama で LLM に接続**\n",
    "- 詳細は [01_connect_oss_llm.ipynb](01_connect_oss_llm.ipynb) を参照。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ChatOllama で LLM に接続する。\n",
    "from langchain_ollama import ChatOllama  # type: ignore\n",
    "\n",
    "llm = ChatOllama(\n",
    "    model=\"gpt-oss:20b\",\n",
    "    num_ctx=16384,\n",
    "    num_predict=-1,\n",
    "    temperature=0.8,\n",
    "    top_k=40,\n",
    "    top_p=0.9,\n",
    "    repeat_penalty=1.1,\n",
    "    reasoning=\"medium\",  # gpt-oss は 'low'/'medium'/'high' のみ受け付ける\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Embedding モデル（ruri-v3-310m）と Reranker モデルのセットアップ**\n",
    "- 詳細は [04_AI_Embedding_RAG.ipynb](04_AI_Embedding_RAG.ipynb) を参照。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:104: UserWarning: \n",
      "Error while fetching `HF_TOKEN` secret value from your vault: 'Requesting secret HF_TOKEN timed out. Secrets can only be fetched when running from the Colab UI.'.\n",
      "You are not authenticated with the Hugging Face Hub in this notebook.\n",
      "If the error persists, please let us know by opening an issue on GitHub (https://github.com/huggingface/huggingface_hub/issues/new).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b9691018f2345b38bc911bc4b7f0bb7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/229 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9b3c65ebb50453db434db0a12aa7e56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/205 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "261a07c43cbd47e39d540d6056952fa1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b66384c87c540cfa95b1a82b8a51ce8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/54.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n",
      "WARNING:huggingface_hub.utils._http:Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "436f38976bb8456a8cdd871f40d5cd05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1475d03a2324705a1f63047c0cd0cec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.26G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76b17143c7d84856969b8b2d43316a4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/152 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f33005f297d546b58b24966b5b98fa25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33568697025d42f7b4614a0ad2c62fc8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac04d66f6fd7435a824c9eb63dffac5b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/1.83M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "845fe2272823436a8da11b424dd589da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/968 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed34b6074abd45eeac12b4e6717c7b83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/296 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding dim: 768\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f5a09191b2c4b45b0a77a5913a5f5d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21b99505304e4c5f871d3b6bcd80871c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.26G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76ca3aa5287e4fbe80cc69b05261bc7e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/156 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66cad922920a4850a14acfd77df9dfb3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "edaee711eb9f400cbd0524fb4d3b3256",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "770a85cfaef440da860522ec5adef35f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/1.83M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2306922c9a35475f9642a4ab801795b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/968 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e5be6339eab4390ba168e7d6e461961",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reranker model loaded.\n"
     ]
    }
   ],
   "source": [
    "# Embedding: ruri-v3-310m (Sentence Transformers 経由)\n",
    "from langchain_core.embeddings import Embeddings  # type: ignore\n",
    "from sentence_transformers import SentenceTransformer, CrossEncoder  # type: ignore\n",
    "\n",
    "\n",
    "class RuriEmbeddings(Embeddings):\n",
    "    \"\"\"ruri-v3 を LangChain の Embeddings インターフェースでラップする。\"\"\"\n",
    "\n",
    "    def __init__(self, model_name: str = \"cl-nagoya/ruri-v3-310m\") -> None:\n",
    "        self.model = SentenceTransformer(model_name)\n",
    "\n",
    "    def embed_documents(self, texts: list[str]) -> list[list[float]]:\n",
    "        prefixed = [f\"検索文書: {t}\" for t in texts]\n",
    "        return self.model.encode(prefixed).tolist()\n",
    "\n",
    "    def embed_query(self, text: str) -> list[float]:\n",
    "        return self.model.encode(f\"検索クエリ: {text}\").tolist()\n",
    "\n",
    "\n",
    "embeddings = RuriEmbeddings()\n",
    "test_vec = embeddings.embed_query(\"テスト文です\")\n",
    "print(f\"Embedding dim: {len(test_vec)}\")\n",
    "\n",
    "# Reranker: cl-nagoya/ruri-v3-reranker-310m\n",
    "reranker = CrossEncoder(\"cl-nagoya/ruri-v3-reranker-310m\")\n",
    "print(\"Reranker model loaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**JAXA リポジトリからデータをダウンロード → 前処理 → チャンク分割**\n",
    "> [井澤克彦, 市川信一郎, 高速回転ホイール: 高速回転ホイール開発を通しての知見, 宇宙航空研究開発機構研究開発報告, 2008](https://jaxa.repo.nii.ac.jp/records/2149)\n",
    "- 詳細は [04_AI_Embedding_RAG.ipynb](04_AI_Embedding_RAG.ipynb) を参照。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ダウンロード完了: 高速回転ホイール.pdf\n",
      "変換後の文字数: 86888\n",
      "クリーニング後の文字数: 79515\n"
     ]
    }
   ],
   "source": [
    "# JAXA リポジトリから PDF をダウンロードし、MarkItDown で markdown に変換する。\n",
    "import urllib.request\n",
    "import unicodedata\n",
    "import re\n",
    "from pathlib import Path\n",
    "from markitdown import MarkItDown  # type: ignore\n",
    "\n",
    "pdf_url = \"https://jaxa.repo.nii.ac.jp/record/2149/files/63826000.pdf\"\n",
    "pdf_path = Path(\"高速回転ホイール.pdf\")\n",
    "\n",
    "if not pdf_path.exists():\n",
    "    urllib.request.urlretrieve(pdf_url, pdf_path)\n",
    "    print(f\"ダウンロード完了: {pdf_path}\")\n",
    "\n",
    "md = MarkItDown()\n",
    "result = md.convert(str(pdf_path))\n",
    "raw_text = result.text_content\n",
    "print(f\"変換後の文字数: {len(raw_text)}\")\n",
    "\n",
    "# Unicode 正規化 (NFKC)\n",
    "text = unicodedata.normalize(\"NFKC\", raw_text)\n",
    "\n",
    "\n",
    "# PDF 抽出テキストの汎用クリーニング\n",
    "def clean_pdf_text(text: str) -> str:\n",
    "    \"\"\"1文字行ブロックの除去 + 空行圧縮。\"\"\"\n",
    "    text = re.sub(\n",
    "        r\"(^[^\\S\\n]*\\S[^\\S\\n]*$\\n?){3,}\",\n",
    "        \"\\n\",\n",
    "        text,\n",
    "        flags=re.MULTILINE,\n",
    "    )\n",
    "    text = re.sub(r\"\\n{3,}\", \"\\n\\n\", text)\n",
    "    return text.strip()\n",
    "\n",
    "\n",
    "text = clean_pdf_text(text)\n",
    "print(f\"クリーニング後の文字数: {len(text)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**notebook 04 からの変更点**\n",
    "- `CHUNK_SIZE` ：1500 から 500 に変更\n",
    "- `CHUNK_OVERLAP` ：300 から 100 に変更\n",
    "> チャンクサイズが大きく、かつ、この Workflow だと複数回検索することにより、LLM のコンテキスト長不足になることが想定されるため。チャンクサイズを小さく取り、複数回の検索によって、重要な情報のみを、ピンポイントでデータから抽出することを意図している。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "チャンク数: 214\n"
     ]
    }
   ],
   "source": [
    "# SpacyTextSplitter でチャンク分割する。\n",
    "# Sudachi の入力上限 (49,149 bytes) を超えないように事前分割してから渡す。\n",
    "from langchain_text_splitters import SpacyTextSplitter  # type: ignore\n",
    "\n",
    "CHUNK_SIZE = 500\n",
    "CHUNK_OVERLAP = 100\n",
    "BLOCK_MAX_BYTES = 40_000\n",
    "BLOCK_OVERLAP_CHARS = CHUNK_SIZE\n",
    "\n",
    "\n",
    "def split_into_safe_blocks(\n",
    "    text: str,\n",
    "    max_bytes: int = BLOCK_MAX_BYTES,\n",
    "    overlap_chars: int = BLOCK_OVERLAP_CHARS,\n",
    ") -> list[str]:\n",
    "    \"\"\"テキストを段落区切りで max_bytes 以下のブロックに分割する。\"\"\"\n",
    "    paragraphs = text.split(\"\\n\\n\")\n",
    "    blocks: list[str] = []\n",
    "    current = \"\"\n",
    "    for para in paragraphs:\n",
    "        candidate = current + \"\\n\\n\" + para if current else para\n",
    "        if len(candidate.encode(\"utf-8\")) > max_bytes and current:\n",
    "            blocks.append(current)\n",
    "            current = current[-overlap_chars:] + \"\\n\\n\" + para\n",
    "        else:\n",
    "            current = candidate\n",
    "    if current:\n",
    "        blocks.append(current)\n",
    "    return blocks\n",
    "\n",
    "\n",
    "text_splitter = SpacyTextSplitter(\n",
    "    separator=\"\\n\\n\",\n",
    "    pipeline=\"ja_ginza\",\n",
    "    chunk_size=CHUNK_SIZE,\n",
    "    chunk_overlap=CHUNK_OVERLAP,\n",
    ")\n",
    "\n",
    "blocks = split_into_safe_blocks(text)\n",
    "chunks: list[str] = []\n",
    "for block in blocks:\n",
    "    chunks.extend(text_splitter.split_text(block))\n",
    "\n",
    "print(f\"チャンク数: {len(chunks)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ベクトルデータベースの構築（ChromaDB, インメモリ）**\n",
    "- 詳細は [04_AI_Embedding_RAG.ipynb](04_AI_Embedding_RAG.ipynb) を参照。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChromaDB に 214 件のチャンクを格納しました。\n"
     ]
    }
   ],
   "source": [
    "# ChromaDB にチャンクを格納する（インメモリ）。\n",
    "import chromadb  # type: ignore\n",
    "\n",
    "chroma_client = chromadb.Client()\n",
    "\n",
    "collection = chroma_client.create_collection(\n",
    "    name=\"jaxa_wheel\",\n",
    "    metadata={\"hnsw:space\": \"cosine\"},\n",
    ")\n",
    "\n",
    "chunk_embeddings = embeddings.embed_documents(chunks)\n",
    "\n",
    "collection.add(\n",
    "    ids=[f\"chunk_{i}\" for i in range(len(chunks))],\n",
    "    documents=chunks,\n",
    "    embeddings=chunk_embeddings,\n",
    ")\n",
    "\n",
    "print(f\"ChromaDB に {collection.count()} 件のチャンクを格納しました。\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**検索機能の実装（BM25 + セマンティック + ハイブリッド + Reranker）と tool 定義**\n",
    "- 詳細は [04_AI_Embedding_RAG.ipynb](04_AI_Embedding_RAG.ipynb) を参照。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BM25 インデックス構築完了: 214 件\n"
     ]
    }
   ],
   "source": [
    "# BM25 用の前処理: spaCy (ja_ginza) で形態素解析してトークン化する。\n",
    "import numpy as np  # type: ignore\n",
    "import spacy  # type: ignore\n",
    "from rank_bm25 import BM25Okapi  # type: ignore\n",
    "\n",
    "nlp = spacy.load(\"ja_ginza\", disable=[\"parser\", \"ner\"])\n",
    "\n",
    "\n",
    "def tokenize(text: str) -> list[str]:\n",
    "    \"\"\"spaCy で形態素解析し、BM25 用のトークンリストを返す。\"\"\"\n",
    "    doc = nlp(text)\n",
    "    tokens = []\n",
    "    include_pos = {\"NOUN\", \"VERB\", \"ADJ\", \"PROPN\", \"NUM\"}\n",
    "    for token in doc:\n",
    "        if token.pos_ not in include_pos:\n",
    "            continue\n",
    "        if token.is_stop:\n",
    "            continue\n",
    "        lemma = token.lemma_\n",
    "        if len(lemma) == 1 and re.match(r\"[ぁ-ん\\u30fc!-/:-@\\[-`{-~]\", lemma):\n",
    "            continue\n",
    "        tokens.append(lemma)\n",
    "    return tokens\n",
    "\n",
    "\n",
    "tokenized_chunks = [tokenize(chunk) for chunk in chunks]\n",
    "bm25 = BM25Okapi(tokenized_chunks)\n",
    "print(f\"BM25 インデックス構築完了: {len(tokenized_chunks)} 件\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "検索関数を定義しました: search_bm25, search_semantic, search_hybrid, rerank\n"
     ]
    }
   ],
   "source": [
    "# 検索関数の定義（BM25, セマンティック, ハイブリッド, Reranker）\n",
    "RETRIEVAL_TOP_K = 20\n",
    "RERANK_TOP_K = 5\n",
    "BM25_WEIGHT = 0.3\n",
    "\n",
    "\n",
    "def search_bm25(query: str, top_k: int = RETRIEVAL_TOP_K) -> list[dict]:\n",
    "    \"\"\"BM25 によるキーワード検索を行う。\"\"\"\n",
    "    tokenized_query = tokenize(query)\n",
    "    scores = bm25.get_scores(tokenized_query)\n",
    "    top_indices = np.argsort(scores)[::-1][:top_k]\n",
    "    return [\n",
    "        {\n",
    "            \"rank\": rank + 1,\n",
    "            \"chunk_id\": int(idx),\n",
    "            \"score\": float(scores[idx]),\n",
    "            \"text\": chunks[idx],\n",
    "        }\n",
    "        for rank, idx in enumerate(top_indices)\n",
    "        if scores[idx] > 0\n",
    "    ]\n",
    "\n",
    "\n",
    "def search_semantic(query: str, top_k: int = RETRIEVAL_TOP_K) -> list[dict]:\n",
    "    \"\"\"Embedding model によるセマンティック検索を行う。\"\"\"\n",
    "    query_embedding = embeddings.embed_query(query)\n",
    "    results = collection.query(query_embeddings=[query_embedding], n_results=top_k)\n",
    "    return [\n",
    "        {\n",
    "            \"rank\": rank + 1,\n",
    "            \"chunk_id\": int(doc_id.split(\"_\")[1]),\n",
    "            \"score\": 1.0 - dist,\n",
    "            \"text\": doc,\n",
    "        }\n",
    "        for rank, (doc_id, doc, dist) in enumerate(\n",
    "            zip(results[\"ids\"][0], results[\"documents\"][0], results[\"distances\"][0])\n",
    "        )\n",
    "    ]\n",
    "\n",
    "\n",
    "def search_hybrid(\n",
    "    query: str, top_k: int = RETRIEVAL_TOP_K, bm25_weight: float = BM25_WEIGHT\n",
    ") -> list[dict]:\n",
    "    \"\"\"BM25 とセマンティック検索の RRF ハイブリッド検索を行う。\"\"\"\n",
    "    k = 60\n",
    "    bm25_results = search_bm25(query, top_k=top_k)\n",
    "    semantic_results = search_semantic(query, top_k=top_k)\n",
    "\n",
    "    scores: dict[int, float] = {}\n",
    "    texts: dict[int, str] = {}\n",
    "\n",
    "    for r in bm25_results:\n",
    "        cid = r[\"chunk_id\"]\n",
    "        scores[cid] = scores.get(cid, 0) + bm25_weight / (k + r[\"rank\"])\n",
    "        texts[cid] = r[\"text\"]\n",
    "\n",
    "    semantic_weight = 1.0 - bm25_weight\n",
    "    for r in semantic_results:\n",
    "        cid = r[\"chunk_id\"]\n",
    "        scores[cid] = scores.get(cid, 0) + semantic_weight / (k + r[\"rank\"])\n",
    "        texts[cid] = r[\"text\"]\n",
    "\n",
    "    sorted_ids = sorted(scores, key=lambda cid: scores[cid], reverse=True)[:top_k]\n",
    "    return [\n",
    "        {\"rank\": rank + 1, \"chunk_id\": cid, \"score\": scores[cid], \"text\": texts[cid]}\n",
    "        for rank, cid in enumerate(sorted_ids)\n",
    "    ]\n",
    "\n",
    "\n",
    "def rerank(query: str, results: list[dict], top_k: int = RERANK_TOP_K) -> list[dict]:\n",
    "    \"\"\"Reranker (CrossEncoder) で検索結果を再順位付けする。\"\"\"\n",
    "    if not results:\n",
    "        return []\n",
    "    pairs = [(query, r[\"text\"]) for r in results]\n",
    "    scores = reranker.predict(pairs)\n",
    "    ranked_indices = np.argsort(scores)[::-1][:top_k]\n",
    "    return [\n",
    "        {\n",
    "            \"rank\": rank + 1,\n",
    "            \"chunk_id\": results[idx][\"chunk_id\"],\n",
    "            \"score\": float(scores[idx]),\n",
    "            \"text\": results[idx][\"text\"],\n",
    "        }\n",
    "        for rank, idx in enumerate(ranked_indices)\n",
    "    ]\n",
    "\n",
    "\n",
    "print(\"検索関数を定義しました: search_bm25, search_semantic, search_hybrid, rerank\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAG Tool: search_document\n"
     ]
    }
   ],
   "source": [
    "# 検索機能を LLM の tool として定義する。\n",
    "from langchain_core.tools import tool  # type: ignore\n",
    "\n",
    "MAX_RETURN_CHARS = 8000\n",
    "\n",
    "\n",
    "@tool\n",
    "def search_document(query: str) -> str:\n",
    "    \"\"\"外部ナレッジベースから、クエリに関連する情報を検索・取得します。\n",
    "    ユーザーの質問に対し、具体的な事実、データ、あるいは詳細な文脈が必要な場合、\n",
    "    自身の知識だけで回答せずに必ずこのツールを使用してください。\n",
    "\n",
    "    Args:\n",
    "        query: 検索したい内容を表す、具体的かつ完全な文章（日本語）。\n",
    "    \"\"\"\n",
    "    try:\n",
    "        hybrid_results = search_hybrid(query)\n",
    "        reranked = rerank(query, hybrid_results)\n",
    "    except Exception as e:\n",
    "        return f\"検索中にエラーが発生しました: {e}\"\n",
    "\n",
    "    if not reranked:\n",
    "        return \"検索結果が見つかりませんでした。\"\n",
    "\n",
    "    passages = []\n",
    "    total_chars = 0\n",
    "    for r in reranked:\n",
    "        passage = f\"[チャンク {r['chunk_id']}] (スコア: {r['score']:.4f})\\n{r['text']}\"\n",
    "        total_chars += len(passage)\n",
    "        if total_chars > MAX_RETURN_CHARS:\n",
    "            break\n",
    "        passages.append(passage)\n",
    "    return \"\\n\\n---\\n\\n\".join(passages)\n",
    "\n",
    "\n",
    "search_tool = search_document\n",
    "print(f\"RAG Tool: {search_tool.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**LangGraph による Workflow の実装**\n",
    "\n",
    "03_02 の Workflow を RAG 検索用に適応する。web_search ノードを doc_search ノードに置き換え、\n",
    "MCP サーバの代わりに `search_document` ツールで検索する。\n",
    "- 詳細は [03_02_AI_Agent_Workflow.ipynb](03_02_AI_Agent_Workflow.ipynb) を参照。\n",
    "\n",
    "**Workflow の流れ**\n",
    "1. **task_planning**: ユーザの質問を受け取り、回答に必要なサブタスク（目的＋検索クエリ）を構造化して作成する。\n",
    "2. **doc_search**: 各サブタスクの検索クエリを `search_document` ツールで実行し、目的と紐付けた検索結果を蓄積する。\n",
    "3. **judge**: サブタスクの目的ごとに、検索結果が十分かを LLM が判断する。不足なら追加サブタスクを生成して doc_search に戻る。\n",
    "4. **generate_answer**: 目的ごとに整理された検索結果をもとに、ユーザの質問に対する最終回答を生成する。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pydantic / with_structured output による構造化出力**\n",
    "\n",
    "検索結果に LaTeX 数式が含まれるなどの場合、LLM の出力が引きずられて、notebook 03_02 の JSONパース（自作した `extract_json_text` ）関数が機能しない場合があった。このため、Pydantic / with_structured output による構造化出力によって、望ましい出力を LLM に強制させる。\n",
    "\n",
    "<u>使い方<u/>\n",
    "- `queries: list[str] = Field(description=\"検索クエリのリスト\")` では、LLM が出力するデータ型を、`list[str]` で規定し、LLM に出力させたい `str` の内容を、`Field(description=\" \")` で規定する。\n",
    "- 入れ子にもできる。 `subtasks: list[Subtask] = Field(description=\"サブタスクのリスト（最大3個）\")` では、`list` の型を、`class Subtask(BaseModel)` として規定している。\n",
    "- 以下では、`list[Subtask]` か、`None` のどちらかの型であれば良いという指示になる。コードとしては、`sufficient = True` で `None` 、` = False` で、`list[Subtask]` となることを期待しており、LLM がそのような選択をすることを期待した `description` を設定している。\n",
    "`\n",
    "sufficient: bool = Field(description=\"情報が十分かどうか\")\n",
    "    reason: str = Field(description=\"判断理由を日本語で1文で\")\n",
    "    additional_subtasks: list[Subtask] | None = Field(\n",
    "        default=None,\n",
    "        description=\"不足時の追加サブタスク\",\n",
    "    )\n",
    "`\n",
    "> しかし、上記の場合、動作の安定性が LLM 任せとなってしまうので、`@model_validator(mode=\"after\")` で、LLM の出力を強制的に固定するようにしている。（`sufficient = True` なら必ず `None`、` = False` なのに、`additional_subtasks` が空の場合は、エラー出力をした上、`sufficient = True` + `None` に設定する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Workflow の状態定義・Pydantic モデル・システムプロンプトを定義しました。\n"
     ]
    }
   ],
   "source": [
    "# Workflow の状態定義・Pydantic モデル・システムプロンプト\n",
    "from typing import TypedDict\n",
    "from pydantic import BaseModel, Field, model_validator  # type: ignore\n",
    "from langchain_core.messages import HumanMessage, SystemMessage  # type: ignore\n",
    "from langgraph.graph import StateGraph, START, END  # type: ignore\n",
    "from IPython.display import Image, display\n",
    "\n",
    "# --- グローバル設定 ---\n",
    "MAX_LOOP_COUNT = 2  # judge → doc_search 再調査ループの上限回数\n",
    "\n",
    "\n",
    "# --- Workflow の状態 ---\n",
    "class WorkflowState(TypedDict):\n",
    "    question: str\n",
    "    subtasks: list[dict]  # [{\"purpose\": str, \"queries\": [str]}]\n",
    "    search_results: list[str]  # 目的と紐付けた検索結果\n",
    "    answer: str\n",
    "    loop_count: int\n",
    "\n",
    "\n",
    "# --- with_structured_output 用の Pydantic モデル ---\n",
    "class Subtask(BaseModel):\n",
    "    purpose: str = Field(description=\"このサブタスクで明らかにしたいこと\")\n",
    "    queries: list[str] = Field(description=\"検索クエリのリスト\")\n",
    "\n",
    "\n",
    "class TaskPlanningResult(BaseModel):\n",
    "    subtasks: list[Subtask] = Field(description=\"サブタスクのリスト（最大3個）\")\n",
    "\n",
    "\n",
    "class JudgeResult(BaseModel):\n",
    "    sufficient: bool = Field(description=\"情報が十分かどうか\")\n",
    "    reason: str = Field(description=\"判断理由を日本語で1文で\")\n",
    "    additional_subtasks: list[Subtask] | None = Field(\n",
    "        default=None,\n",
    "        description=\"不足時の追加サブタスク\",\n",
    "    )\n",
    "\n",
    "    @model_validator(mode=\"after\")\n",
    "    def force_consistency(self):\n",
    "        \"\"\"LLM 出力の論理矛盾を自動補正する。\"\"\"\n",
    "        # 十分なのにタスクがある → タスクを破棄\n",
    "        if self.sufficient:\n",
    "            self.additional_subtasks = None\n",
    "\n",
    "        # 不足なのにタスクがない → 十分と見なして回答生成へ強制移行\n",
    "        if not self.sufficient and not self.additional_subtasks:\n",
    "            print(\n",
    "                \"【自動補正】情報不足と判定されましたが、\"\n",
    "                \"追加タスクが空のため、回答生成へ強制移行します。\"\n",
    "            )\n",
    "            self.sufficient = True\n",
    "            self.reason += (\n",
    "                \" (※追加調査事項が具体化できなかったため、現状の情報で回答します)\"\n",
    "            )\n",
    "            self.additional_subtasks = None\n",
    "\n",
    "        return self\n",
    "\n",
    "\n",
    "# --- 各ノードのシステムプロンプト ---\n",
    "SYSTEM_PROMPT_TASK_PLANNING = \"\"\"\\\n",
    "あなたはリサーチプランナーです。\n",
    "ユーザの質問に回答するために、ナレッジベース（技術文書）を検索するためのサブタスクを作成してください。\n",
    "\n",
    "サブタスクは最大3個までとしてください。\n",
    "purpose は判定ステップで「この目的に十分な情報が得られたか」を評価する基準になります。\n",
    "具体的かつ明確に書いてください。\n",
    "検索クエリは、技術文書から関連情報を検索するための日本語の具体的なフレーズにしてください。\n",
    "\"\"\"\n",
    "\n",
    "SYSTEM_PROMPT_JUDGE = \"\"\"\\\n",
    "あなたはリサーチの品質を判定する審査員です。\n",
    "ユーザの質問と検索結果を見て、回答に十分な情報があるか判断してください。\n",
    "検索結果には【目的: ...】タグが付いています。\n",
    "各目的について十分な情報が得られているかを確認してください。\n",
    "\n",
    "sufficient が true なら回答作成に進みます。\n",
    "sufficient が false なら、不足している目的について additional_subtasks を生成してください。\n",
    "\"\"\"\n",
    "\n",
    "SYSTEM_PROMPT_GENERATE_ANSWER = \"\"\"\\\n",
    "あなたはリサーチ結果をもとに回答するAIアシスタントです。\n",
    "検索結果を参考に、ユーザの質問に日本語で丁寧に回答してください。\n",
    "回答は必ず検索結果に基づいて作成し、検索結果に含まれない情報は含めないでください。\n",
    "回答の最後に、以下の形式で結論をまとめてください。\n",
    "\n",
    "# 結論\n",
    "- ユーザの質問: （質問内容）\n",
    "- 回答: （簡潔な回答）\n",
    "\"\"\"\n",
    "\n",
    "print(\"Workflow の状態定義・Pydantic モデル・システムプロンプトを定義しました。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Workflow ノードを定義しました: task_planning, doc_search, judge, generate_answer\n"
     ]
    }
   ],
   "source": [
    "# Workflow ノードの定義\n",
    "\n",
    "# ノード 1: task_planning（タスク分割）— with_structured_output で JSON 出力を強制\n",
    "async def task_planning(state: WorkflowState) -> dict:\n",
    "    \"\"\"ユーザの質問を分析し、サブタスク（目的＋検索クエリ）を作成する。\"\"\"\n",
    "    question = state[\"question\"]\n",
    "    structured_llm = llm.with_structured_output(TaskPlanningResult)\n",
    "\n",
    "    try:\n",
    "        result = await structured_llm.ainvoke(\n",
    "            [\n",
    "                SystemMessage(content=SYSTEM_PROMPT_TASK_PLANNING),\n",
    "                HumanMessage(content=question),\n",
    "            ]\n",
    "        )\n",
    "        subtasks = [st.model_dump() for st in result.subtasks]\n",
    "    except Exception as e:\n",
    "        print(f\"[task_planning] 構造化出力失敗 → フォールバック: {e}\")\n",
    "        subtasks = [{\"purpose\": \"基本調査\", \"queries\": [question]}]\n",
    "\n",
    "    print(f\"[task_planning] サブタスク数: {len(subtasks)}\")\n",
    "    for i, st in enumerate(subtasks):\n",
    "        print(f\"  {i + 1}. 目的: {st['purpose']}\")\n",
    "        print(f\"     クエリ: {st['queries']}\")\n",
    "    return {\"subtasks\": subtasks, \"search_results\": [], \"loop_count\": 0}\n",
    "\n",
    "\n",
    "# ノード 2: doc_search（ドキュメント検索）\n",
    "async def doc_search(state: WorkflowState) -> dict:\n",
    "    \"\"\"各サブタスクの検索クエリを search_document ツールで実行し、結果を蓄積する。\"\"\"\n",
    "    subtasks = state[\"subtasks\"]\n",
    "    results = list(state.get(\"search_results\") or [])\n",
    "\n",
    "    for st in subtasks:\n",
    "        purpose = st[\"purpose\"]\n",
    "        print(f\"[doc_search] 目的: {purpose}\")\n",
    "        for query in st[\"queries\"]:\n",
    "            print(f\"  検索中: {query}\")\n",
    "            try:\n",
    "                result = search_tool.invoke({\"query\": query})\n",
    "            except Exception as e:\n",
    "                print(f\"  [ERROR] クエリ失敗: {query} → {e}\")\n",
    "                continue\n",
    "            if not result or result == \"検索結果が見つかりませんでした。\":\n",
    "                print(f\"  [SKIP] 検索結果なし: {query}\")\n",
    "                continue\n",
    "            results.append(f\"【目的: {purpose}】\\n【クエリ: {query}】\\n{result}\")\n",
    "\n",
    "    return {\"search_results\": results, \"subtasks\": []}\n",
    "\n",
    "\n",
    "# ノード 3: judge（判定）— with_structured_output で JSON 出力を強制\n",
    "async def judge(state: WorkflowState) -> dict:\n",
    "    \"\"\"検索結果が十分かを判断し、不足なら追加サブタスクを生成する。\"\"\"\n",
    "    question = state[\"question\"]\n",
    "    results = state[\"search_results\"]\n",
    "    loop_count = state.get(\"loop_count\", 0)\n",
    "\n",
    "    if loop_count >= MAX_LOOP_COUNT:\n",
    "        print(\"[judge] ループ上限に到達 → 回答作成へ\")\n",
    "        return {\"subtasks\": [], \"loop_count\": loop_count}\n",
    "\n",
    "    results_text = \"\\n\\n\".join(results)\n",
    "    structured_llm = llm.with_structured_output(JudgeResult)\n",
    "\n",
    "    try:\n",
    "        judgment = await structured_llm.ainvoke(\n",
    "            [\n",
    "                SystemMessage(content=SYSTEM_PROMPT_JUDGE),\n",
    "                HumanMessage(content=f\"質問: {question}\\n\\n検索結果:\\n{results_text}\"),\n",
    "            ]\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"[judge] 構造化出力失敗 → 回答作成へ: {e}\")\n",
    "        return {\"subtasks\": [], \"loop_count\": loop_count + 1}\n",
    "\n",
    "    reason = judgment.reason\n",
    "\n",
    "    if judgment.sufficient:\n",
    "        print(f\"[judge] 情報十分 → 回答作成へ（理由: {reason}）\")\n",
    "        return {\"subtasks\": [], \"loop_count\": loop_count + 1}\n",
    "    else:\n",
    "        additional = [st.model_dump() for st in (judgment.additional_subtasks or [])]\n",
    "        print(f\"[judge] 情報不足（理由: {reason}）→ 追加サブタスク:\")\n",
    "        for i, st in enumerate(additional):\n",
    "            print(f\"  {i + 1}. 目的: {st.get('purpose', '?')}\")\n",
    "            print(f\"     クエリ: {st.get('queries', [])}\")\n",
    "        return {\"subtasks\": additional, \"loop_count\": loop_count + 1}\n",
    "\n",
    "\n",
    "# ルーター: judge の結果で分岐\n",
    "def should_continue_search(state: WorkflowState) -> str:\n",
    "    \"\"\"追加サブタスクがあれば doc_search に戻り、なければ回答生成へ。\"\"\"\n",
    "    if state.get(\"subtasks\"):\n",
    "        return \"doc_search\"\n",
    "    return \"generate_answer\"\n",
    "\n",
    "\n",
    "# ノード 4: generate_answer（回答生成）\n",
    "async def generate_answer(state: WorkflowState) -> dict:\n",
    "    \"\"\"蓄積した検索結果をもとに最終回答を生成する。\"\"\"\n",
    "    question = state[\"question\"]\n",
    "    results_text = \"\\n\\n\".join(state[\"search_results\"])\n",
    "\n",
    "    response = await llm.ainvoke(\n",
    "        [\n",
    "            SystemMessage(content=SYSTEM_PROMPT_GENERATE_ANSWER),\n",
    "            HumanMessage(content=f\"質問: {question}\\n\\n検索結果:\\n{results_text}\"),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    answer = response.content or \"\"\n",
    "\n",
    "    if not answer:\n",
    "        print(\"[generate_answer] WARNING: response.content が空です\")\n",
    "        print(f\"  response type: {type(response)}\")\n",
    "        print(f\"  response repr: {repr(response)[:500]}\")\n",
    "\n",
    "    print(\"[generate_answer] 回答生成完了\")\n",
    "    return {\"answer\": answer}\n",
    "\n",
    "\n",
    "print(\n",
    "    \"Workflow ノードを定義しました: task_planning, doc_search, judge, generate_answer\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Workflow グラフの構築とコンパイル\n",
    "workflow = StateGraph(WorkflowState)\n",
    "\n",
    "# ノードの登録\n",
    "workflow.add_node(\"task_planning\", task_planning)\n",
    "workflow.add_node(\"doc_search\", doc_search)\n",
    "workflow.add_node(\"judge\", judge)\n",
    "workflow.add_node(\"generate_answer\", generate_answer)\n",
    "\n",
    "# エッジの定義\n",
    "workflow.add_edge(START, \"task_planning\")\n",
    "workflow.add_edge(\"task_planning\", \"doc_search\")\n",
    "workflow.add_edge(\"doc_search\", \"judge\")\n",
    "\n",
    "# 条件分岐: judge → doc_search（再調査） or generate_answer（回答生成）\n",
    "workflow.add_conditional_edges(\n",
    "    \"judge\",\n",
    "    should_continue_search,\n",
    "    {\n",
    "        \"doc_search\": \"doc_search\",\n",
    "        \"generate_answer\": \"generate_answer\",\n",
    "    },\n",
    ")\n",
    "workflow.add_edge(\"generate_answer\", END)\n",
    "\n",
    "# コンパイル\n",
    "app = workflow.compile()\n",
    "\n",
    "# グラフの可視化\n",
    "display(Image(app.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**動作確認**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[task_planning] サブタスク数: 3\n",
      "  1. 目的: 高速回転ホイールの寿命試験に関する実験データや結論が掲載されている技術文書を特定する。\n",
      "     クエリ: ['高速回転ホイール 寿命試験 報告書', '高速回転ホイール 寿命 試験 結果', '高速回転ホイール 破壊解析 報告']\n",
      "  2. 目的: 特定した文書から、寿命試験の主要な結果（試験条件、破壊モード、サイクル数、残存寿命評価など）を抜粋し、整理する。\n",
      "     クエリ: ['高速回転ホイール 試験 条件 破壊モード', '高速回転ホイール サイクル数 寿命', '高速回転ホイール 破壊原因']\n",
      "  3. 目的: 抜粋した情報を統合し、試験結果の総括（成功・失敗要因、設計改善提案、今後の研究課題）をまとめる。\n",
      "     クエリ: ['高速回転ホイール 試験 結果 まとめ', '高速回転ホイール 寿命 改善提案', '高速回転ホイール 研究課題']\n",
      "[doc_search] 目的: 高速回転ホイールの寿命試験に関する実験データや結論が掲載されている技術文書を特定する。\n",
      "  検索中: 高速回転ホイール 寿命試験 報告書\n",
      "  検索中: 高速回転ホイール 寿命 試験 結果\n",
      "  検索中: 高速回転ホイール 破壊解析 報告\n",
      "[doc_search] 目的: 特定した文書から、寿命試験の主要な結果（試験条件、破壊モード、サイクル数、残存寿命評価など）を抜粋し、整理する。\n",
      "  検索中: 高速回転ホイール 試験 条件 破壊モード\n",
      "  検索中: 高速回転ホイール サイクル数 寿命\n",
      "  検索中: 高速回転ホイール 破壊原因\n",
      "[doc_search] 目的: 抜粋した情報を統合し、試験結果の総括（成功・失敗要因、設計改善提案、今後の研究課題）をまとめる。\n",
      "  検索中: 高速回転ホイール 試験 結果 まとめ\n",
      "  検索中: 高速回転ホイール 寿命 改善提案\n",
      "  検索中: 高速回転ホイール 研究課題\n",
      "[judge] 情報十分 → 回答作成へ（理由: The search results contain enough details to answer the question.  They include descriptions of the specific lifetime‑test programs (e.g., maximum‑speed continuous operation for 3 months, maximum‑acceleration test of 560 k cycles, zero‑cross test of 1 M cycles), the observed failure mechanisms (retainer‑instability, heating, friction increase, wear, oil degradation, eventual burn‑in), and quantitative estimates (e.g., a calculated life of ≈58 years at 6 000 rpm).  These data are sufficient to summarize what results were obtained in the high‑speed flywheel lifetime tests.}）\n",
      "[generate_answer] 回答生成完了\n",
      "\n",
      "--- 以下は HTML による全文ログ（トランケート回避） ---\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style='white-space:pre-wrap'>[task_planning] サブタスク数: 3\n",
       "  1. 目的: 高速回転ホイールの寿命試験に関する実験データや結論が掲載されている技術文書を特定する。\n",
       "     クエリ: ['高速回転ホイール 寿命試験 報告書', '高速回転ホイール 寿命 試験 結果', '高速回転ホイール 破壊解析 報告']\n",
       "  2. 目的: 特定した文書から、寿命試験の主要な結果（試験条件、破壊モード、サイクル数、残存寿命評価など）を抜粋し、整理する。\n",
       "     クエリ: ['高速回転ホイール 試験 条件 破壊モード', '高速回転ホイール サイクル数 寿命', '高速回転ホイール 破壊原因']\n",
       "  3. 目的: 抜粋した情報を統合し、試験結果の総括（成功・失敗要因、設計改善提案、今後の研究課題）をまとめる。\n",
       "     クエリ: ['高速回転ホイール 試験 結果 まとめ', '高速回転ホイール 寿命 改善提案', '高速回転ホイール 研究課題']\n",
       "[doc_search] 目的: 高速回転ホイールの寿命試験に関する実験データや結論が掲載されている技術文書を特定する。\n",
       "  検索中: 高速回転ホイール 寿命試験 報告書\n",
       "  検索中: 高速回転ホイール 寿命 試験 結果\n",
       "  検索中: 高速回転ホイール 破壊解析 報告\n",
       "[doc_search] 目的: 特定した文書から、寿命試験の主要な結果（試験条件、破壊モード、サイクル数、残存寿命評価など）を抜粋し、整理する。\n",
       "  検索中: 高速回転ホイール 試験 条件 破壊モード\n",
       "  検索中: 高速回転ホイール サイクル数 寿命\n",
       "  検索中: 高速回転ホイール 破壊原因\n",
       "[doc_search] 目的: 抜粋した情報を統合し、試験結果の総括（成功・失敗要因、設計改善提案、今後の研究課題）をまとめる。\n",
       "  検索中: 高速回転ホイール 試験 結果 まとめ\n",
       "  検索中: 高速回転ホイール 寿命 改善提案\n",
       "  検索中: 高速回転ホイール 研究課題\n",
       "[judge] 情報十分 → 回答作成へ（理由: The search results contain enough details to answer the question.  They include descriptions of the specific lifetime‑test programs (e.g., maximum‑speed continuous operation for 3 months, maximum‑acceleration test of 560 k cycles, zero‑cross test of 1 M cycles), the observed failure mechanisms (retainer‑instability, heating, friction increase, wear, oil degradation, eventual burn‑in), and quantitative estimates (e.g., a calculated life of ≈58 years at 6 000 rpm).  These data are sufficient to summarize what results were obtained in the high‑speed flywheel lifetime tests.}）\n",
       "[generate_answer] 回答生成完了\n",
       "</pre>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Workflow エージェントの実行結果 ===\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "高速回転ホイール（JAXA‑RR‑07‑025）の寿命試験は、設計寿命（10 年）に対し、実際の運用条件下での劣化挙動を検証するために、以下のような試験を実施しました。  \n",
       "\n",
       "| 試験 | 試験条件 | 評価項目 | 主な結果 |\n",
       "|------|----------|----------|----------|\n",
       "| **最大回転数連続運用試験** | 3 か月間、最大回転数（約 6000 rpm）で連続運転 | 残存油量・油量分布、ランニングトルクトレンド、温度、リテーナインスタビリティの有無 | 油量はほぼ一定に保たれ、温度は設計許容範囲内に収まった。リテーナインスタビリティは確認されず、摩耗・熱発生も抑えられた。 |\n",
       "| **最大加減速運用試験** | 56 万回の急加減速 | 付近の摩耗・温度変化、油量分布 | 連続運転に比べて局所的な摩耗が観測されたが、全体としては許容範囲内。 |\n",
       "| **ゼロクロス運用試験** | 100 万回のゼロクロス | クリーニング・摩耗、油量分布、リテーナインスタビリティ | ゼロクロス時に発生するタコパルスの減少に伴い、摩耗がやや増加。リテーナインスタビリティは検出されず、油分布も均一。 |\n",
       "| **200 h PROD 試験（ランニング後油分布確認）** | 200 h連続運転後 | 油分布確認 | 油分布に偏りは見られず、潤滑状態は良好。 |\n",
       "| **寿命計算（EHL・フレーミング）** | 6000 rpm、6000 rpm時のベアリング最低粘度 ν₁ = 12 cSt、使用油 ν = 27 cSt | 転がり疲れ寿命 | 計算結果は **L<sub>nf</sub> ≈ 1.839 × 10¹¹ 回転（約 58 年相当）** で、設計寿命よりも十分に長い寿命を示した。 |\n",
       "\n",
       "### 成功・失敗要因  \n",
       "- **成功**  \n",
       "  - 連続運転・急加減速・ゼロクロスすべての試験で、油量分布や温度が設計許容範囲内に留まった。  \n",
       "  - リテーナインスタビリティ（予圧増大・熱発生）が検出されず、摩耗・熱の進行が抑えられた。  \n",
       "\n",
       "- **失敗（潜在的な失敗要因）**  \n",
       "  - 急加減速・ゼロクロス時の局所摩耗が観測された。  \n",
       "  - 油の白色化（酸化反応）を防止するために、酸化剤の選定と含有量を最適化したが、未だに潤滑剤の移動を完全に制御することは難しい。  \n",
       "  - リテーナインスタビリティは、保持器形状（丸穴→角穴）や寸法の最適化により解決したものの、複数要因（摩擦・摩耗・バランス）から再発のリスクは残る。  \n",
       "\n",
       "### 設計改善提案  \n",
       "1. **保持器（リテーナ）設計の継続的最適化**  \n",
       "   - 角穴形状・不等配置の採用でリテーナインスタビリティを抑制。  \n",
       "2. **油分布管理**  \n",
       "   - 連続運転・ゼロクロス時に油分布の微小偏りを検出し、必要に応じてオイルリザーバー容量や分布を調整。  \n",
       "3. **ゼロクロス回数のバイアス化**  \n",
       "   - ゼロクロス回数が多いミッションでは、回転数に微小バイアスを加えてゼロクロスを減らすことで摩耗を抑制。  \n",
       "\n",
       "### 今後の研究課題  \n",
       "- 真空・低温・高温環境下での**リテーナインスタビリティ**発生機構の詳細解析。  \n",
       "- **油の長期酸化安定性**とベアリング材質との相互適合性の検証。  \n",
       "- **運用パターン（回転数・ゼロクロス・急加減速）**に応じた個別寿命評価手法の確立。  \n",
       "\n",
       "---\n",
       "\n",
       "#### 結論  \n",
       "試験により、最大回転数での3 か月間連続運転でも設計寿命を十分に上回る性能（約 58 年相当）が確認され、リテーナインスタビリティや熱・摩耗の進行は抑えられました。一方で急加減速・ゼロクロス時に局所的な摩耗が観測されることや、リテーナインスタビリティが発生した場合の熱増大・最終的な焼付けリスクは依然として課題として残ります。これらの結果を踏まえ、保持器設計の最適化や油分布管理、運用パターン別の寿命評価が今後の高信頼化・高性能化に向けて重要なポイントとなります。"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Workflow エージェントの動作確認\n",
    "import io\n",
    "import sys\n",
    "from IPython.display import Markdown, HTML, display\n",
    "\n",
    "# 中間ログをキャプチャしつつ、リアルタイムでもセルに出力する\n",
    "log_buffer = io.StringIO()\n",
    "\n",
    "\n",
    "class TeeStream:\n",
    "    \"\"\"stdout への出力を画面表示しつつバッファにも記録する。\"\"\"\n",
    "\n",
    "    def __init__(self, original, buffer):\n",
    "        self.original = original\n",
    "        self.buffer = buffer\n",
    "\n",
    "    def write(self, text):\n",
    "        self.original.write(text)\n",
    "        self.buffer.write(text)\n",
    "\n",
    "    def flush(self):\n",
    "        self.original.flush()\n",
    "\n",
    "\n",
    "_original_stdout = sys.stdout\n",
    "sys.stdout = TeeStream(_original_stdout, log_buffer)\n",
    "try:\n",
    "    result = await app.ainvoke(\n",
    "        {\"question\": \"高速回転ホイールの寿命試験ではどのような結果が得られましたか？\"}\n",
    "    )\n",
    "finally:\n",
    "    sys.stdout = _original_stdout\n",
    "\n",
    "# 中間ログを HTML で全文表示（Colab のセル出力トランケートを回避）\n",
    "log_text = log_buffer.getvalue()\n",
    "print(\"\\n--- 以下は HTML による全文ログ（トランケート回避） ---\")\n",
    "display(HTML(f\"<pre style='white-space:pre-wrap'>{log_text}</pre>\"))\n",
    "\n",
    "# 最終回答の表示\n",
    "print(\"=== Workflow エージェントの実行結果 ===\\n\")\n",
    "answer = result.get(\"answer\", \"\")\n",
    "if answer:\n",
    "    display(Markdown(answer))\n",
    "else:\n",
    "    print(\"[WARNING] 回答が空です。result keys:\", list(result.keys()))\n",
    "    print(\"search_results 件数:\", len(result.get(\"search_results\", [])))\n",
    "    print(\"loop_count:\", result.get(\"loop_count\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
