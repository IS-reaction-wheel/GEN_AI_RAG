{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 05 AI Agent Workflow + RAG（検索拡張生成）\n",
    "gpt-oss:20bを使用する構成のため、**Colab GPU は L4 を使用すること。**\n",
    "- 必要なライブラリをインストール\n",
    "- Google Colab に Ollama をセットアップ\n",
    "  - LLM モデルは gpt-oss:20b を使用（Ollama）\n",
    "  - Embedding モデルは ruri-v3-310m を使用（Sentence Transformers）\n",
    "  - Reranker モデルは cl-nagoya/ruri-v3-reranker-310m を使用（Sentence Transformers）\n",
    "- JAXA（宇宙航空研究開発機構）のリポジトリからデータをダウンロードして読み込み\n",
    "> [井澤克彦, 市川信一郎, 高速回転ホイール: 高速回転ホイール開発を通しての知見, 宇宙航空研究開発機構研究開発報告, 2008](https://jaxa.repo.nii.ac.jp/records/2149)\n",
    "- データの前処理\n",
    "  - markdown に変換（MarkItDown を使用）\n",
    "  - Unicode正規化 (NFKC), 1文字行ブロックの除去, 空行圧縮\n",
    "  - チャンク分割\n",
    "    - LangChain の SpacyTextSplitter を使用\n",
    "    - spaCy の日本語モデルは、ja_ginza を使用\n",
    "- ベクトルデータベースの構築（ChromaDB, インメモリ）\n",
    "- 検索機能の実装\n",
    "  - キーワード検索 @ BM25（spaCyで形態素解析の前処理が必要）\n",
    "  - Embedding model によるセマンティック検索\n",
    "  - ハイブリッド検索\n",
    "  - Reranker による再順位付け\n",
    "  - 検索機能をLLM の tool として定義\n",
    "- LangGraph による Workflow の実装\n",
    "  1. ユーザの質問を入力。\n",
    "  2. ユーザの質問に回答するためのタスク分割, 作成。\n",
    "  3. tool による検索。\n",
    "  4. tool による検索を終えて回答作成に進むか判断。再調査なら 3 に戻る。\n",
    "  5. ユーザへの回答の作成と提示。\n",
    "- 動作確認"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "**必要なライブラリをインストール**",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Google Colab に必要なライブラリをインストールする。\n# 1行にまとめることで pip が全パッケージの依存関係を一括解決する。\n# NOTE: Colab では uv ではなく pip を使う。uv は依存解決の過程で\n#       numpy 等をアップグレードし、プリインストール済みの scipy 等を壊すため。\n# NOTE: langchain 関連は 1.x 系に明示的に指定する。\n#       Colab プリインストールの 0.3.x が残ると langchain-mcp-adapters が動作しない。\n%pip install -U ollama langchain-ollama \\\n     \"langchain>=1.2.8\" \"langchain-core>=1.2.8\" \\\n     \"langgraph>=1.0.7\" \\\n     \"markitdown[all]\" chromadb \\\n     \"langchain-text-splitters>=0.3\" \\\n     spacy ginza ja-ginza \\\n     rank-bm25 sentence-transformers",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "**Google Colab に Ollama をセットアップ**",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Ollama のインストール・起動・モデルのダウンロード\n# 詳細は 01_connect_oss_llm.ipynb を参照\nimport subprocess\nimport time\nimport ollama  # type: ignore\n\n!apt-get install -y -qq zstd\n!curl -fsSL https://ollama.com/install.sh | sh\n\nprocess = subprocess.Popen(\n    [\"ollama\", \"serve\"],\n    stdout=subprocess.DEVNULL,\n    stderr=subprocess.DEVNULL,\n)\ntime.sleep(5)\n\n\ndef ollama_pull(model: str) -> None:\n    \"\"\"Ollama モデルをダウンロードし、進捗をインライン表示する。\"\"\"\n    for progress in ollama.pull(model, stream=True):\n        status = progress.get(\"status\", \"\")\n        total = progress.get(\"total\") or 0\n        completed = progress.get(\"completed\") or 0\n        if total:\n            line = f\"{status}: {completed / total:.0%}\"\n        else:\n            line = status\n        print(f\"\\r{line:<60}\", end=\"\", flush=True)\n    print(f\"\\n{model}: Done!\")\n\n\nmodel_name = \"gpt-oss:20b\"\nollama_pull(model_name)\n!ollama show {model_name}",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "**ChatOllama で LLM に接続**",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# ChatOllama で LLM に接続する。\nfrom langchain_ollama import ChatOllama  # type: ignore\n\nllm = ChatOllama(\n    model=\"gpt-oss:20b\",\n    num_ctx=16384,\n    num_predict=-1,\n    temperature=0.8,\n    top_k=40,\n    top_p=0.9,\n    repeat_penalty=1.1,\n    reasoning=None,\n)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "**Embedding モデル（ruri-v3-310m）と Reranker モデルのセットアップ**",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Embedding: ruri-v3-310m (Sentence Transformers 経由)\nfrom langchain_core.embeddings import Embeddings  # type: ignore\nfrom sentence_transformers import SentenceTransformer, CrossEncoder  # type: ignore\n\n\nclass RuriEmbeddings(Embeddings):\n    \"\"\"ruri-v3 を LangChain の Embeddings インターフェースでラップする。\"\"\"\n\n    def __init__(self, model_name: str = \"cl-nagoya/ruri-v3-310m\") -> None:\n        self.model = SentenceTransformer(model_name)\n\n    def embed_documents(self, texts: list[str]) -> list[list[float]]:\n        prefixed = [f\"検索文書: {t}\" for t in texts]\n        return self.model.encode(prefixed).tolist()\n\n    def embed_query(self, text: str) -> list[float]:\n        return self.model.encode(f\"検索クエリ: {text}\").tolist()\n\n\nembeddings = RuriEmbeddings()\ntest_vec = embeddings.embed_query(\"テスト文です\")\nprint(f\"Embedding dim: {len(test_vec)}\")\n\n# Reranker: cl-nagoya/ruri-v3-reranker-310m\nreranker = CrossEncoder(\"cl-nagoya/ruri-v3-reranker-310m\")\nprint(\"Reranker model loaded.\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "**JAXA リポジトリからデータをダウンロード → 前処理 → チャンク分割**",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# JAXA リポジトリから PDF をダウンロードし、MarkItDown で markdown に変換する。\nimport urllib.request\nimport unicodedata\nimport re\nfrom pathlib import Path\nfrom markitdown import MarkItDown  # type: ignore\n\npdf_url = \"https://jaxa.repo.nii.ac.jp/record/2149/files/63826000.pdf\"\npdf_path = Path(\"高速回転ホイール.pdf\")\n\nif not pdf_path.exists():\n    urllib.request.urlretrieve(pdf_url, pdf_path)\n    print(f\"ダウンロード完了: {pdf_path}\")\n\nmd = MarkItDown()\nresult = md.convert(str(pdf_path))\nraw_text = result.text_content\nprint(f\"変換後の文字数: {len(raw_text)}\")\n\n# Unicode 正規化 (NFKC)\ntext = unicodedata.normalize(\"NFKC\", raw_text)\n\n\n# PDF 抽出テキストの汎用クリーニング\ndef clean_pdf_text(text: str) -> str:\n    \"\"\"1文字行ブロックの除去 + 空行圧縮。\"\"\"\n    text = re.sub(\n        r\"(^[^\\S\\n]*\\S[^\\S\\n]*$\\n?){3,}\",\n        \"\\n\",\n        text,\n        flags=re.MULTILINE,\n    )\n    text = re.sub(r\"\\n{3,}\", \"\\n\\n\", text)\n    return text.strip()\n\n\ntext = clean_pdf_text(text)\nprint(f\"クリーニング後の文字数: {len(text)}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# SpacyTextSplitter でチャンク分割する。\n# Sudachi の入力上限 (49,149 bytes) を超えないように事前分割してから渡す。\nfrom langchain_text_splitters import SpacyTextSplitter  # type: ignore\n\nCHUNK_SIZE = 1500\nCHUNK_OVERLAP = 300\nBLOCK_MAX_BYTES = 40_000\nBLOCK_OVERLAP_CHARS = CHUNK_SIZE\n\n\ndef split_into_safe_blocks(\n    text: str,\n    max_bytes: int = BLOCK_MAX_BYTES,\n    overlap_chars: int = BLOCK_OVERLAP_CHARS,\n) -> list[str]:\n    \"\"\"テキストを段落区切りで max_bytes 以下のブロックに分割する。\"\"\"\n    paragraphs = text.split(\"\\n\\n\")\n    blocks: list[str] = []\n    current = \"\"\n    for para in paragraphs:\n        candidate = current + \"\\n\\n\" + para if current else para\n        if len(candidate.encode(\"utf-8\")) > max_bytes and current:\n            blocks.append(current)\n            current = current[-overlap_chars:] + \"\\n\\n\" + para\n        else:\n            current = candidate\n    if current:\n        blocks.append(current)\n    return blocks\n\n\ntext_splitter = SpacyTextSplitter(\n    separator=\"\\n\\n\",\n    pipeline=\"ja_ginza\",\n    chunk_size=CHUNK_SIZE,\n    chunk_overlap=CHUNK_OVERLAP,\n)\n\nblocks = split_into_safe_blocks(text)\nchunks: list[str] = []\nfor block in blocks:\n    chunks.extend(text_splitter.split_text(block))\n\nprint(f\"チャンク数: {len(chunks)}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "**ベクトルデータベースの構築（ChromaDB, インメモリ）**",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# ChromaDB にチャンクを格納する（インメモリ）。\nimport chromadb  # type: ignore\n\nchroma_client = chromadb.Client()\n\ncollection = chroma_client.create_collection(\n    name=\"jaxa_wheel\",\n    metadata={\"hnsw:space\": \"cosine\"},\n)\n\nchunk_embeddings = embeddings.embed_documents(chunks)\n\ncollection.add(\n    ids=[f\"chunk_{i}\" for i in range(len(chunks))],\n    documents=chunks,\n    embeddings=chunk_embeddings,\n)\n\nprint(f\"ChromaDB に {collection.count()} 件のチャンクを格納しました。\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "**検索機能の実装（BM25 + セマンティック + ハイブリッド + Reranker）と tool 定義**",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# BM25 用の前処理: spaCy (ja_ginza) で形態素解析してトークン化する。\nimport numpy as np  # type: ignore\nimport spacy  # type: ignore\nfrom rank_bm25 import BM25Okapi  # type: ignore\n\nnlp = spacy.load(\"ja_ginza\", disable=[\"parser\", \"ner\"])\n\n\ndef tokenize(text: str) -> list[str]:\n    \"\"\"spaCy で形態素解析し、BM25 用のトークンリストを返す。\"\"\"\n    doc = nlp(text)\n    tokens = []\n    include_pos = {\"NOUN\", \"VERB\", \"ADJ\", \"PROPN\", \"NUM\"}\n    for token in doc:\n        if token.pos_ not in include_pos:\n            continue\n        if token.is_stop:\n            continue\n        lemma = token.lemma_\n        if len(lemma) == 1 and re.match(r\"[ぁ-ん\\u30fc!-/:-@\\[-`{-~]\", lemma):\n            continue\n        tokens.append(lemma)\n    return tokens\n\n\ntokenized_chunks = [tokenize(chunk) for chunk in chunks]\nbm25 = BM25Okapi(tokenized_chunks)\nprint(f\"BM25 インデックス構築完了: {len(tokenized_chunks)} 件\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# 検索関数の定義（BM25, セマンティック, ハイブリッド, Reranker）\nRETRIEVAL_TOP_K = 20\nRERANK_TOP_K = 5\nBM25_WEIGHT = 0.3\n\n\ndef search_bm25(query: str, top_k: int = RETRIEVAL_TOP_K) -> list[dict]:\n    \"\"\"BM25 によるキーワード検索を行う。\"\"\"\n    tokenized_query = tokenize(query)\n    scores = bm25.get_scores(tokenized_query)\n    top_indices = np.argsort(scores)[::-1][:top_k]\n    return [\n        {\"rank\": rank + 1, \"chunk_id\": int(idx), \"score\": float(scores[idx]), \"text\": chunks[idx]}\n        for rank, idx in enumerate(top_indices)\n        if scores[idx] > 0\n    ]\n\n\ndef search_semantic(query: str, top_k: int = RETRIEVAL_TOP_K) -> list[dict]:\n    \"\"\"Embedding model によるセマンティック検索を行う。\"\"\"\n    query_embedding = embeddings.embed_query(query)\n    results = collection.query(query_embeddings=[query_embedding], n_results=top_k)\n    return [\n        {\"rank\": rank + 1, \"chunk_id\": int(doc_id.split(\"_\")[1]), \"score\": 1.0 - dist, \"text\": doc}\n        for rank, (doc_id, doc, dist) in enumerate(\n            zip(results[\"ids\"][0], results[\"documents\"][0], results[\"distances\"][0])\n        )\n    ]\n\n\ndef search_hybrid(\n    query: str, top_k: int = RETRIEVAL_TOP_K, bm25_weight: float = BM25_WEIGHT\n) -> list[dict]:\n    \"\"\"BM25 とセマンティック検索の RRF ハイブリッド検索を行う。\"\"\"\n    k = 60\n    bm25_results = search_bm25(query, top_k=top_k)\n    semantic_results = search_semantic(query, top_k=top_k)\n\n    scores: dict[int, float] = {}\n    texts: dict[int, str] = {}\n\n    for r in bm25_results:\n        cid = r[\"chunk_id\"]\n        scores[cid] = scores.get(cid, 0) + bm25_weight / (k + r[\"rank\"])\n        texts[cid] = r[\"text\"]\n\n    semantic_weight = 1.0 - bm25_weight\n    for r in semantic_results:\n        cid = r[\"chunk_id\"]\n        scores[cid] = scores.get(cid, 0) + semantic_weight / (k + r[\"rank\"])\n        texts[cid] = r[\"text\"]\n\n    sorted_ids = sorted(scores, key=lambda cid: scores[cid], reverse=True)[:top_k]\n    return [\n        {\"rank\": rank + 1, \"chunk_id\": cid, \"score\": scores[cid], \"text\": texts[cid]}\n        for rank, cid in enumerate(sorted_ids)\n    ]\n\n\ndef rerank(query: str, results: list[dict], top_k: int = RERANK_TOP_K) -> list[dict]:\n    \"\"\"Reranker (CrossEncoder) で検索結果を再順位付けする。\"\"\"\n    if not results:\n        return []\n    pairs = [(query, r[\"text\"]) for r in results]\n    scores = reranker.predict(pairs)\n    ranked_indices = np.argsort(scores)[::-1][:top_k]\n    return [\n        {\"rank\": rank + 1, \"chunk_id\": results[idx][\"chunk_id\"], \"score\": float(scores[idx]), \"text\": results[idx][\"text\"]}\n        for rank, idx in enumerate(ranked_indices)\n    ]\n\n\nprint(\"検索関数を定義しました: search_bm25, search_semantic, search_hybrid, rerank\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# 検索機能を LLM の tool として定義する。\nfrom langchain_core.tools import tool  # type: ignore\n\nMAX_RETURN_CHARS = 8000\n\n\n@tool\ndef search_document(query: str) -> str:\n    \"\"\"外部ナレッジベースから、クエリに関連する情報を検索・取得します。\n    ユーザーの質問に対し、具体的な事実、データ、あるいは詳細な文脈が必要な場合、\n    自身の知識だけで回答せずに必ずこのツールを使用してください。\n\n    Args:\n        query: 検索したい内容を表す、具体的かつ完全な文章（日本語）。\n    \"\"\"\n    try:\n        hybrid_results = search_hybrid(query)\n        reranked = rerank(query, hybrid_results)\n    except Exception as e:\n        return f\"検索中にエラーが発生しました: {e}\"\n\n    if not reranked:\n        return \"検索結果が見つかりませんでした。\"\n\n    passages = []\n    total_chars = 0\n    for r in reranked:\n        passage = f\"[チャンク {r['chunk_id']}] (スコア: {r['score']:.4f})\\n{r['text']}\"\n        total_chars += len(passage)\n        if total_chars > MAX_RETURN_CHARS:\n            break\n        passages.append(passage)\n    return \"\\n\\n---\\n\\n\".join(passages)\n\n\nsearch_tool = search_document\nprint(f\"RAG Tool: {search_tool.name}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "**LangGraph による Workflow の実装**\n\n03_02 の Workflow を RAG 検索用に適応する。web_search ノードを doc_search ノードに置き換え、\nMCP サーバの代わりに `search_document` ツールで検索する。\n\n**Workflow の流れ**\n1. **task_planning**: ユーザの質問を受け取り、回答に必要なサブタスク（目的＋検索クエリ）を構造化して作成する。\n2. **doc_search**: 各サブタスクの検索クエリを `search_document` ツールで実行し、目的と紐付けた検索結果を蓄積する。\n3. **judge**: サブタスクの目的ごとに、検索結果が十分かを LLM が判断する。不足なら追加サブタスクを生成して doc_search に戻る。\n4. **generate_answer**: 目的ごとに整理された検索結果をもとに、ユーザの質問に対する最終回答を生成する。",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Workflow の状態定義・共通ユーティリティ・システムプロンプト\nimport json\nfrom typing import TypedDict\nfrom langchain_core.messages import HumanMessage, SystemMessage  # type: ignore\nfrom langgraph.graph import StateGraph, START, END  # type: ignore\nfrom IPython.display import Image, display\n\n# --- グローバル設定 ---\nMAX_LOOP_COUNT = 2  # judge → doc_search 再調査ループの上限回数\n\n# --- Workflow の状態 ---\nclass WorkflowState(TypedDict):\n    question: str\n    subtasks: list[dict]      # [{\"purpose\": str, \"queries\": [str]}]\n    search_results: list[str]  # 目的と紐付けた検索結果\n    answer: str\n    loop_count: int\n\n\n# --- 共通ユーティリティ: LLM 出力から JSON を抽出 ---\ndef extract_json_text(raw: str) -> str:\n    \"\"\"LLM の出力から JSON 文字列を抽出する。\"\"\"\n    text = raw.strip()\n\n    code_block_match = re.search(r\"```(?:json)?\\s*(.*?)```\", text, re.DOTALL)\n    if code_block_match:\n        text = code_block_match.group(1).strip()\n\n    try:\n        json.loads(text)\n        return text\n    except json.JSONDecodeError:\n        pass\n\n    match_start = re.search(r\"[\\{\\[]\", text)\n    if not match_start:\n        return text\n\n    first_brace_index = match_start.start()\n    start_char = text[first_brace_index]\n    end_char = \"]\" if start_char == \"[\" else \"}\"\n\n    last_brace_index = text.rfind(end_char)\n    if last_brace_index > first_brace_index:\n        return text[first_brace_index : last_brace_index + 1]\n\n    return text\n\n\n# --- 各ノードのシステムプロンプト ---\nSYSTEM_PROMPT_TASK_PLANNING = \"\"\"\\\nあなたはリサーチプランナーです。\nユーザの質問に回答するために、ナレッジベース（技術文書）を検索するためのサブタスクを作成してください。\n\n出力は以下の JSON 配列のみとし、他のテキストは一切含めないでください。\nサブタスクは最大3個までとしてください。\n\n出力形式:\n[\n  {\"purpose\": \"このサブタスクで明らかにしたいこと\",\n   \"queries\": [\"検索クエリ1\", \"検索クエリ2\"]},\n  ...\n]\n\npurpose は判定ステップで「この目的に十分な情報が得られたか」を評価する基準になります。\n具体的かつ明確に書いてください。\n検索クエリは、技術文書から関連情報を検索するための日本語の具体的なフレーズにしてください。\n\"\"\"\n\nSYSTEM_PROMPT_JUDGE = \"\"\"\\\nあなたはリサーチの品質を判定する審査員です。\nユーザの質問と検索結果を見て、回答に十分な情報があるか判断してください。\n検索結果には【目的: ...】タグが付いています。\n各目的について十分な情報が得られているかを確認してください。\n\n十分な場合:\n{\"sufficient\": true, \"reason\": \"判断理由を日本語で1文で\"}\n\n不足の場合（不足している目的について追加サブタスクを生成）:\n{\"sufficient\": false, \"reason\": \"何が不足しているかを日本語で1文で\",\n \"additional_subtasks\": [\n    {\"purpose\": \"追加で明らかにしたいこと\",\n     \"queries\": [\"追加クエリ1\"]}\n  ]\n}\n\nJSON のみ出力し、他のテキストは含めないでください。\n\"\"\"\n\nSYSTEM_PROMPT_GENERATE_ANSWER = \"\"\"\\\nあなたはリサーチ結果をもとに回答するAIアシスタントです。\n検索結果を参考に、ユーザの質問に日本語で丁寧に回答してください。\n回答は必ず検索結果に基づいて作成し、検索結果に含まれない情報は含めないでください。\n回答の最後に、以下の形式で結論をまとめてください。\n\n# 結論\n- ユーザの質問: （質問内容）\n- 回答: （簡潔な回答）\n\"\"\"\n\nprint(\"Workflow の状態定義・ユーティリティ・システムプロンプトを定義しました。\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Workflow ノードの定義\n\n# ノード 1: task_planning（タスク分割）\nasync def task_planning(state: WorkflowState) -> dict:\n    \"\"\"ユーザの質問を分析し、サブタスク（目的＋検索クエリ）を作成する。\"\"\"\n    question = state[\"question\"]\n\n    response = await llm.ainvoke(\n        [\n            SystemMessage(content=SYSTEM_PROMPT_TASK_PLANNING),\n            HumanMessage(content=question),\n        ]\n    )\n\n    text = extract_json_text(response.content)\n\n    try:\n        subtasks = json.loads(text)\n    except json.JSONDecodeError:\n        print(f\"[task_planning] JSON パース失敗 → フォールバック: {text[:100]}\")\n        subtasks = [{\"purpose\": \"基本調査\", \"queries\": [question]}]\n\n    print(f\"[task_planning] サブタスク数: {len(subtasks)}\")\n    for i, st in enumerate(subtasks):\n        print(f\"  {i + 1}. 目的: {st['purpose']}\")\n        print(f\"     クエリ: {st['queries']}\")\n    return {\"subtasks\": subtasks, \"search_results\": [], \"loop_count\": 0}\n\n\n# ノード 2: doc_search（ドキュメント検索）\nasync def doc_search(state: WorkflowState) -> dict:\n    \"\"\"各サブタスクの検索クエリを search_document ツールで実行し、結果を蓄積する。\"\"\"\n    subtasks = state[\"subtasks\"]\n    results = list(state.get(\"search_results\") or [])\n\n    for st in subtasks:\n        purpose = st[\"purpose\"]\n        print(f\"[doc_search] 目的: {purpose}\")\n        for query in st[\"queries\"]:\n            print(f\"  検索中: {query}\")\n            try:\n                result = search_tool.invoke({\"query\": query})\n            except Exception as e:\n                print(f\"  [ERROR] クエリ失敗: {query} → {e}\")\n                continue\n            if not result or result == \"検索結果が見つかりませんでした。\":\n                print(f\"  [SKIP] 検索結果なし: {query}\")\n                continue\n            results.append(f\"【目的: {purpose}】\\n【クエリ: {query}】\\n{result}\")\n\n    return {\"search_results\": results, \"subtasks\": []}\n\n\n# ノード 3: judge（判定）\nasync def judge(state: WorkflowState) -> dict:\n    \"\"\"検索結果が十分かを判断し、不足なら追加サブタスクを生成する。\"\"\"\n    question = state[\"question\"]\n    results = state[\"search_results\"]\n    loop_count = state.get(\"loop_count\", 0)\n\n    if loop_count >= MAX_LOOP_COUNT:\n        print(\"[judge] ループ上限に到達 → 回答作成へ\")\n        return {\"subtasks\": [], \"loop_count\": loop_count}\n\n    results_text = \"\\n\\n\".join(results)\n\n    response = await llm.ainvoke(\n        [\n            SystemMessage(content=SYSTEM_PROMPT_JUDGE),\n            HumanMessage(content=f\"質問: {question}\\n\\n検索結果:\\n{results_text}\"),\n        ]\n    )\n\n    text = extract_json_text(response.content)\n\n    try:\n        judgment = json.loads(text)\n    except json.JSONDecodeError:\n        print(f\"[judge] JSON パース失敗 → 回答作成へ: {text[:100]}\")\n        return {\"subtasks\": [], \"loop_count\": loop_count + 1}\n\n    reason = judgment.get(\"reason\", \"\")\n\n    if judgment.get(\"sufficient\", True):\n        print(f\"[judge] 情報十分 → 回答作成へ（理由: {reason}）\")\n        return {\"subtasks\": [], \"loop_count\": loop_count + 1}\n    else:\n        additional = judgment.get(\"additional_subtasks\", [])\n        print(f\"[judge] 情報不足（理由: {reason}）→ 追加サブタスク:\")\n        for i, st in enumerate(additional):\n            print(f\"  {i + 1}. 目的: {st.get('purpose', '?')}\")\n            print(f\"     クエリ: {st.get('queries', [])}\")\n        return {\"subtasks\": additional, \"loop_count\": loop_count + 1}\n\n\n# ルーター: judge の結果で分岐\ndef should_continue_search(state: WorkflowState) -> str:\n    \"\"\"追加サブタスクがあれば doc_search に戻り、なければ回答生成へ。\"\"\"\n    if state.get(\"subtasks\"):\n        return \"doc_search\"\n    return \"generate_answer\"\n\n\n# ノード 4: generate_answer（回答生成）\nasync def generate_answer(state: WorkflowState) -> dict:\n    \"\"\"蓄積した検索結果をもとに最終回答を生成する。\"\"\"\n    question = state[\"question\"]\n    results_text = \"\\n\\n\".join(state[\"search_results\"])\n\n    response = await llm.ainvoke(\n        [\n            SystemMessage(content=SYSTEM_PROMPT_GENERATE_ANSWER),\n            HumanMessage(content=f\"質問: {question}\\n\\n検索結果:\\n{results_text}\"),\n        ]\n    )\n\n    answer = response.content or \"\"\n\n    if not answer:\n        print(\"[generate_answer] WARNING: response.content が空です\")\n        print(f\"  response type: {type(response)}\")\n        print(f\"  response repr: {repr(response)[:500]}\")\n\n    print(\"[generate_answer] 回答生成完了\")\n    return {\"answer\": answer}\n\n\nprint(\"Workflow ノードを定義しました: task_planning, doc_search, judge, generate_answer\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Workflow グラフの構築とコンパイル\nworkflow = StateGraph(WorkflowState)\n\n# ノードの登録\nworkflow.add_node(\"task_planning\", task_planning)\nworkflow.add_node(\"doc_search\", doc_search)\nworkflow.add_node(\"judge\", judge)\nworkflow.add_node(\"generate_answer\", generate_answer)\n\n# エッジの定義\nworkflow.add_edge(START, \"task_planning\")\nworkflow.add_edge(\"task_planning\", \"doc_search\")\nworkflow.add_edge(\"doc_search\", \"judge\")\n\n# 条件分岐: judge → doc_search（再調査） or generate_answer（回答生成）\nworkflow.add_conditional_edges(\n    \"judge\",\n    should_continue_search,\n    {\n        \"doc_search\": \"doc_search\",\n        \"generate_answer\": \"generate_answer\",\n    },\n)\nworkflow.add_edge(\"generate_answer\", END)\n\n# コンパイル\napp = workflow.compile()\n\n# グラフの可視化\ndisplay(Image(app.get_graph().draw_mermaid_png()))",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "**動作確認**",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Workflow エージェントの動作確認\nimport io\nimport sys\nfrom IPython.display import Markdown, HTML, display\n\n# 中間ログをキャプチャしつつ、リアルタイムでもセルに出力する\nlog_buffer = io.StringIO()\n\n\nclass TeeStream:\n    \"\"\"stdout への出力を画面表示しつつバッファにも記録する。\"\"\"\n\n    def __init__(self, original, buffer):\n        self.original = original\n        self.buffer = buffer\n\n    def write(self, text):\n        self.original.write(text)\n        self.buffer.write(text)\n\n    def flush(self):\n        self.original.flush()\n\n\n_original_stdout = sys.stdout\nsys.stdout = TeeStream(_original_stdout, log_buffer)\ntry:\n    result = await app.ainvoke(\n        {\"question\": \"高速回転ホイールの寿命試験ではどのような結果が得られましたか？\"}\n    )\nfinally:\n    sys.stdout = _original_stdout\n\n# 中間ログを HTML で全文表示（Colab のセル出力トランケートを回避）\nlog_text = log_buffer.getvalue()\nprint(\"\\n--- 以下は HTML による全文ログ（トランケート回避） ---\")\ndisplay(HTML(f\"<pre style='white-space:pre-wrap'>{log_text}</pre>\"))\n\n# 最終回答の表示\nprint(\"=== Workflow エージェントの実行結果 ===\\n\")\nanswer = result.get(\"answer\", \"\")\nif answer:\n    display(Markdown(answer))\nelse:\n    print(\"[WARNING] 回答が空です。result keys:\", list(result.keys()))\n    print(\"search_results 件数:\", len(result.get(\"search_results\", [])))\n    print(\"loop_count:\", result.get(\"loop_count\"))",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}