{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 05 AI Agent Workflow + RAG（検索拡張生成）\n",
    "gpt-oss:20bを使用する構成のため、**Colab GPU は L4 を使用すること。**\n",
    "- 必要なライブラリをインストール\n",
    "- Google Colab に Ollama をセットアップ\n",
    "  - LLM モデルは gpt-oss:20b を使用（Ollama）\n",
    "  - Embedding モデルは ruri-v3-310m を使用（Sentence Transformers）\n",
    "  - Reranker モデルは cl-nagoya/ruri-v3-reranker-310m を使用（Sentence Transformers）\n",
    "- JAXA（宇宙航空研究開発機構）のリポジトリからデータをダウンロードして読み込み\n",
    "> [井澤克彦, 市川信一郎, 高速回転ホイール: 高速回転ホイール開発を通しての知見, 宇宙航空研究開発機構研究開発報告, 2008](https://jaxa.repo.nii.ac.jp/records/2149)\n",
    "- データの前処理\n",
    "  - markdown に変換（MarkItDown を使用）\n",
    "  - Unicode正規化 (NFKC), 1文字行ブロックの除去, 空行圧縮\n",
    "  - チャンク分割\n",
    "    - LangChain の SpacyTextSplitter を使用\n",
    "    - spaCy の日本語モデルは、ja_ginza を使用\n",
    "- ベクトルデータベースの構築（ChromaDB, インメモリ）\n",
    "- 検索機能の実装\n",
    "  - キーワード検索 @ BM25（spaCyで形態素解析の前処理が必要）\n",
    "  - Embedding model によるセマンティック検索\n",
    "  - ハイブリッド検索\n",
    "  - Reranker による再順位付け\n",
    "  - 検索機能をLLM の tool として定義\n",
    "- LangGraph による Workflow の実装\n",
    "  1. ユーザの質問を入力。\n",
    "  2. ユーザの質問に回答するためのタスク分割, 作成。\n",
    "  3. tool による検索。\n",
    "  4. tool による検索を終えて回答作成に進むか判断。再調査なら 3 に戻る。\n",
    "  5. ユーザへの回答の作成と提示。\n",
    "- 動作確認"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**必要なライブラリをインストール**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ollama\n",
      "  Downloading ollama-0.6.1-py3-none-any.whl.metadata (4.3 kB)\n",
      "Collecting langchain-ollama\n",
      "  Downloading langchain_ollama-1.0.1-py3-none-any.whl.metadata (2.5 kB)\n",
      "Requirement already satisfied: langchain>=1.2.8 in /usr/local/lib/python3.12/dist-packages (1.2.8)\n",
      "Collecting langchain>=1.2.8\n",
      "  Downloading langchain-1.2.10-py3-none-any.whl.metadata (5.7 kB)\n",
      "Requirement already satisfied: langchain-core>=1.2.8 in /usr/local/lib/python3.12/dist-packages (1.2.9)\n",
      "Collecting langchain-core>=1.2.8\n",
      "  Downloading langchain_core-1.2.11-py3-none-any.whl.metadata (4.4 kB)\n",
      "Requirement already satisfied: langgraph>=1.0.7 in /usr/local/lib/python3.12/dist-packages (1.0.7)\n",
      "Collecting langgraph>=1.0.7\n",
      "  Downloading langgraph-1.0.8-py3-none-any.whl.metadata (7.4 kB)\n",
      "Collecting chromadb\n",
      "  Downloading chromadb-1.5.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.2 kB)\n",
      "Collecting langchain-text-splitters>=0.3\n",
      "  Downloading langchain_text_splitters-1.1.0-py3-none-any.whl.metadata (2.7 kB)\n",
      "Requirement already satisfied: spacy in /usr/local/lib/python3.12/dist-packages (3.8.11)\n",
      "Collecting ginza\n",
      "  Downloading ginza-5.2.0-py3-none-any.whl.metadata (448 bytes)\n",
      "Collecting ja-ginza\n",
      "  Downloading ja_ginza-5.2.0-py3-none-any.whl.metadata (5.8 kB)\n",
      "Collecting rank-bm25\n",
      "  Downloading rank_bm25-0.2.2-py3-none-any.whl.metadata (3.2 kB)\n",
      "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.12/dist-packages (5.2.2)\n",
      "Collecting markitdown[all]\n",
      "  Downloading markitdown-0.1.4-py3-none-any.whl.metadata (4.0 kB)\n",
      "Requirement already satisfied: httpx>=0.27 in /usr/local/lib/python3.12/dist-packages (from ollama) (0.28.1)\n",
      "Requirement already satisfied: pydantic>=2.9 in /usr/local/lib/python3.12/dist-packages (from ollama) (2.12.3)\n",
      "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core>=1.2.8) (1.33)\n",
      "Requirement already satisfied: langsmith<1.0.0,>=0.3.45 in /usr/local/lib/python3.12/dist-packages (from langchain-core>=1.2.8) (0.6.9)\n",
      "Requirement already satisfied: packaging>=23.2.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core>=1.2.8) (26.0)\n",
      "Requirement already satisfied: pyyaml<7.0.0,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core>=1.2.8) (6.0.3)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core>=1.2.8) (9.1.3)\n",
      "Requirement already satisfied: typing-extensions<5.0.0,>=4.7.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core>=1.2.8) (4.15.0)\n",
      "Requirement already satisfied: uuid-utils<1.0,>=0.12.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core>=1.2.8) (0.14.0)\n",
      "Requirement already satisfied: langgraph-checkpoint<5.0.0,>=2.1.0 in /usr/local/lib/python3.12/dist-packages (from langgraph>=1.0.7) (4.0.0)\n",
      "Requirement already satisfied: langgraph-prebuilt<1.1.0,>=1.0.7 in /usr/local/lib/python3.12/dist-packages (from langgraph>=1.0.7) (1.0.7)\n",
      "Requirement already satisfied: langgraph-sdk<0.4.0,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from langgraph>=1.0.7) (0.3.3)\n",
      "Requirement already satisfied: xxhash>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from langgraph>=1.0.7) (3.6.0)\n",
      "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.12/dist-packages (from markitdown[all]) (4.13.5)\n",
      "Requirement already satisfied: charset-normalizer in /usr/local/lib/python3.12/dist-packages (from markitdown[all]) (3.4.4)\n",
      "Requirement already satisfied: defusedxml in /usr/local/lib/python3.12/dist-packages (from markitdown[all]) (0.7.1)\n",
      "Collecting magika~=0.6.1 (from markitdown[all])\n",
      "  Downloading magika-0.6.3-py3-none-manylinux_2_28_x86_64.whl.metadata (10 kB)\n",
      "Collecting markdownify (from markitdown[all])\n",
      "  Downloading markdownify-1.2.2-py3-none-any.whl.metadata (9.9 kB)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from markitdown[all]) (2.32.4)\n",
      "Collecting azure-ai-documentintelligence (from markitdown[all])\n",
      "  Downloading azure_ai_documentintelligence-1.0.2-py3-none-any.whl.metadata (53 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.0/53.0 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting azure-identity (from markitdown[all])\n",
      "  Downloading azure_identity-1.25.2-py3-none-any.whl.metadata (90 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.9/90.9 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: lxml in /usr/local/lib/python3.12/dist-packages (from markitdown[all]) (6.0.2)\n",
      "Collecting mammoth~=1.11.0 (from markitdown[all])\n",
      "  Downloading mammoth-1.11.0-py2.py3-none-any.whl.metadata (26 kB)\n",
      "Collecting olefile (from markitdown[all])\n",
      "  Downloading olefile-0.47-py2.py3-none-any.whl.metadata (9.7 kB)\n",
      "Requirement already satisfied: openpyxl in /usr/local/lib/python3.12/dist-packages (from markitdown[all]) (3.1.5)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from markitdown[all]) (2.2.2)\n",
      "Collecting pdfminer-six>=20251107 (from markitdown[all])\n",
      "  Downloading pdfminer_six-20260107-py3-none-any.whl.metadata (4.3 kB)\n",
      "Requirement already satisfied: pydub in /usr/local/lib/python3.12/dist-packages (from markitdown[all]) (0.25.1)\n",
      "Collecting python-pptx (from markitdown[all])\n",
      "  Downloading python_pptx-1.0.2-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting speechrecognition (from markitdown[all])\n",
      "  Downloading speechrecognition-3.14.5-py3-none-any.whl.metadata (30 kB)\n",
      "Requirement already satisfied: xlrd in /usr/local/lib/python3.12/dist-packages (from markitdown[all]) (2.0.2)\n",
      "Collecting youtube-transcript-api~=1.0.0 (from markitdown[all])\n",
      "  Downloading youtube_transcript_api-1.0.3-py3-none-any.whl.metadata (23 kB)\n",
      "Collecting build>=1.0.3 (from chromadb)\n",
      "  Downloading build-1.4.0-py3-none-any.whl.metadata (5.8 kB)\n",
      "Collecting pybase64>=1.4.1 (from chromadb)\n",
      "  Downloading pybase64-1.4.3-cp312-cp312-manylinux1_x86_64.manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_5_x86_64.whl.metadata (8.7 kB)\n",
      "Requirement already satisfied: uvicorn>=0.18.3 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.40.0)\n",
      "Requirement already satisfied: numpy>=1.22.5 in /usr/local/lib/python3.12/dist-packages (from chromadb) (2.0.2)\n",
      "Collecting posthog<6.0.0,>=2.4.0 (from chromadb)\n",
      "  Downloading posthog-5.4.0-py3-none-any.whl.metadata (5.7 kB)\n",
      "Collecting onnxruntime>=1.14.1 (from chromadb)\n",
      "  Downloading onnxruntime-1.24.1-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (4.9 kB)\n",
      "Requirement already satisfied: opentelemetry-api>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (1.38.0)\n",
      "Collecting opentelemetry-exporter-otlp-proto-grpc>=1.2.0 (from chromadb)\n",
      "  Downloading opentelemetry_exporter_otlp_proto_grpc-1.39.1-py3-none-any.whl.metadata (2.5 kB)\n",
      "Requirement already satisfied: opentelemetry-sdk>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (1.38.0)\n",
      "Requirement already satisfied: tokenizers>=0.13.2 in /usr/local/lib/python3.12/dist-packages (from chromadb) (0.22.2)\n",
      "Collecting pypika>=0.48.9 (from chromadb)\n",
      "  Downloading pypika-0.51.1-py2.py3-none-any.whl.metadata (51 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.0/52.0 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: tqdm>=4.65.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (4.67.3)\n",
      "Requirement already satisfied: overrides>=7.3.1 in /usr/local/lib/python3.12/dist-packages (from chromadb) (7.7.0)\n",
      "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.12/dist-packages (from chromadb) (6.5.2)\n",
      "Requirement already satisfied: grpcio>=1.58.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (1.76.0)\n",
      "Collecting bcrypt>=4.0.1 (from chromadb)\n",
      "  Downloading bcrypt-5.0.0-cp39-abi3-manylinux_2_34_x86_64.whl.metadata (10 kB)\n",
      "Requirement already satisfied: typer>=0.9.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (0.21.1)\n",
      "Collecting kubernetes>=28.1.0 (from chromadb)\n",
      "  Downloading kubernetes-35.0.0-py2.py3-none-any.whl.metadata (1.7 kB)\n",
      "Requirement already satisfied: mmh3>=4.0.1 in /usr/local/lib/python3.12/dist-packages (from chromadb) (5.2.0)\n",
      "Requirement already satisfied: orjson>=3.9.12 in /usr/local/lib/python3.12/dist-packages (from chromadb) (3.11.7)\n",
      "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (13.9.4)\n",
      "Requirement already satisfied: jsonschema>=4.19.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (4.26.0)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (1.0.15)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.0.13)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /usr/local/lib/python3.12/dist-packages (from spacy) (8.3.10)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.12/dist-packages (from spacy) (1.1.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.5.2)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.5.0,>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from spacy) (0.4.3)\n",
      "Requirement already satisfied: typer-slim<1.0.0,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (0.21.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.1.6)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from spacy) (75.2.0)\n",
      "Collecting plac>=1.3.3 (from ginza)\n",
      "  Downloading plac-1.4.5-py2.py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting SudachiPy<0.7.0,>=0.6.2 (from ginza)\n",
      "  Downloading SudachiPy-0.6.10-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Collecting SudachiDict-core>=20210802 (from ginza)\n",
      "  Downloading sudachidict_core-20260116-py3-none-any.whl.metadata (2.7 kB)\n",
      "Requirement already satisfied: transformers<6.0.0,>=4.41.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (5.0.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (1.4.0)\n",
      "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (2.9.0+cu128)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (1.6.1)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (1.16.3)\n",
      "Collecting pyproject_hooks (from build>=1.0.3->chromadb)\n",
      "  Downloading pyproject_hooks-1.2.0-py3-none-any.whl.metadata (1.3 kB)\n",
      "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx>=0.27->ollama) (4.12.1)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx>=0.27->ollama) (2026.1.4)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx>=0.27->ollama) (1.0.9)\n",
      "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx>=0.27->ollama) (3.11)\n",
      "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx>=0.27->ollama) (0.16.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.20.3)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.3.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (1.2.0)\n",
      "Requirement already satisfied: shellingham in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (1.5.4)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core>=1.2.8) (3.0.0)\n",
      "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.19.0->chromadb) (25.4.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.19.0->chromadb) (2025.9.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.19.0->chromadb) (0.37.0)\n",
      "Requirement already satisfied: rpds-py>=0.25.0 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.19.0->chromadb) (0.30.0)\n",
      "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (1.17.0)\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (2.9.0.post0)\n",
      "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (1.9.0)\n",
      "Requirement already satisfied: requests-oauthlib in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (2.0.0)\n",
      "Requirement already satisfied: urllib3!=2.6.0,>=1.24.2 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (2.5.0)\n",
      "Collecting durationpy>=0.7 (from kubernetes>=28.1.0->chromadb)\n",
      "  Downloading durationpy-0.10-py3-none-any.whl.metadata (340 bytes)\n",
      "Requirement already satisfied: ormsgpack>=1.12.0 in /usr/local/lib/python3.12/dist-packages (from langgraph-checkpoint<5.0.0,>=2.1.0->langgraph>=1.0.7) (1.12.2)\n",
      "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core>=1.2.8) (1.0.0)\n",
      "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core>=1.2.8) (0.25.0)\n",
      "Requirement already satisfied: click>=8.1.7 in /usr/local/lib/python3.12/dist-packages (from magika~=0.6.1->markitdown[all]) (8.3.1)\n",
      "Requirement already satisfied: python-dotenv>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from magika~=0.6.1->markitdown[all]) (1.2.1)\n",
      "Collecting cobble<0.2,>=0.1.3 (from mammoth~=1.11.0->markitdown[all])\n",
      "  Downloading cobble-0.1.4-py3-none-any.whl.metadata (2.7 kB)\n",
      "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.12/dist-packages (from onnxruntime>=1.14.1->chromadb) (25.12.19)\n",
      "Requirement already satisfied: protobuf in /usr/local/lib/python3.12/dist-packages (from onnxruntime>=1.14.1->chromadb) (5.29.6)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.12/dist-packages (from onnxruntime>=1.14.1->chromadb) (1.14.0)\n",
      "Requirement already satisfied: importlib-metadata<8.8.0,>=6.0 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-api>=1.2.0->chromadb) (8.7.1)\n",
      "Requirement already satisfied: googleapis-common-protos~=1.57 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.72.0)\n",
      "Collecting opentelemetry-exporter-otlp-proto-common==1.39.1 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n",
      "  Downloading opentelemetry_exporter_otlp_proto_common-1.39.1-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting opentelemetry-proto==1.39.1 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n",
      "  Downloading opentelemetry_proto-1.39.1-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting opentelemetry-sdk>=1.2.0 (from chromadb)\n",
      "  Downloading opentelemetry_sdk-1.39.1-py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting opentelemetry-api>=1.2.0 (from chromadb)\n",
      "  Downloading opentelemetry_api-1.39.1-py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting opentelemetry-semantic-conventions==0.60b1 (from opentelemetry-sdk>=1.2.0->chromadb)\n",
      "  Downloading opentelemetry_semantic_conventions-0.60b1-py3-none-any.whl.metadata (2.4 kB)\n",
      "Requirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.12/dist-packages (from pdfminer-six>=20251107->markitdown[all]) (43.0.3)\n",
      "Collecting backoff>=1.10.0 (from posthog<6.0.0,>=2.4.0->chromadb)\n",
      "  Downloading backoff-2.2.1-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: distro>=1.5.0 in /usr/local/lib/python3.12/dist-packages (from posthog<6.0.0,>=2.4.0->chromadb) (1.9.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.9->ollama) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.9->ollama) (2.41.4)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.9->ollama) (0.4.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->chromadb) (4.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->chromadb) (2.19.2)\n",
      "Requirement already satisfied: blis<1.4.0,>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.3.3)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.12/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
      "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (3.6.1)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.8.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (11.3.3.83)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (10.3.9.90)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (11.7.3.90)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.5.8.93)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (2.27.5)\n",
      "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (3.3.20)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.8.90)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.1.3)\n",
      "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (3.5.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers<6.0.0,>=4.41.0->sentence-transformers) (2025.11.3)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers<6.0.0,>=4.41.0->sentence-transformers) (0.7.0)\n",
      "Requirement already satisfied: httptools>=0.6.3 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.7.1)\n",
      "Requirement already satisfied: uvloop>=0.15.1 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.22.1)\n",
      "Requirement already satisfied: watchfiles>=0.13 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.1.1)\n",
      "Requirement already satisfied: websockets>=10.4 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (15.0.1)\n",
      "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from weasel<0.5.0,>=0.4.2->spacy) (0.23.0)\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.12/dist-packages (from weasel<0.5.0,>=0.4.2->spacy) (7.5.0)\n",
      "Collecting isodate>=0.6.1 (from azure-ai-documentintelligence->markitdown[all])\n",
      "  Downloading isodate-0.7.2-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting azure-core>=1.30.0 (from azure-ai-documentintelligence->markitdown[all])\n",
      "  Downloading azure_core-1.38.1-py3-none-any.whl.metadata (47 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.9/47.9 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting msal>=1.31.0 (from azure-identity->markitdown[all])\n",
      "  Downloading msal-1.34.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting msal-extensions>=1.2.0 (from azure-identity->markitdown[all])\n",
      "  Downloading msal_extensions-1.3.1-py3-none-any.whl.metadata (7.8 kB)\n",
      "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4->markitdown[all]) (2.8.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->spacy) (3.0.3)\n",
      "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.12/dist-packages (from openpyxl->markitdown[all]) (2.0.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->markitdown[all]) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->markitdown[all]) (2025.3)\n",
      "Requirement already satisfied: Pillow>=3.3.2 in /usr/local/lib/python3.12/dist-packages (from python-pptx->markitdown[all]) (11.3.0)\n",
      "Collecting XlsxWriter>=0.5.7 (from python-pptx->markitdown[all])\n",
      "  Downloading xlsxwriter-3.2.9-py3-none-any.whl.metadata (2.7 kB)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->sentence-transformers) (1.5.3)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
      "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.12/dist-packages (from cryptography>=36.0.0->pdfminer-six>=20251107->markitdown[all]) (2.0.0)\n",
      "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.12/dist-packages (from importlib-metadata<8.8.0,>=6.0->opentelemetry-api>=1.2.0->chromadb) (3.23.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb) (0.1.2)\n",
      "Requirement already satisfied: PyJWT<3,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from PyJWT[crypto]<3,>=1.0.0->msal>=1.31.0->azure-identity->markitdown[all]) (2.11.0)\n",
      "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.4.2->spacy) (2.1.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.12/dist-packages (from requests-oauthlib->kubernetes>=28.1.0->chromadb) (3.3.1)\n",
      "Requirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer-six>=20251107->markitdown[all]) (3.0)\n",
      "Downloading ollama-0.6.1-py3-none-any.whl (14 kB)\n",
      "Downloading langchain_ollama-1.0.1-py3-none-any.whl (29 kB)\n",
      "Downloading langchain-1.2.10-py3-none-any.whl (111 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m111.7/111.7 kB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading langchain_core-1.2.11-py3-none-any.whl (500 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m500.1/500.1 kB\u001b[0m \u001b[31m34.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading langgraph-1.0.8-py3-none-any.whl (158 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m158.1/158.1 kB\u001b[0m \u001b[31m22.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading chromadb-1.5.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (21.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.4/21.4 MB\u001b[0m \u001b[31m121.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading langchain_text_splitters-1.1.0-py3-none-any.whl (34 kB)\n",
      "Downloading ginza-5.2.0-py3-none-any.whl (21 kB)\n",
      "Downloading ja_ginza-5.2.0-py3-none-any.whl (59.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.1/59.1 MB\u001b[0m \u001b[31m39.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading rank_bm25-0.2.2-py3-none-any.whl (8.6 kB)\n",
      "Downloading bcrypt-5.0.0-cp39-abi3-manylinux_2_34_x86_64.whl (278 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m278.2/278.2 kB\u001b[0m \u001b[31m34.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading build-1.4.0-py3-none-any.whl (24 kB)\n",
      "Downloading kubernetes-35.0.0-py2.py3-none-any.whl (2.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m102.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading magika-0.6.3-py3-none-manylinux_2_28_x86_64.whl (15.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.4/15.4 MB\u001b[0m \u001b[31m140.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hDownloading mammoth-1.11.0-py2.py3-none-any.whl (54 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.8/54.8 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading onnxruntime-1.24.1-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (17.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.1/17.1 MB\u001b[0m \u001b[31m131.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading opentelemetry_exporter_otlp_proto_grpc-1.39.1-py3-none-any.whl (19 kB)\n",
      "Downloading opentelemetry_exporter_otlp_proto_common-1.39.1-py3-none-any.whl (18 kB)\n",
      "Downloading opentelemetry_proto-1.39.1-py3-none-any.whl (72 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.5/72.5 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading opentelemetry_sdk-1.39.1-py3-none-any.whl (132 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.6/132.6 kB\u001b[0m \u001b[31m18.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading opentelemetry_api-1.39.1-py3-none-any.whl (66 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.4/66.4 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading opentelemetry_semantic_conventions-0.60b1-py3-none-any.whl (219 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m220.0/220.0 kB\u001b[0m \u001b[31m29.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pdfminer_six-20260107-py3-none-any.whl (6.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m149.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading plac-1.4.5-py2.py3-none-any.whl (22 kB)\n",
      "Downloading posthog-5.4.0-py3-none-any.whl (105 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.4/105.4 kB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pybase64-1.4.3-cp312-cp312-manylinux1_x86_64.manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_5_x86_64.whl (71 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.6/71.6 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pypika-0.51.1-py2.py3-none-any.whl (60 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.6/60.6 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading sudachidict_core-20260116-py3-none-any.whl (72.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.2/72.2 MB\u001b[0m \u001b[31m31.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading SudachiPy-0.6.10-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m101.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading youtube_transcript_api-1.0.3-py3-none-any.whl (2.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m113.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading azure_ai_documentintelligence-1.0.2-py3-none-any.whl (106 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.0/106.0 kB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading azure_identity-1.25.2-py3-none-any.whl (191 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m191.4/191.4 kB\u001b[0m \u001b[31m24.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading markdownify-1.2.2-py3-none-any.whl (15 kB)\n",
      "Downloading markitdown-0.1.4-py3-none-any.whl (58 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading olefile-0.47-py2.py3-none-any.whl (114 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.6/114.6 kB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading python_pptx-1.0.2-py3-none-any.whl (472 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m472.8/472.8 kB\u001b[0m \u001b[31m49.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading speechrecognition-3.14.5-py3-none-any.whl (32.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m32.9/32.9 MB\u001b[0m \u001b[31m76.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading azure_core-1.38.1-py3-none-any.whl (217 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m217.9/217.9 kB\u001b[0m \u001b[31m30.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
      "Downloading cobble-0.1.4-py3-none-any.whl (4.0 kB)\n",
      "Downloading durationpy-0.10-py3-none-any.whl (3.9 kB)\n",
      "Downloading isodate-0.7.2-py3-none-any.whl (22 kB)\n",
      "Downloading msal-1.34.0-py3-none-any.whl (116 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.0/117.0 kB\u001b[0m \u001b[31m18.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading msal_extensions-1.3.1-py3-none-any.whl (20 kB)\n",
      "Downloading xlsxwriter-3.2.9-py3-none-any.whl (175 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m175.3/175.3 kB\u001b[0m \u001b[31m24.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pyproject_hooks-1.2.0-py3-none-any.whl (10 kB)\n",
      "Installing collected packages: SudachiPy, pypika, plac, durationpy, XlsxWriter, SudachiDict-core, speechrecognition, rank-bm25, pyproject_hooks, pybase64, opentelemetry-proto, olefile, isodate, cobble, bcrypt, backoff, youtube-transcript-api, python-pptx, posthog, opentelemetry-exporter-otlp-proto-common, opentelemetry-api, onnxruntime, markdownify, mammoth, build, azure-core, pdfminer-six, opentelemetry-semantic-conventions, ollama, magika, kubernetes, azure-ai-documentintelligence, opentelemetry-sdk, msal, markitdown, langchain-core, opentelemetry-exporter-otlp-proto-grpc, msal-extensions, langchain-text-splitters, langchain-ollama, ginza, chromadb, azure-identity, langgraph, ja-ginza, langchain\n",
      "  Attempting uninstall: opentelemetry-proto\n",
      "    Found existing installation: opentelemetry-proto 1.38.0\n",
      "    Uninstalling opentelemetry-proto-1.38.0:\n",
      "      Successfully uninstalled opentelemetry-proto-1.38.0\n",
      "  Attempting uninstall: opentelemetry-exporter-otlp-proto-common\n",
      "    Found existing installation: opentelemetry-exporter-otlp-proto-common 1.38.0\n",
      "    Uninstalling opentelemetry-exporter-otlp-proto-common-1.38.0:\n",
      "      Successfully uninstalled opentelemetry-exporter-otlp-proto-common-1.38.0\n",
      "  Attempting uninstall: opentelemetry-api\n",
      "    Found existing installation: opentelemetry-api 1.38.0\n",
      "    Uninstalling opentelemetry-api-1.38.0:\n",
      "      Successfully uninstalled opentelemetry-api-1.38.0\n",
      "  Attempting uninstall: opentelemetry-semantic-conventions\n",
      "    Found existing installation: opentelemetry-semantic-conventions 0.59b0\n",
      "    Uninstalling opentelemetry-semantic-conventions-0.59b0:\n",
      "      Successfully uninstalled opentelemetry-semantic-conventions-0.59b0\n",
      "  Attempting uninstall: opentelemetry-sdk\n",
      "    Found existing installation: opentelemetry-sdk 1.38.0\n",
      "    Uninstalling opentelemetry-sdk-1.38.0:\n",
      "      Successfully uninstalled opentelemetry-sdk-1.38.0\n",
      "  Attempting uninstall: langchain-core\n",
      "    Found existing installation: langchain-core 1.2.9\n",
      "    Uninstalling langchain-core-1.2.9:\n",
      "      Successfully uninstalled langchain-core-1.2.9\n",
      "  Attempting uninstall: langgraph\n",
      "    Found existing installation: langgraph 1.0.7\n",
      "    Uninstalling langgraph-1.0.7:\n",
      "      Successfully uninstalled langgraph-1.0.7\n",
      "  Attempting uninstall: langchain\n",
      "    Found existing installation: langchain 1.2.8\n",
      "    Uninstalling langchain-1.2.8:\n",
      "      Successfully uninstalled langchain-1.2.8\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "opentelemetry-exporter-gcp-logging 1.11.0a0 requires opentelemetry-sdk<1.39.0,>=1.35.0, but you have opentelemetry-sdk 1.39.1 which is incompatible.\n",
      "opentelemetry-exporter-otlp-proto-http 1.38.0 requires opentelemetry-exporter-otlp-proto-common==1.38.0, but you have opentelemetry-exporter-otlp-proto-common 1.39.1 which is incompatible.\n",
      "opentelemetry-exporter-otlp-proto-http 1.38.0 requires opentelemetry-proto==1.38.0, but you have opentelemetry-proto 1.39.1 which is incompatible.\n",
      "opentelemetry-exporter-otlp-proto-http 1.38.0 requires opentelemetry-sdk~=1.38.0, but you have opentelemetry-sdk 1.39.1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed SudachiDict-core-20260116 SudachiPy-0.6.10 XlsxWriter-3.2.9 azure-ai-documentintelligence-1.0.2 azure-core-1.38.1 azure-identity-1.25.2 backoff-2.2.1 bcrypt-5.0.0 build-1.4.0 chromadb-1.5.0 cobble-0.1.4 durationpy-0.10 ginza-5.2.0 isodate-0.7.2 ja-ginza-5.2.0 kubernetes-35.0.0 langchain-1.2.10 langchain-core-1.2.11 langchain-ollama-1.0.1 langchain-text-splitters-1.1.0 langgraph-1.0.8 magika-0.6.3 mammoth-1.11.0 markdownify-1.2.2 markitdown-0.1.4 msal-1.34.0 msal-extensions-1.3.1 olefile-0.47 ollama-0.6.1 onnxruntime-1.24.1 opentelemetry-api-1.39.1 opentelemetry-exporter-otlp-proto-common-1.39.1 opentelemetry-exporter-otlp-proto-grpc-1.39.1 opentelemetry-proto-1.39.1 opentelemetry-sdk-1.39.1 opentelemetry-semantic-conventions-0.60b1 pdfminer-six-20260107 plac-1.4.5 posthog-5.4.0 pybase64-1.4.3 pypika-0.51.1 pyproject_hooks-1.2.0 python-pptx-1.0.2 rank-bm25-0.2.2 speechrecognition-3.14.5 youtube-transcript-api-1.0.3\n"
     ]
    }
   ],
   "source": [
    "# Google Colab に必要なライブラリをインストールする。\n",
    "# 1行にまとめることで pip が全パッケージの依存関係を一括解決する。\n",
    "# NOTE: Colab では uv ではなく pip を使う。uv は依存解決の過程で\n",
    "#       numpy 等をアップグレードし、プリインストール済みの scipy 等を壊すため。\n",
    "# NOTE: langchain 関連は 1.x 系に明示的に指定する。\n",
    "#       Colab プリインストールの 0.3.x が残ると langchain-mcp-adapters が動作しない。\n",
    "%pip install -U ollama langchain-ollama \\\n",
    "     \"langchain>=1.2.8\" \"langchain-core>=1.2.8\" \\\n",
    "     \"langgraph>=1.0.7\" \\\n",
    "     \"markitdown[all]\" chromadb \\\n",
    "     \"langchain-text-splitters>=0.3\" \\\n",
    "     spacy ginza ja-ginza \\\n",
    "     rank-bm25 sentence-transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Google Colab に Ollama をセットアップ**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selecting previously unselected package zstd.\n",
      "(Reading database ... 121852 files and directories currently installed.)\n",
      "Preparing to unpack .../zstd_1.4.8+dfsg-3build1_amd64.deb ...\n",
      "Unpacking zstd (1.4.8+dfsg-3build1) ...\n",
      "Setting up zstd (1.4.8+dfsg-3build1) ...\n",
      "Processing triggers for man-db (2.10.2-1) ...\n",
      ">>> Installing ollama to /usr/local\n",
      ">>> Downloading ollama-linux-amd64.tar.zst\n",
      "######################################################################## 100.0%\n",
      ">>> Creating ollama user...\n",
      ">>> Adding ollama user to video group...\n",
      ">>> Adding current user to ollama group...\n",
      ">>> Creating ollama systemd service...\n",
      "\u001b[1m\u001b[31mWARNING:\u001b[m systemd is not running\n",
      "\u001b[1m\u001b[31mWARNING:\u001b[m Unable to detect NVIDIA/AMD GPU. Install lspci or lshw to automatically detect and install GPU dependencies.\n",
      ">>> The Ollama API is now available at 127.0.0.1:11434.\n",
      ">>> Install complete. Run \"ollama\" from the command line.\n",
      "success                                                     \n",
      "gpt-oss:20b: Done!\n",
      "  Model\n",
      "    architecture        gptoss    \n",
      "    parameters          20.9B     \n",
      "    context length      131072    \n",
      "    embedding length    2880      \n",
      "    quantization        MXFP4     \n",
      "\n",
      "  Capabilities\n",
      "    completion    \n",
      "    tools         \n",
      "    thinking      \n",
      "\n",
      "  Parameters\n",
      "    temperature    1    \n",
      "\n",
      "  License\n",
      "    Apache License               \n",
      "    Version 2.0, January 2004    \n",
      "    ...                          \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Ollama のインストール・起動・モデルのダウンロード\n",
    "# 詳細は 01_connect_oss_llm.ipynb を参照\n",
    "import subprocess\n",
    "import time\n",
    "import ollama  # type: ignore\n",
    "\n",
    "!apt-get install -y -qq zstd\n",
    "!curl -fsSL https://ollama.com/install.sh | sh\n",
    "\n",
    "process = subprocess.Popen(\n",
    "    [\"ollama\", \"serve\"],\n",
    "    stdout=subprocess.DEVNULL,\n",
    "    stderr=subprocess.DEVNULL,\n",
    ")\n",
    "time.sleep(5)\n",
    "\n",
    "\n",
    "def ollama_pull(model: str) -> None:\n",
    "    \"\"\"Ollama モデルをダウンロードし、進捗をインライン表示する。\"\"\"\n",
    "    for progress in ollama.pull(model, stream=True):\n",
    "        status = progress.get(\"status\", \"\")\n",
    "        total = progress.get(\"total\") or 0\n",
    "        completed = progress.get(\"completed\") or 0\n",
    "        if total:\n",
    "            line = f\"{status}: {completed / total:.0%}\"\n",
    "        else:\n",
    "            line = status\n",
    "        print(f\"\\r{line:<60}\", end=\"\", flush=True)\n",
    "    print(f\"\\n{model}: Done!\")\n",
    "\n",
    "\n",
    "model_name = \"gpt-oss:20b\"\n",
    "ollama_pull(model_name)\n",
    "!ollama show {model_name}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ChatOllama で LLM に接続**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ChatOllama で LLM に接続する。\n",
    "from langchain_ollama import ChatOllama  # type: ignore\n",
    "\n",
    "llm = ChatOllama(\n",
    "    model=\"gpt-oss:20b\",\n",
    "    num_ctx=16384,\n",
    "    num_predict=-1,\n",
    "    temperature=0.8,\n",
    "    top_k=40,\n",
    "    top_p=0.9,\n",
    "    repeat_penalty=1.1,\n",
    "    reasoning=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Embedding モデル（ruri-v3-310m）と Reranker モデルのセットアップ**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:104: UserWarning: \n",
      "Error while fetching `HF_TOKEN` secret value from your vault: 'Requesting secret HF_TOKEN timed out. Secrets can only be fetched when running from the Colab UI.'.\n",
      "You are not authenticated with the Hugging Face Hub in this notebook.\n",
      "If the error persists, please let us know by opening an issue on GitHub (https://github.com/huggingface/huggingface_hub/issues/new).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3def5541a99843a1b07e712e60b31df8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/229 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7de89818b2654cf08afe55498ddef4b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/205 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "095561dbdb2b482eb3e3e2915f1df13d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7dfef17449f54baea09b3bfe9dd5d620",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/54.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b7de36c979846f297f6463e9aa5bfcc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1ac3cd792dc4c61b50882546a0c8bf3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.26G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f752577bca14b40802ca2a66cbff84a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/152 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5943d71eddd24abab8900de10250ce6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c0ca0f9f4d444d9a6ecf3cefb90e450",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2ba7360c9df48c7943446ce657aa892",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/1.83M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n",
      "WARNING:huggingface_hub.utils._http:Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d7c0d945347479f9da46eef6c7f7558",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/968 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b617f6c9799c46249e5df23e5e3a4214",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/296 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding dim: 768\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c6a3a796f00471c95b44863512b8add",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "851d2450b8d2430faa871f1a432a1144",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.26G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7fa9b315fc8848e4a288a87a6aba77ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/156 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7ad2d9c59f04414a13cbb7d8454b9e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c03718e76aa408fb1603e11a1bfcede",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00d9c55ff19f46b8b6f8b6ec031085fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/1.83M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79ea011d57154c6b9c9b26e91fcbacff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/968 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98633f46299647c5acbf6e00194bcee7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reranker model loaded.\n"
     ]
    }
   ],
   "source": [
    "# Embedding: ruri-v3-310m (Sentence Transformers 経由)\n",
    "from langchain_core.embeddings import Embeddings  # type: ignore\n",
    "from sentence_transformers import SentenceTransformer, CrossEncoder  # type: ignore\n",
    "\n",
    "\n",
    "class RuriEmbeddings(Embeddings):\n",
    "    \"\"\"ruri-v3 を LangChain の Embeddings インターフェースでラップする。\"\"\"\n",
    "\n",
    "    def __init__(self, model_name: str = \"cl-nagoya/ruri-v3-310m\") -> None:\n",
    "        self.model = SentenceTransformer(model_name)\n",
    "\n",
    "    def embed_documents(self, texts: list[str]) -> list[list[float]]:\n",
    "        prefixed = [f\"検索文書: {t}\" for t in texts]\n",
    "        return self.model.encode(prefixed).tolist()\n",
    "\n",
    "    def embed_query(self, text: str) -> list[float]:\n",
    "        return self.model.encode(f\"検索クエリ: {text}\").tolist()\n",
    "\n",
    "\n",
    "embeddings = RuriEmbeddings()\n",
    "test_vec = embeddings.embed_query(\"テスト文です\")\n",
    "print(f\"Embedding dim: {len(test_vec)}\")\n",
    "\n",
    "# Reranker: cl-nagoya/ruri-v3-reranker-310m\n",
    "reranker = CrossEncoder(\"cl-nagoya/ruri-v3-reranker-310m\")\n",
    "print(\"Reranker model loaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**JAXA リポジトリからデータをダウンロード → 前処理 → チャンク分割**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ダウンロード完了: 高速回転ホイール.pdf\n",
      "変換後の文字数: 86888\n",
      "クリーニング後の文字数: 79515\n"
     ]
    }
   ],
   "source": [
    "# JAXA リポジトリから PDF をダウンロードし、MarkItDown で markdown に変換する。\n",
    "import urllib.request\n",
    "import unicodedata\n",
    "import re\n",
    "from pathlib import Path\n",
    "from markitdown import MarkItDown  # type: ignore\n",
    "\n",
    "pdf_url = \"https://jaxa.repo.nii.ac.jp/record/2149/files/63826000.pdf\"\n",
    "pdf_path = Path(\"高速回転ホイール.pdf\")\n",
    "\n",
    "if not pdf_path.exists():\n",
    "    urllib.request.urlretrieve(pdf_url, pdf_path)\n",
    "    print(f\"ダウンロード完了: {pdf_path}\")\n",
    "\n",
    "md = MarkItDown()\n",
    "result = md.convert(str(pdf_path))\n",
    "raw_text = result.text_content\n",
    "print(f\"変換後の文字数: {len(raw_text)}\")\n",
    "\n",
    "# Unicode 正規化 (NFKC)\n",
    "text = unicodedata.normalize(\"NFKC\", raw_text)\n",
    "\n",
    "\n",
    "# PDF 抽出テキストの汎用クリーニング\n",
    "def clean_pdf_text(text: str) -> str:\n",
    "    \"\"\"1文字行ブロックの除去 + 空行圧縮。\"\"\"\n",
    "    text = re.sub(\n",
    "        r\"(^[^\\S\\n]*\\S[^\\S\\n]*$\\n?){3,}\",\n",
    "        \"\\n\",\n",
    "        text,\n",
    "        flags=re.MULTILINE,\n",
    "    )\n",
    "    text = re.sub(r\"\\n{3,}\", \"\\n\\n\", text)\n",
    "    return text.strip()\n",
    "\n",
    "\n",
    "text = clean_pdf_text(text)\n",
    "print(f\"クリーニング後の文字数: {len(text)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "チャンク数: 76\n"
     ]
    }
   ],
   "source": [
    "# SpacyTextSplitter でチャンク分割する。\n",
    "# Sudachi の入力上限 (49,149 bytes) を超えないように事前分割してから渡す。\n",
    "from langchain_text_splitters import SpacyTextSplitter  # type: ignore\n",
    "\n",
    "CHUNK_SIZE = 500\n",
    "CHUNK_OVERLAP = 100\n",
    "BLOCK_MAX_BYTES = 40_000\n",
    "BLOCK_OVERLAP_CHARS = CHUNK_SIZE\n",
    "\n",
    "\n",
    "def split_into_safe_blocks(\n",
    "    text: str,\n",
    "    max_bytes: int = BLOCK_MAX_BYTES,\n",
    "    overlap_chars: int = BLOCK_OVERLAP_CHARS,\n",
    ") -> list[str]:\n",
    "    \"\"\"テキストを段落区切りで max_bytes 以下のブロックに分割する。\"\"\"\n",
    "    paragraphs = text.split(\"\\n\\n\")\n",
    "    blocks: list[str] = []\n",
    "    current = \"\"\n",
    "    for para in paragraphs:\n",
    "        candidate = current + \"\\n\\n\" + para if current else para\n",
    "        if len(candidate.encode(\"utf-8\")) > max_bytes and current:\n",
    "            blocks.append(current)\n",
    "            current = current[-overlap_chars:] + \"\\n\\n\" + para\n",
    "        else:\n",
    "            current = candidate\n",
    "    if current:\n",
    "        blocks.append(current)\n",
    "    return blocks\n",
    "\n",
    "\n",
    "text_splitter = SpacyTextSplitter(\n",
    "    separator=\"\\n\\n\",\n",
    "    pipeline=\"ja_ginza\",\n",
    "    chunk_size=CHUNK_SIZE,\n",
    "    chunk_overlap=CHUNK_OVERLAP,\n",
    ")\n",
    "\n",
    "blocks = split_into_safe_blocks(text)\n",
    "chunks: list[str] = []\n",
    "for block in blocks:\n",
    "    chunks.extend(text_splitter.split_text(block))\n",
    "\n",
    "print(f\"チャンク数: {len(chunks)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ベクトルデータベースの構築（ChromaDB, インメモリ）**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChromaDB に 76 件のチャンクを格納しました。\n"
     ]
    }
   ],
   "source": [
    "# ChromaDB にチャンクを格納する（インメモリ）。\n",
    "import chromadb  # type: ignore\n",
    "\n",
    "chroma_client = chromadb.Client()\n",
    "\n",
    "collection = chroma_client.create_collection(\n",
    "    name=\"jaxa_wheel\",\n",
    "    metadata={\"hnsw:space\": \"cosine\"},\n",
    ")\n",
    "\n",
    "chunk_embeddings = embeddings.embed_documents(chunks)\n",
    "\n",
    "collection.add(\n",
    "    ids=[f\"chunk_{i}\" for i in range(len(chunks))],\n",
    "    documents=chunks,\n",
    "    embeddings=chunk_embeddings,\n",
    ")\n",
    "\n",
    "print(f\"ChromaDB に {collection.count()} 件のチャンクを格納しました。\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**検索機能の実装（BM25 + セマンティック + ハイブリッド + Reranker）と tool 定義**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BM25 インデックス構築完了: 76 件\n"
     ]
    }
   ],
   "source": [
    "# BM25 用の前処理: spaCy (ja_ginza) で形態素解析してトークン化する。\n",
    "import numpy as np  # type: ignore\n",
    "import spacy  # type: ignore\n",
    "from rank_bm25 import BM25Okapi  # type: ignore\n",
    "\n",
    "nlp = spacy.load(\"ja_ginza\", disable=[\"parser\", \"ner\"])\n",
    "\n",
    "\n",
    "def tokenize(text: str) -> list[str]:\n",
    "    \"\"\"spaCy で形態素解析し、BM25 用のトークンリストを返す。\"\"\"\n",
    "    doc = nlp(text)\n",
    "    tokens = []\n",
    "    include_pos = {\"NOUN\", \"VERB\", \"ADJ\", \"PROPN\", \"NUM\"}\n",
    "    for token in doc:\n",
    "        if token.pos_ not in include_pos:\n",
    "            continue\n",
    "        if token.is_stop:\n",
    "            continue\n",
    "        lemma = token.lemma_\n",
    "        if len(lemma) == 1 and re.match(r\"[ぁ-ん\\u30fc!-/:-@\\[-`{-~]\", lemma):\n",
    "            continue\n",
    "        tokens.append(lemma)\n",
    "    return tokens\n",
    "\n",
    "\n",
    "tokenized_chunks = [tokenize(chunk) for chunk in chunks]\n",
    "bm25 = BM25Okapi(tokenized_chunks)\n",
    "print(f\"BM25 インデックス構築完了: {len(tokenized_chunks)} 件\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 検索関数の定義（BM25, セマンティック, ハイブリッド, Reranker）\n",
    "RETRIEVAL_TOP_K = 20\n",
    "RERANK_TOP_K = 5\n",
    "BM25_WEIGHT = 0.3\n",
    "\n",
    "\n",
    "def search_bm25(query: str, top_k: int = RETRIEVAL_TOP_K) -> list[dict]:\n",
    "    \"\"\"BM25 によるキーワード検索を行う。\"\"\"\n",
    "    tokenized_query = tokenize(query)\n",
    "    scores = bm25.get_scores(tokenized_query)\n",
    "    top_indices = np.argsort(scores)[::-1][:top_k]\n",
    "    return [\n",
    "        {\n",
    "            \"rank\": rank + 1,\n",
    "            \"chunk_id\": int(idx),\n",
    "            \"score\": float(scores[idx]),\n",
    "            \"text\": chunks[idx],\n",
    "        }\n",
    "        for rank, idx in enumerate(top_indices)\n",
    "        if scores[idx] > 0\n",
    "    ]\n",
    "\n",
    "\n",
    "def search_semantic(query: str, top_k: int = RETRIEVAL_TOP_K) -> list[dict]:\n",
    "    \"\"\"Embedding model によるセマンティック検索を行う。\"\"\"\n",
    "    query_embedding = embeddings.embed_query(query)\n",
    "    results = collection.query(query_embeddings=[query_embedding], n_results=top_k)\n",
    "    return [\n",
    "        {\n",
    "            \"rank\": rank + 1,\n",
    "            \"chunk_id\": int(doc_id.split(\"_\")[1]),\n",
    "            \"score\": 1.0 - dist,\n",
    "            \"text\": doc,\n",
    "        }\n",
    "        for rank, (doc_id, doc, dist) in enumerate(\n",
    "            zip(results[\"ids\"][0], results[\"documents\"][0], results[\"distances\"][0])\n",
    "        )\n",
    "    ]\n",
    "\n",
    "\n",
    "def search_hybrid(\n",
    "    query: str, top_k: int = RETRIEVAL_TOP_K, bm25_weight: float = BM25_WEIGHT\n",
    ") -> list[dict]:\n",
    "    \"\"\"BM25 とセマンティック検索の RRF ハイブリッド検索を行う。\"\"\"\n",
    "    k = 60\n",
    "    bm25_results = search_bm25(query, top_k=top_k)\n",
    "    semantic_results = search_semantic(query, top_k=top_k)\n",
    "\n",
    "    scores: dict[int, float] = {}\n",
    "    texts: dict[int, str] = {}\n",
    "\n",
    "    for r in bm25_results:\n",
    "        cid = r[\"chunk_id\"]\n",
    "        scores[cid] = scores.get(cid, 0) + bm25_weight / (k + r[\"rank\"])\n",
    "        texts[cid] = r[\"text\"]\n",
    "\n",
    "    semantic_weight = 1.0 - bm25_weight\n",
    "    for r in semantic_results:\n",
    "        cid = r[\"chunk_id\"]\n",
    "        scores[cid] = scores.get(cid, 0) + semantic_weight / (k + r[\"rank\"])\n",
    "        texts[cid] = r[\"text\"]\n",
    "\n",
    "    sorted_ids = sorted(scores, key=lambda cid: scores[cid], reverse=True)[:top_k]\n",
    "    return [\n",
    "        {\"rank\": rank + 1, \"chunk_id\": cid, \"score\": scores[cid], \"text\": texts[cid]}\n",
    "        for rank, cid in enumerate(sorted_ids)\n",
    "    ]\n",
    "\n",
    "\n",
    "def rerank(query: str, results: list[dict], top_k: int = RERANK_TOP_K) -> list[dict]:\n",
    "    \"\"\"Reranker (CrossEncoder) で検索結果を再順位付けする。\"\"\"\n",
    "    if not results:\n",
    "        return []\n",
    "    pairs = [(query, r[\"text\"]) for r in results]\n",
    "    scores = reranker.predict(pairs)\n",
    "    ranked_indices = np.argsort(scores)[::-1][:top_k]\n",
    "    return [\n",
    "        {\n",
    "            \"rank\": rank + 1,\n",
    "            \"chunk_id\": results[idx][\"chunk_id\"],\n",
    "            \"score\": float(scores[idx]),\n",
    "            \"text\": results[idx][\"text\"],\n",
    "        }\n",
    "        for rank, idx in enumerate(ranked_indices)\n",
    "    ]\n",
    "\n",
    "\n",
    "print(\"検索関数を定義しました: search_bm25, search_semantic, search_hybrid, rerank\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 検索機能を LLM の tool として定義する。\n",
    "from langchain_core.tools import tool  # type: ignore\n",
    "\n",
    "MAX_RETURN_CHARS = 8000\n",
    "\n",
    "\n",
    "@tool\n",
    "def search_document(query: str) -> str:\n",
    "    \"\"\"外部ナレッジベースから、クエリに関連する情報を検索・取得します。\n",
    "    ユーザーの質問に対し、具体的な事実、データ、あるいは詳細な文脈が必要な場合、\n",
    "    自身の知識だけで回答せずに必ずこのツールを使用してください。\n",
    "\n",
    "    Args:\n",
    "        query: 検索したい内容を表す、具体的かつ完全な文章（日本語）。\n",
    "    \"\"\"\n",
    "    try:\n",
    "        hybrid_results = search_hybrid(query)\n",
    "        reranked = rerank(query, hybrid_results)\n",
    "    except Exception as e:\n",
    "        return f\"検索中にエラーが発生しました: {e}\"\n",
    "\n",
    "    if not reranked:\n",
    "        return \"検索結果が見つかりませんでした。\"\n",
    "\n",
    "    passages = []\n",
    "    total_chars = 0\n",
    "    for r in reranked:\n",
    "        passage = f\"[チャンク {r['chunk_id']}] (スコア: {r['score']:.4f})\\n{r['text']}\"\n",
    "        total_chars += len(passage)\n",
    "        if total_chars > MAX_RETURN_CHARS:\n",
    "            break\n",
    "        passages.append(passage)\n",
    "    return \"\\n\\n---\\n\\n\".join(passages)\n",
    "\n",
    "\n",
    "search_tool = search_document\n",
    "print(f\"RAG Tool: {search_tool.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**LangGraph による Workflow の実装**\n",
    "\n",
    "03_02 の Workflow を RAG 検索用に適応する。web_search ノードを doc_search ノードに置き換え、\n",
    "MCP サーバの代わりに `search_document` ツールで検索する。\n",
    "\n",
    "**Workflow の流れ**\n",
    "1. **task_planning**: ユーザの質問を受け取り、回答に必要なサブタスク（目的＋検索クエリ）を構造化して作成する。\n",
    "2. **doc_search**: 各サブタスクの検索クエリを `search_document` ツールで実行し、目的と紐付けた検索結果を蓄積する。\n",
    "3. **judge**: サブタスクの目的ごとに、検索結果が十分かを LLM が判断する。不足なら追加サブタスクを生成して doc_search に戻る。\n",
    "4. **generate_answer**: 目的ごとに整理された検索結果をもとに、ユーザの質問に対する最終回答を生成する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Workflow の状態定義・共通ユーティリティ・システムプロンプト\n",
    "import json\n",
    "from typing import TypedDict\n",
    "from langchain_core.messages import HumanMessage, SystemMessage  # type: ignore\n",
    "from langgraph.graph import StateGraph, START, END  # type: ignore\n",
    "from IPython.display import Image, display\n",
    "\n",
    "# --- グローバル設定 ---\n",
    "MAX_LOOP_COUNT = 2  # judge → doc_search 再調査ループの上限回数\n",
    "\n",
    "\n",
    "# --- Workflow の状態 ---\n",
    "class WorkflowState(TypedDict):\n",
    "    question: str\n",
    "    subtasks: list[dict]  # [{\"purpose\": str, \"queries\": [str]}]\n",
    "    search_results: list[str]  # 目的と紐付けた検索結果\n",
    "    answer: str\n",
    "    loop_count: int\n",
    "\n",
    "\n",
    "# --- 共通ユーティリティ: LLM 出力から JSON を抽出 ---\n",
    "def extract_json_text(raw: str) -> str:\n",
    "    \"\"\"LLM の出力から JSON 文字列を抽出する。\"\"\"\n",
    "    text = raw.strip()\n",
    "\n",
    "    code_block_match = re.search(r\"```(?:json)?\\s*(.*?)```\", text, re.DOTALL)\n",
    "    if code_block_match:\n",
    "        text = code_block_match.group(1).strip()\n",
    "\n",
    "    try:\n",
    "        json.loads(text)\n",
    "        return text\n",
    "    except json.JSONDecodeError:\n",
    "        pass\n",
    "\n",
    "    match_start = re.search(r\"[\\{\\[]\", text)\n",
    "    if not match_start:\n",
    "        return text\n",
    "\n",
    "    first_brace_index = match_start.start()\n",
    "    start_char = text[first_brace_index]\n",
    "    end_char = \"]\" if start_char == \"[\" else \"}\"\n",
    "\n",
    "    last_brace_index = text.rfind(end_char)\n",
    "    if last_brace_index > first_brace_index:\n",
    "        return text[first_brace_index : last_brace_index + 1]\n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "# --- 各ノードのシステムプロンプト ---\n",
    "SYSTEM_PROMPT_TASK_PLANNING = \"\"\"\\\n",
    "あなたはリサーチプランナーです。\n",
    "ユーザの質問に回答するために、ナレッジベース（技術文書）を検索するためのサブタスクを作成してください。\n",
    "\n",
    "出力は以下の JSON 配列のみとし、他のテキストは一切含めないでください。\n",
    "サブタスクは最大3個までとしてください。\n",
    "\n",
    "出力形式:\n",
    "[\n",
    "  {\"purpose\": \"このサブタスクで明らかにしたいこと\",\n",
    "   \"queries\": [\"検索クエリ1\", \"検索クエリ2\"]},\n",
    "  ...\n",
    "]\n",
    "\n",
    "purpose は判定ステップで「この目的に十分な情報が得られたか」を評価する基準になります。\n",
    "具体的かつ明確に書いてください。\n",
    "検索クエリは、技術文書から関連情報を検索するための日本語の具体的なフレーズにしてください。\n",
    "\"\"\"\n",
    "\n",
    "SYSTEM_PROMPT_JUDGE = \"\"\"\\\n",
    "あなたはリサーチの品質を判定する審査員です。\n",
    "ユーザの質問と検索結果を見て、回答に十分な情報があるか判断してください。\n",
    "検索結果には【目的: ...】タグが付いています。\n",
    "各目的について十分な情報が得られているかを確認してください。\n",
    "\n",
    "十分な場合:\n",
    "{\"sufficient\": true, \"reason\": \"判断理由を日本語で1文で\"}\n",
    "\n",
    "不足の場合（不足している目的について追加サブタスクを生成）:\n",
    "{\"sufficient\": false, \"reason\": \"何が不足しているかを日本語で1文で\",\n",
    " \"additional_subtasks\": [\n",
    "    {\"purpose\": \"追加で明らかにしたいこと\",\n",
    "     \"queries\": [\"追加クエリ1\"]}\n",
    "  ]\n",
    "}\n",
    "\n",
    "JSON のみ出力し、他のテキストは含めないでください。\n",
    "\"\"\"\n",
    "\n",
    "SYSTEM_PROMPT_GENERATE_ANSWER = \"\"\"\\\n",
    "あなたはリサーチ結果をもとに回答するAIアシスタントです。\n",
    "検索結果を参考に、ユーザの質問に日本語で丁寧に回答してください。\n",
    "回答は必ず検索結果に基づいて作成し、検索結果に含まれない情報は含めないでください。\n",
    "回答の最後に、以下の形式で結論をまとめてください。\n",
    "\n",
    "# 結論\n",
    "- ユーザの質問: （質問内容）\n",
    "- 回答: （簡潔な回答）\n",
    "\"\"\"\n",
    "\n",
    "print(\"Workflow の状態定義・ユーティリティ・システムプロンプトを定義しました。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Workflow ノードの定義\n",
    "\n",
    "# ノード 1: task_planning（タスク分割）\n",
    "async def task_planning(state: WorkflowState) -> dict:\n",
    "    \"\"\"ユーザの質問を分析し、サブタスク（目的＋検索クエリ）を作成する。\"\"\"\n",
    "    question = state[\"question\"]\n",
    "\n",
    "    response = await llm.ainvoke(\n",
    "        [\n",
    "            SystemMessage(content=SYSTEM_PROMPT_TASK_PLANNING),\n",
    "            HumanMessage(content=question),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    text = extract_json_text(response.content)\n",
    "\n",
    "    try:\n",
    "        subtasks = json.loads(text)\n",
    "    except json.JSONDecodeError:\n",
    "        print(f\"[task_planning] JSON パース失敗 → フォールバック: {text[:100]}\")\n",
    "        subtasks = [{\"purpose\": \"基本調査\", \"queries\": [question]}]\n",
    "\n",
    "    print(f\"[task_planning] サブタスク数: {len(subtasks)}\")\n",
    "    for i, st in enumerate(subtasks):\n",
    "        print(f\"  {i + 1}. 目的: {st['purpose']}\")\n",
    "        print(f\"     クエリ: {st['queries']}\")\n",
    "    return {\"subtasks\": subtasks, \"search_results\": [], \"loop_count\": 0}\n",
    "\n",
    "\n",
    "# ノード 2: doc_search（ドキュメント検索）\n",
    "async def doc_search(state: WorkflowState) -> dict:\n",
    "    \"\"\"各サブタスクの検索クエリを search_document ツールで実行し、結果を蓄積する。\"\"\"\n",
    "    subtasks = state[\"subtasks\"]\n",
    "    results = list(state.get(\"search_results\") or [])\n",
    "\n",
    "    for st in subtasks:\n",
    "        purpose = st[\"purpose\"]\n",
    "        print(f\"[doc_search] 目的: {purpose}\")\n",
    "        for query in st[\"queries\"]:\n",
    "            print(f\"  検索中: {query}\")\n",
    "            try:\n",
    "                result = search_tool.invoke({\"query\": query})\n",
    "            except Exception as e:\n",
    "                print(f\"  [ERROR] クエリ失敗: {query} → {e}\")\n",
    "                continue\n",
    "            if not result or result == \"検索結果が見つかりませんでした。\":\n",
    "                print(f\"  [SKIP] 検索結果なし: {query}\")\n",
    "                continue\n",
    "            results.append(f\"【目的: {purpose}】\\n【クエリ: {query}】\\n{result}\")\n",
    "\n",
    "    return {\"search_results\": results, \"subtasks\": []}\n",
    "\n",
    "\n",
    "# ノード 3: judge（判定）\n",
    "async def judge(state: WorkflowState) -> dict:\n",
    "    \"\"\"検索結果が十分かを判断し、不足なら追加サブタスクを生成する。\"\"\"\n",
    "    question = state[\"question\"]\n",
    "    results = state[\"search_results\"]\n",
    "    loop_count = state.get(\"loop_count\", 0)\n",
    "\n",
    "    if loop_count >= MAX_LOOP_COUNT:\n",
    "        print(\"[judge] ループ上限に到達 → 回答作成へ\")\n",
    "        return {\"subtasks\": [], \"loop_count\": loop_count}\n",
    "\n",
    "    results_text = \"\\n\\n\".join(results)\n",
    "\n",
    "    response = await llm.ainvoke(\n",
    "        [\n",
    "            SystemMessage(content=SYSTEM_PROMPT_JUDGE),\n",
    "            HumanMessage(content=f\"質問: {question}\\n\\n検索結果:\\n{results_text}\"),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    text = extract_json_text(response.content)\n",
    "\n",
    "    try:\n",
    "        judgment = json.loads(text)\n",
    "    except json.JSONDecodeError:\n",
    "        print(f\"[judge] JSON パース失敗 → 回答作成へ: {text[:100]}\")\n",
    "        return {\"subtasks\": [], \"loop_count\": loop_count + 1}\n",
    "\n",
    "    reason = judgment.get(\"reason\", \"\")\n",
    "\n",
    "    if judgment.get(\"sufficient\", True):\n",
    "        print(f\"[judge] 情報十分 → 回答作成へ（理由: {reason}）\")\n",
    "        return {\"subtasks\": [], \"loop_count\": loop_count + 1}\n",
    "    else:\n",
    "        additional = judgment.get(\"additional_subtasks\", [])\n",
    "        print(f\"[judge] 情報不足（理由: {reason}）→ 追加サブタスク:\")\n",
    "        for i, st in enumerate(additional):\n",
    "            print(f\"  {i + 1}. 目的: {st.get('purpose', '?')}\")\n",
    "            print(f\"     クエリ: {st.get('queries', [])}\")\n",
    "        return {\"subtasks\": additional, \"loop_count\": loop_count + 1}\n",
    "\n",
    "\n",
    "# ルーター: judge の結果で分岐\n",
    "def should_continue_search(state: WorkflowState) -> str:\n",
    "    \"\"\"追加サブタスクがあれば doc_search に戻り、なければ回答生成へ。\"\"\"\n",
    "    if state.get(\"subtasks\"):\n",
    "        return \"doc_search\"\n",
    "    return \"generate_answer\"\n",
    "\n",
    "\n",
    "# ノード 4: generate_answer（回答生成）\n",
    "async def generate_answer(state: WorkflowState) -> dict:\n",
    "    \"\"\"蓄積した検索結果をもとに最終回答を生成する。\"\"\"\n",
    "    question = state[\"question\"]\n",
    "    results_text = \"\\n\\n\".join(state[\"search_results\"])\n",
    "\n",
    "    response = await llm.ainvoke(\n",
    "        [\n",
    "            SystemMessage(content=SYSTEM_PROMPT_GENERATE_ANSWER),\n",
    "            HumanMessage(content=f\"質問: {question}\\n\\n検索結果:\\n{results_text}\"),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    answer = response.content or \"\"\n",
    "\n",
    "    if not answer:\n",
    "        print(\"[generate_answer] WARNING: response.content が空です\")\n",
    "        print(f\"  response type: {type(response)}\")\n",
    "        print(f\"  response repr: {repr(response)[:500]}\")\n",
    "\n",
    "    print(\"[generate_answer] 回答生成完了\")\n",
    "    return {\"answer\": answer}\n",
    "\n",
    "\n",
    "print(\n",
    "    \"Workflow ノードを定義しました: task_planning, doc_search, judge, generate_answer\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Workflow グラフの構築とコンパイル\n",
    "workflow = StateGraph(WorkflowState)\n",
    "\n",
    "# ノードの登録\n",
    "workflow.add_node(\"task_planning\", task_planning)\n",
    "workflow.add_node(\"doc_search\", doc_search)\n",
    "workflow.add_node(\"judge\", judge)\n",
    "workflow.add_node(\"generate_answer\", generate_answer)\n",
    "\n",
    "# エッジの定義\n",
    "workflow.add_edge(START, \"task_planning\")\n",
    "workflow.add_edge(\"task_planning\", \"doc_search\")\n",
    "workflow.add_edge(\"doc_search\", \"judge\")\n",
    "\n",
    "# 条件分岐: judge → doc_search（再調査） or generate_answer（回答生成）\n",
    "workflow.add_conditional_edges(\n",
    "    \"judge\",\n",
    "    should_continue_search,\n",
    "    {\n",
    "        \"doc_search\": \"doc_search\",\n",
    "        \"generate_answer\": \"generate_answer\",\n",
    "    },\n",
    ")\n",
    "workflow.add_edge(\"generate_answer\", END)\n",
    "\n",
    "# コンパイル\n",
    "app = workflow.compile()\n",
    "\n",
    "# グラフの可視化\n",
    "display(Image(app.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**動作確認**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Workflow エージェントの動作確認\n",
    "import io\n",
    "import sys\n",
    "from IPython.display import Markdown, HTML, display\n",
    "\n",
    "# 中間ログをキャプチャしつつ、リアルタイムでもセルに出力する\n",
    "log_buffer = io.StringIO()\n",
    "\n",
    "\n",
    "class TeeStream:\n",
    "    \"\"\"stdout への出力を画面表示しつつバッファにも記録する。\"\"\"\n",
    "\n",
    "    def __init__(self, original, buffer):\n",
    "        self.original = original\n",
    "        self.buffer = buffer\n",
    "\n",
    "    def write(self, text):\n",
    "        self.original.write(text)\n",
    "        self.buffer.write(text)\n",
    "\n",
    "    def flush(self):\n",
    "        self.original.flush()\n",
    "\n",
    "\n",
    "_original_stdout = sys.stdout\n",
    "sys.stdout = TeeStream(_original_stdout, log_buffer)\n",
    "try:\n",
    "    result = await app.ainvoke(\n",
    "        {\"question\": \"高速回転ホイールの寿命試験ではどのような結果が得られましたか？\"}\n",
    "    )\n",
    "finally:\n",
    "    sys.stdout = _original_stdout\n",
    "\n",
    "# 中間ログを HTML で全文表示（Colab のセル出力トランケートを回避）\n",
    "log_text = log_buffer.getvalue()\n",
    "print(\"\\n--- 以下は HTML による全文ログ（トランケート回避） ---\")\n",
    "display(HTML(f\"<pre style='white-space:pre-wrap'>{log_text}</pre>\"))\n",
    "\n",
    "# 最終回答の表示\n",
    "print(\"=== Workflow エージェントの実行結果 ===\\n\")\n",
    "answer = result.get(\"answer\", \"\")\n",
    "if answer:\n",
    "    display(Markdown(answer))\n",
    "else:\n",
    "    print(\"[WARNING] 回答が空です。result keys:\", list(result.keys()))\n",
    "    print(\"search_results 件数:\", len(result.get(\"search_results\", [])))\n",
    "    print(\"loop_count:\", result.get(\"loop_count\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
