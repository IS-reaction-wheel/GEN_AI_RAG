{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 03 WebサーチをするAIエージェント\n",
    "#### 03_02 Workflow（Ollama）\n",
    "gpt-oss:20b＋MCPサーバ構成のため、**Colab GPU は L4 を使用すること。**\n",
    "- 必要なライブラリをインストール\n",
    "- Google Colab に Ollama をセットアップ\n",
    "- ChatOllama で LLM に接続\n",
    "- MCPサーバ（ddg-search）によるweb検索の実装\n",
    "- LangGraph による Workflow の実装\n",
    "  1. ユーザの質問を入力。\n",
    "  2. ユーザの質問に回答するためのタスク分割, 作成。\n",
    "  3. MCPサーバ（ddg-search）によるweb検索。\n",
    "  4. web検索を終えて回答作成に進むか判断。再調査なら 3 に戻る。\n",
    "  5. ユーザへの回答の作成と提示。\n",
    "- 動作確認\n",
    "> AIエージェントワークフローで参考にした設計(https://github.com/masamasa59/genai-agent-advanced-book/tree/main/chapter4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**必要なライブラリをインストール**\n",
    "- 1行にまとめることで pip が全パッケージの依存関係を一括解決する。\n",
    "- 分割すると後勝ちで依存関係が壊れるリスクがある。\n",
    "- NOTE: Colab では uv ではなく pip を使う。\n",
    "> uv は依存解決の過程で numpy 等をアップグレードし、プリインストール済みの scipy 等を壊すため。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ollama\n",
      "  Downloading ollama-0.6.1-py3-none-any.whl.metadata (4.3 kB)\n",
      "Collecting langchain-ollama\n",
      "  Downloading langchain_ollama-1.0.1-py3-none-any.whl.metadata (2.5 kB)\n",
      "Requirement already satisfied: langchain>=1.2.8 in /usr/local/lib/python3.12/dist-packages (1.2.8)\n",
      "Collecting langchain>=1.2.8\n",
      "  Downloading langchain-1.2.9-py3-none-any.whl.metadata (5.7 kB)\n",
      "Requirement already satisfied: langchain-core>=1.2.8 in /usr/local/lib/python3.12/dist-packages (1.2.8)\n",
      "Collecting langchain-core>=1.2.8\n",
      "  Downloading langchain_core-1.2.9-py3-none-any.whl.metadata (4.4 kB)\n",
      "Requirement already satisfied: langgraph>=1.0.7 in /usr/local/lib/python3.12/dist-packages (1.0.7)\n",
      "Collecting langgraph>=1.0.7\n",
      "  Downloading langgraph-1.0.8-py3-none-any.whl.metadata (7.4 kB)\n",
      "Collecting langchain-mcp-adapters>=0.2.1\n",
      "  Downloading langchain_mcp_adapters-0.2.1-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting duckduckgo-mcp-server\n",
      "  Downloading duckduckgo_mcp_server-0.1.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: mcp in /usr/local/lib/python3.12/dist-packages (1.26.0)\n",
      "Requirement already satisfied: httpx>=0.27 in /usr/local/lib/python3.12/dist-packages (from ollama) (0.28.1)\n",
      "Requirement already satisfied: pydantic>=2.9 in /usr/local/lib/python3.12/dist-packages (from ollama) (2.12.3)\n",
      "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core>=1.2.8) (1.33)\n",
      "Requirement already satisfied: langsmith<1.0.0,>=0.3.45 in /usr/local/lib/python3.12/dist-packages (from langchain-core>=1.2.8) (0.6.8)\n",
      "Requirement already satisfied: packaging>=23.2.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core>=1.2.8) (26.0)\n",
      "Requirement already satisfied: pyyaml<7.0.0,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core>=1.2.8) (6.0.3)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core>=1.2.8) (9.1.2)\n",
      "Requirement already satisfied: typing-extensions<5.0.0,>=4.7.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core>=1.2.8) (4.15.0)\n",
      "Requirement already satisfied: uuid-utils<1.0,>=0.12.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core>=1.2.8) (0.14.0)\n",
      "Requirement already satisfied: langgraph-checkpoint<5.0.0,>=2.1.0 in /usr/local/lib/python3.12/dist-packages (from langgraph>=1.0.7) (4.0.0)\n",
      "Requirement already satisfied: langgraph-prebuilt<1.1.0,>=1.0.7 in /usr/local/lib/python3.12/dist-packages (from langgraph>=1.0.7) (1.0.7)\n",
      "Requirement already satisfied: langgraph-sdk<0.4.0,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from langgraph>=1.0.7) (0.3.3)\n",
      "Requirement already satisfied: xxhash>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from langgraph>=1.0.7) (3.6.0)\n",
      "Requirement already satisfied: beautifulsoup4>=4.13.3 in /usr/local/lib/python3.12/dist-packages (from duckduckgo-mcp-server) (4.13.5)\n",
      "Requirement already satisfied: anyio>=4.5 in /usr/local/lib/python3.12/dist-packages (from mcp) (4.12.1)\n",
      "Requirement already satisfied: httpx-sse>=0.4 in /usr/local/lib/python3.12/dist-packages (from mcp) (0.4.3)\n",
      "Requirement already satisfied: jsonschema>=4.20.0 in /usr/local/lib/python3.12/dist-packages (from mcp) (4.26.0)\n",
      "Requirement already satisfied: pydantic-settings>=2.5.2 in /usr/local/lib/python3.12/dist-packages (from mcp) (2.12.0)\n",
      "Requirement already satisfied: pyjwt>=2.10.1 in /usr/local/lib/python3.12/dist-packages (from pyjwt[crypto]>=2.10.1->mcp) (2.11.0)\n",
      "Requirement already satisfied: python-multipart>=0.0.9 in /usr/local/lib/python3.12/dist-packages (from mcp) (0.0.22)\n",
      "Requirement already satisfied: sse-starlette>=1.6.1 in /usr/local/lib/python3.12/dist-packages (from mcp) (3.2.0)\n",
      "Requirement already satisfied: starlette>=0.27 in /usr/local/lib/python3.12/dist-packages (from mcp) (0.50.0)\n",
      "Requirement already satisfied: typing-inspection>=0.4.1 in /usr/local/lib/python3.12/dist-packages (from mcp) (0.4.2)\n",
      "Requirement already satisfied: uvicorn>=0.31.1 in /usr/local/lib/python3.12/dist-packages (from mcp) (0.40.0)\n",
      "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio>=4.5->mcp) (3.11)\n",
      "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4>=4.13.3->duckduckgo-mcp-server) (2.8.3)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx>=0.27->ollama) (2026.1.4)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx>=0.27->ollama) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx>=0.27->ollama) (0.16.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core>=1.2.8) (3.0.0)\n",
      "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.20.0->mcp) (25.4.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.20.0->mcp) (2025.9.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.20.0->mcp) (0.37.0)\n",
      "Requirement already satisfied: rpds-py>=0.25.0 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.20.0->mcp) (0.30.0)\n",
      "Requirement already satisfied: ormsgpack>=1.12.0 in /usr/local/lib/python3.12/dist-packages (from langgraph-checkpoint<5.0.0,>=2.1.0->langgraph>=1.0.7) (1.12.2)\n",
      "Requirement already satisfied: orjson>=3.10.1 in /usr/local/lib/python3.12/dist-packages (from langgraph-sdk<0.4.0,>=0.3.0->langgraph>=1.0.7) (3.11.7)\n",
      "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core>=1.2.8) (1.0.0)\n",
      "Requirement already satisfied: requests>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core>=1.2.8) (2.32.4)\n",
      "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core>=1.2.8) (0.25.0)\n",
      "Requirement already satisfied: python-dotenv>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from mcp[cli]>=1.3.0->duckduckgo-mcp-server) (1.2.1)\n",
      "Requirement already satisfied: typer>=0.16.0 in /usr/local/lib/python3.12/dist-packages (from mcp[cli]>=1.3.0->duckduckgo-mcp-server) (0.21.1)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.9->ollama) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.9->ollama) (2.41.4)\n",
      "Requirement already satisfied: cryptography>=3.4.0 in /usr/local/lib/python3.12/dist-packages (from pyjwt[crypto]>=2.10.1->mcp) (43.0.3)\n",
      "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.12/dist-packages (from uvicorn>=0.31.1->mcp) (8.3.1)\n",
      "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.12/dist-packages (from cryptography>=3.4.0->pyjwt[crypto]>=2.10.1->mcp) (2.0.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.0.0->langsmith<1.0.0,>=0.3.45->langchain-core>=1.2.8) (3.4.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.0.0->langsmith<1.0.0,>=0.3.45->langchain-core>=1.2.8) (2.5.0)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from typer>=0.16.0->mcp[cli]>=1.3.0->duckduckgo-mcp-server) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.12/dist-packages (from typer>=0.16.0->mcp[cli]>=1.3.0->duckduckgo-mcp-server) (13.9.4)\n",
      "Requirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from cffi>=1.12->cryptography>=3.4.0->pyjwt[crypto]>=2.10.1->mcp) (3.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->typer>=0.16.0->mcp[cli]>=1.3.0->duckduckgo-mcp-server) (4.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->typer>=0.16.0->mcp[cli]>=1.3.0->duckduckgo-mcp-server) (2.19.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer>=0.16.0->mcp[cli]>=1.3.0->duckduckgo-mcp-server) (0.1.2)\n",
      "Downloading ollama-0.6.1-py3-none-any.whl (14 kB)\n",
      "Downloading langchain_ollama-1.0.1-py3-none-any.whl (29 kB)\n",
      "Downloading langchain-1.2.9-py3-none-any.whl (111 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m111.2/111.2 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading langchain_core-1.2.9-py3-none-any.whl (496 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m496.3/496.3 kB\u001b[0m \u001b[31m29.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading langgraph-1.0.8-py3-none-any.whl (158 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m158.1/158.1 kB\u001b[0m \u001b[31m20.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading langchain_mcp_adapters-0.2.1-py3-none-any.whl (22 kB)\n",
      "Downloading duckduckgo_mcp_server-0.1.1-py3-none-any.whl (6.5 kB)\n",
      "Installing collected packages: ollama, langchain-core, langchain-ollama, langchain-mcp-adapters, duckduckgo-mcp-server, langgraph, langchain\n",
      "  Attempting uninstall: langchain-core\n",
      "    Found existing installation: langchain-core 1.2.8\n",
      "    Uninstalling langchain-core-1.2.8:\n",
      "      Successfully uninstalled langchain-core-1.2.8\n",
      "  Attempting uninstall: langgraph\n",
      "    Found existing installation: langgraph 1.0.7\n",
      "    Uninstalling langgraph-1.0.7:\n",
      "      Successfully uninstalled langgraph-1.0.7\n",
      "  Attempting uninstall: langchain\n",
      "    Found existing installation: langchain 1.2.8\n",
      "    Uninstalling langchain-1.2.8:\n",
      "      Successfully uninstalled langchain-1.2.8\n",
      "Successfully installed duckduckgo-mcp-server-0.1.1 langchain-1.2.9 langchain-core-1.2.9 langchain-mcp-adapters-0.2.1 langchain-ollama-1.0.1 langgraph-1.0.8 ollama-0.6.1\n"
     ]
    }
   ],
   "source": [
    "# Google Colab に必要なライブラリをインストールする。\n",
    "# 1行にまとめることで pip が全パッケージの依存関係を一括解決する。\n",
    "# NOTE: Colab では uv ではなく pip を使う。uv は依存解決の過程で\n",
    "#       numpy 等をアップグレードし、プリインストール済みの scipy 等を壊すため。\n",
    "# NOTE: langchain 関連は 1.x 系に明示的に指定する。\n",
    "#       Colab プリインストールの 0.3.x が残ると langchain-mcp-adapters が動作しない。\n",
    "%pip install -U ollama langchain-ollama \\\n",
    "     \"langchain>=1.2.8\" \"langchain-core>=1.2.8\" \\\n",
    "     \"langgraph>=1.0.7\" \\\n",
    "     \"langchain-mcp-adapters>=0.2.1\" duckduckgo-mcp-server mcp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Google Colab に Ollama をセットアップ**\n",
    "- Ollama のインストール・起動・モデルのダウンロードを行う。\n",
    "- 詳細は [01_connect_oss_llm.ipynb](01_connect_oss_llm.ipynb) を参照。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selecting previously unselected package zstd.\n",
      "(Reading database ... 121689 files and directories currently installed.)\n",
      "Preparing to unpack .../zstd_1.4.8+dfsg-3build1_amd64.deb ...\n",
      "Unpacking zstd (1.4.8+dfsg-3build1) ...\n",
      "Setting up zstd (1.4.8+dfsg-3build1) ...\n",
      "Processing triggers for man-db (2.10.2-1) ...\n",
      ">>> Installing ollama to /usr/local\n",
      ">>> Downloading ollama-linux-amd64.tar.zst\n",
      "######################################################################## 100.0%\n",
      ">>> Creating ollama user...\n",
      ">>> Adding ollama user to video group...\n",
      ">>> Adding current user to ollama group...\n",
      ">>> Creating ollama systemd service...\n",
      "\u001b[1m\u001b[31mWARNING:\u001b[m systemd is not running\n",
      "\u001b[1m\u001b[31mWARNING:\u001b[m Unable to detect NVIDIA/AMD GPU. Install lspci or lshw to automatically detect and install GPU dependencies.\n",
      ">>> The Ollama API is now available at 127.0.0.1:11434.\n",
      ">>> Install complete. Run \"ollama\" from the command line.\n",
      "success                                                     \n",
      "Done!\n",
      "  Model\n",
      "    architecture        gptoss    \n",
      "    parameters          20.9B     \n",
      "    context length      131072    \n",
      "    embedding length    2880      \n",
      "    quantization        MXFP4     \n",
      "\n",
      "  Capabilities\n",
      "    completion    \n",
      "    tools         \n",
      "    thinking      \n",
      "\n",
      "  Parameters\n",
      "    temperature    1    \n",
      "\n",
      "  License\n",
      "    Apache License               \n",
      "    Version 2.0, January 2004    \n",
      "    ...                          \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Ollama のインストール・起動・モデルのダウンロード\n",
    "# 詳細は 01_connect_oss_llm.ipynb を参照\n",
    "import subprocess\n",
    "import time\n",
    "import ollama  # type: ignore\n",
    "\n",
    "!apt-get install -y -qq zstd\n",
    "!curl -fsSL https://ollama.com/install.sh | sh\n",
    "\n",
    "process = subprocess.Popen(\n",
    "    [\"ollama\", \"serve\"],\n",
    "    stdout=subprocess.DEVNULL,\n",
    "    stderr=subprocess.DEVNULL,\n",
    ")\n",
    "time.sleep(5)\n",
    "\n",
    "\n",
    "# ReAct エージェントにはツールコール対応モデルが必要。\n",
    "# NOTE: ollama pull のプログレスバーは Colab で文字化けするため、\n",
    "#       Python API 経由でステータスのみ表示する。\n",
    "\n",
    "model_name = \"gpt-oss:20b\"\n",
    "\n",
    "for progress in ollama.pull(model_name, stream=True):\n",
    "    status = progress.get(\"status\", \"\")\n",
    "    total = progress.get(\"total\") or 0\n",
    "    completed = progress.get(\"completed\") or 0\n",
    "    if total:\n",
    "        line = f\"{status}: {completed / total:.0%}\"\n",
    "    else:\n",
    "        line = status\n",
    "    print(f\"\\r{line:<60}\", end=\"\", flush=True)\n",
    "print(\"\\nDone!\")\n",
    "!ollama show {model_name}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ChatOllama で LLM に接続**\n",
    "- 詳細は [01_connect_oss_llm.ipynb](01_connect_oss_llm.ipynb) を参照。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ChatOllama で LLM に接続する。\n",
    "from langchain_ollama import ChatOllama  # type: ignore\n",
    "\n",
    "llm = ChatOllama(\n",
    "    model=\"gpt-oss:20b\",\n",
    "    num_ctx=16384,\n",
    "    num_predict=-1,\n",
    "    temperature=0.8,\n",
    "    top_k=40,\n",
    "    top_p=0.9,\n",
    "    repeat_penalty=1.1,\n",
    "    reasoning=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**MCPサーバ（ddg-search）によるweb検索の実装**\n",
    "- langchain-mcp-adapters の MultiServerMCPClient を使用。\n",
    "- MCPサーバのツールが自動的に LangChain ツールに変換される（@tool の手動定義が不要）。\n",
    "- ReAct エージェントが bind_tools() で認識し、自律的にツールを呼び出せる形式になる。\n",
    "\n",
    "**MCPサーバ設定の補足**\n",
    "\n",
    "github に記載されているMCP接続の設定は以下\n",
    "> \"mcpServers\": {\"ddg-search\": {\"command\": \"uvx\", \"args\": [\"duckduckgo-mcp-server\"]}}\n",
    "\n",
    "今回は、pip install済なので、command=\"duckduckgo-mcp-server\" で直接起動。uvx経由ではないので、args=[] としてよい。\n",
    "\n",
    "**Colab の stderr 問題の回避（Colab 特有。通常の Python 環境では不要）**\n",
    "\n",
    "Colab の stderr は fileno() 未対応のため、MCP の stdio_client が失敗する。\n",
    "stdio_client の関数シグネチャ `errlog=sys.stderr` は**インポート時に評価が確定**するため、\n",
    "後から sys.stderr を差し替えても効果がない（Python のデフォルト引数の仕様）。\n",
    "そこで、langchain_mcp_adapters.sessions 内の stdio_client 参照自体を、\n",
    "errlog のデフォルトを /dev/null に変更したラッパー関数に差し替えて回避する。\n",
    "\n",
    "**httpx 0.28 互換性の回避（Colab 特有。通常の Python 環境では不要の可能性あり）**\n",
    "\n",
    "duckduckgo-mcp-server は httpx.TimeoutError を参照するが、依存先の httpx 0.28 で\n",
    "この属性が削除された（duckduckgo-mcp-server 側のバグ）。\n",
    "MCP サーバはサブプロセスで動作するため、ノートブック側のパッチが効かない。\n",
    "そこで、httpx をパッチしてからサーバを起動する Python ラッパー経由で起動する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== ReActエージェント用ツール ===\n",
      "  - search: \n",
      "    Search DuckDuckGo and return formatted results.\n",
      "\n",
      "    Args:\n",
      "        query: The search query string\n",
      "        max_results: Maximum number of results to return (default: 10)\n",
      "        ctx: MCP context for logging\n",
      "    \n",
      "  - fetch_content: \n",
      "    Fetch and parse content from a webpage URL.\n",
      "\n",
      "    Args:\n",
      "        url: The webpage URL to fetch content from\n",
      "        ctx: MCP context for logging\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "# MCPサーバ（duckduckgo-mcp-server）に接続し、LangChainツールを自動取得\n",
    "# langchain-mcp-adapters が MCP ツールを LangChain 互換に自動変換するため、\n",
    "# @tool による手動ラップが不要\n",
    "import os\n",
    "import sys\n",
    "import contextlib\n",
    "from langchain_mcp_adapters import sessions as _sessions  # type: ignore\n",
    "from langchain_mcp_adapters.client import MultiServerMCPClient  # type: ignore\n",
    "\n",
    "# --- Colab stderr 問題の回避パッチ（Colab 特有。通常環境では不要） ---\n",
    "# stdio_client(server, errlog=sys.stderr) のデフォルト値はインポート時に確定する。\n",
    "# Colab の stderr は fileno() 未対応のため、デフォルトのまま呼ぶと失敗する。\n",
    "# → errlog のデフォルトを /dev/null に差し替えたラッパーで上書きして回避する。\n",
    "# NOTE: _devnull はセッション中ずっと開いたままにする（閉じると書き込み先がなくなる）\n",
    "_devnull = open(os.devnull, \"w\")\n",
    "_original_stdio_client = _sessions.stdio_client\n",
    "\n",
    "\n",
    "@contextlib.asynccontextmanager\n",
    "async def _patched_stdio_client(server, errlog=_devnull):\n",
    "    async with _original_stdio_client(server, errlog=errlog) as result:\n",
    "        yield result\n",
    "\n",
    "\n",
    "_sessions.stdio_client = _patched_stdio_client\n",
    "\n",
    "\n",
    "# httpx 0.28 で削除された TimeoutError を復元してからサーバを起動するラッパー\n",
    "# duckduckgo-mcp-server はサブプロセスで動くため、ノートブック側のパッチが効かない\n",
    "_MCP_LAUNCHER = (\n",
    "    \"import httpx; \"\n",
    "    \"httpx.TimeoutError = getattr(httpx, 'TimeoutError', \"\n",
    "    \"type('TimeoutError', (Exception,), {})); \"\n",
    "    \"exec(open(__import__('shutil').which('duckduckgo-mcp-server')).read())\"\n",
    ")\n",
    "\n",
    "mcp_client = MultiServerMCPClient(\n",
    "    {\n",
    "        \"ddg-search\": {\n",
    "            \"transport\": \"stdio\",\n",
    "            \"command\": sys.executable,  # python 本体を起動\n",
    "            \"args\": [\"-c\", _MCP_LAUNCHER],  # パッチ付きラッパー\n",
    "        }\n",
    "    }\n",
    ")\n",
    "\n",
    "# MCP ツールを LangChain ツールとして自動取得\n",
    "tools = await mcp_client.get_tools()\n",
    "\n",
    "print(\"=== ReActエージェント用ツール ===\")\n",
    "for t in tools:\n",
    "    print(f\"  - {t.name}: {t.description}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**LangGraph による Workflow の実装**\n",
    "\n",
    "ReAct エージェント（03_01）は LLM が自律的にツールを呼び出すが、Workflow ではグラフ構造で処理フローを明示的に制御する。\n",
    "\n",
    "**Workflow の流れ**\n",
    "1. **task_planning**: ユーザの質問を受け取り、回答に必要な **サブタスク**（目的＋検索クエリ）を構造化して作成する。\n",
    "2. **web_search**: 各サブタスクの検索クエリを実行し、目的と紐付けた検索結果を蓄積する。\n",
    "3. **judge**: サブタスクの目的ごとに、検索結果が十分かを LLM が判断する。不足なら追加サブタスクを生成して web_search に戻る。\n",
    "4. **generate_answer**: 目的ごとに整理された検索結果をもとに、ユーザの質問に対する最終回答を生成する。\n",
    "\n",
    "**サブタスク構造**\n",
    "```json\n",
    "{\"purpose\": \"調査の目的\", \"queries\": [\"検索クエリ1\", \"検索クエリ2\"]}\n",
    "```\n",
    "- `purpose`: そのサブタスクで明らかにしたいこと（judge での判定基準になる）。\n",
    "- `queries`: 目的を達成するための具体的な検索クエリ。\n",
    "\n",
    "**ReAct との違い**\n",
    "- ReAct: LLM が思考→行動→観察のループを自律的に回す。ツール選択も LLM 任せ。\n",
    "- Workflow: 開発者がグラフで処理順序を定義し、各ノードの役割を明確に分離する。制御フローが予測可能。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Workflow の状態定義とツールの設定**\n",
    "\n",
    "MCP サーバ（ddg-search）は `search`（検索）と `fetch_content`（URL 本文取得）の2つのツールを提供するが、本 Workflow では `search` のみを使用する。\n",
    "\n",
    "`fetch_content` を追加しない理由:\n",
    "- `num_ctx=16384` の制約下では、Web ページ本文（数千〜数万トークン）を格納するとコンテキストがオーバーフローする。\n",
    "- 回避には fetch 後の LLM 要約が必要だが、追加の LLM 呼び出しで実行時間・コストが増加する。\n",
    "- DuckDuckGo のスニペットで回答品質が不足する場合に、限定的な導入（上位1件のみ fetch → 要約）を検討する。\n",
    "\n",
    "**LLM との入出力形式**\n",
    "\n",
    "各ノードは `messages` 引数で `[SystemMessage(...), HumanMessage(...)]` のリストを渡す統一形式を使用する。\n",
    "システムプロンプトは次のセルで定数として一括定義し、ノード関数はロジックに専念する。\n",
    "\n",
    "**共通ユーティリティ**\n",
    "\n",
    "JSON 配列のパース\n",
    "- `.strip()` で `raw` の LLM の文章から、文字列の「先頭」と「末尾」にある空白文字（スペース、タブ、改行 `\\n` ）をすべて削除する。文字列の「途中」にあるスペースには影響を与えない。\n",
    "- LLM の出力が、` ```json ... ``` ` となってしまう場合があり、 `if` で対処している。 `text[4:]` は、josn の文字数を意味している。（json の次の文字から読み込みする。）\n",
    "- `json.loads(text)` で、json 出力を、Python のリスト型/辞書型に変換する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Workflow の状態定義と共通インポート\n",
    "import json\n",
    "from typing import TypedDict\n",
    "from langchain_core.messages import HumanMessage, SystemMessage  # type: ignore\n",
    "from langgraph.graph import StateGraph, START, END  # type: ignore\n",
    "from IPython.display import Image, display\n",
    "\n",
    "# --- グローバル設定 ---\n",
    "MAX_LOOP_COUNT = 2  # judge → web_search 再調査ループの上限回数（無限ループ防止）\n",
    "MAX_SEARCH_RESULTS = 5  # search ツールが返す検索結果の最大件数\n",
    "\n",
    "# --- 検索ツールの参照を取得 ---\n",
    "search_tool = next((t for t in tools if t.name == \"search\"), None)\n",
    "if search_tool is None:\n",
    "    raise ValueError(\"search_tool が見つかりませんでした。\")\n",
    "\n",
    "\n",
    "# --- 共通ユーティリティ ---\n",
    "def extract_json_text(raw: str) -> str:\n",
    "    \"\"\"LLM の出力からコードブロックを除去し、JSON 文字列を抽出する。\"\"\"\n",
    "    text = raw.strip()\n",
    "    if \"```\" in text:\n",
    "        text = text.split(\"```\")[1]\n",
    "        if text.startswith(\"json\"):\n",
    "            text = text[4:]\n",
    "        text = text.strip()\n",
    "    return text\n",
    "\n",
    "\n",
    "# --- Workflow の状態 ---\n",
    "class WorkflowState(TypedDict):\n",
    "    question: str  # ユーザの質問\n",
    "    subtasks: list[dict]  # サブタスク [{\"purpose\": str, \"queries\": [str]}]\n",
    "    search_results: list[str]  # 目的と紐付けた検索結果\n",
    "    answer: str  # 最終回答\n",
    "    loop_count: int  # 再調査ループ回数（無限ループ防止）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**各ノードのシステムプロンプト定義**\n",
    "\n",
    "各ノード（task_planning / judge / generate_answer）が LLM に渡すシステムプロンプトを一箇所に集約する。\n",
    "- プロンプトの確認・調整が容易になる。\n",
    "- ノード関数のロジックとプロンプトの関心を分離する。\n",
    "\n",
    "**参考：JSON データ形式**\n",
    "- `key` : `value` の組から構成されるデータ\n",
    "- `{ }` で、`\"purpose\"` と `\"queries\"` で 1セットのデータ（辞書）として定義\n",
    "- `[ ]` で、辞書：`{ }` のリストとして定義\n",
    "- ... は、`{ }` の部分の繰り返しであることを意味する\n",
    "- JSON の仕様では、true/false。Python の仕様では、True/False。`SYSTEM_PROMPT_JUDGE` の `\"sufficient\"` の判定は、JSON の仕様で出力するので、true/falseで記述。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 各ノードのシステムプロンプト定義\n",
    "# ノード関数のロジックとプロンプトの関心を分離するため、一箇所に集約する。\n",
    "SYSTEM_PROMPT_TASK_PLANNING = \"\"\"\n",
    "あなたはリサーチプランナーです。\n",
    "ユーザの質問に回答するために必要な調査サブタスクを作成してください。\n",
    "\n",
    "出力は以下の JSON 配列のみとし、他のテキストは一切含めないでください。\n",
    "サブタスクは最大3個までとしてください。\n",
    "\n",
    "出力形式:\n",
    "[\n",
    "  {\"purpose\": \"このサブタスクで明らかにしたいこと\",\n",
    "   \"queries\": [\"検索クエリ1\", \"検索クエリ2\"]},\n",
    "  ...\n",
    "]\n",
    "\n",
    "purpose は判定ステップで「この目的に十分な情報が得られたか」を評価する基準になります。\n",
    "具体的かつ明確に書いてください。\n",
    "\"\"\"\n",
    "\n",
    "SYSTEM_PROMPT_JUDGE = \"\"\"\n",
    "あなたはリサーチの品質を判定する審査員です。\n",
    "ユーザの質問と検索結果を見て、回答に十分な情報があるか判断してください。\n",
    "検索結果には【目的: ...】タグが付いています。\n",
    "各目的について十分な情報が得られているかを確認してください。\n",
    "\n",
    "十分な場合:\n",
    "{\"sufficient\": true, \"reason\": \"判断理由を日本語で1文で\"}\n",
    "\n",
    "不足の場合（不足している目的について追加サブタスクを生成）:\n",
    "{\"sufficient\": false, \"reason\": \"何が不足しているかを日本語で1文で\",\n",
    " \"additional_subtasks\": [\n",
    "    {\"purpose\": \"追加で明らかにしたいこと\",\n",
    "     \"queries\": [\"追加クエリ1\"]}\n",
    "  ]\n",
    "}\n",
    "\n",
    "JSON のみ出力し、他のテキストは含めないでください。\n",
    "\"\"\"\n",
    "\n",
    "SYSTEM_PROMPT_GENERATE_ANSWER = \"\"\"\n",
    "あなたはリサーチ結果をもとに回答するAIアシスタントです。\n",
    "検索結果を参考に、ユーザの質問に日本語で丁寧に回答してください。\n",
    "回答の最後に、以下の形式で結論をまとめてください。\n",
    "\n",
    "# 結論\n",
    "- ユーザの質問: （質問内容）\n",
    "- 回答: （簡潔な回答）\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ノード 1: task_planning（タスク分割）**\n",
    "- ユーザの質問を分析し、回答に必要な **サブタスク**（目的＋検索クエリ）を構造化して作成する。\n",
    "- 各サブタスクは `purpose`（何を明らかにしたいか）と `queries`（検索クエリ群）で構成される。\n",
    "- LLM に JSON 配列のみを出力させ、パースして `subtasks` に格納する。\n",
    "\n",
    "**参考：JSON 配列のパース**\n",
    "- `extract_json_text` で実施。**共通ユーティリティ**を参照。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ノード 1: task_planning（タスク分割）\n",
    "async def task_planning(state: WorkflowState) -> dict:\n",
    "    \"\"\"ユーザの質問を分析し、サブタスク（目的＋検索クエリ）を作成する。\"\"\"\n",
    "    question = state[\"question\"]\n",
    "\n",
    "    response = await llm.ainvoke(\n",
    "        messages=[\n",
    "            SystemMessage(content=SYSTEM_PROMPT_TASK_PLANNING),\n",
    "            HumanMessage(content=question),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # LLM の出力から JSON 配列を抽出\n",
    "    text = extract_json_text(response.content)\n",
    "\n",
    "    try:\n",
    "        subtasks = json.loads(text)\n",
    "    except json.JSONDecodeError:\n",
    "        # パース失敗時は質問そのものを検索クエリにして続行する\n",
    "        print(f\"[task_planning] JSON パース失敗 → フォールバック: {text[:100]}\")\n",
    "        subtasks = [{\"purpose\": \"基本調査\", \"queries\": [question]}]\n",
    "\n",
    "    print(f\"[task_planning] サブタスク数: {len(subtasks)}\")\n",
    "    for i, st in enumerate(subtasks):\n",
    "        print(f\"  {i + 1}. 目的: {st['purpose']}\")\n",
    "        print(f\"     クエリ: {st['queries']}\")\n",
    "    return {\"subtasks\": subtasks, \"search_results\": [], \"loop_count\": 0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# task_planning の単体確認\n",
    "# ensure_ascii=False の指定をしないと日本語が文字化けするので注意\n",
    "_test_state = {\"question\": \"2025年の日本の総人口は何人ですか？\"}\n",
    "_result = await task_planning(_test_state)\n",
    "\n",
    "print(f\"\\nsubtasks: {json.dumps(_result['subtasks'], ensure_ascii=False, indent=2)}\")\n",
    "assert isinstance(_result[\"subtasks\"], list), \"subtasks がリストではありません\"\n",
    "assert len(_result[\"subtasks\"]) > 0, \"サブタスクが空です\"\n",
    "for st in _result[\"subtasks\"]:\n",
    "    assert \"purpose\" in st, f\"purpose キーがありません: {st}\"\n",
    "    assert \"queries\" in st, f\"queries キーがありません: {st}\"\n",
    "    assert isinstance(st[\"queries\"], list), f\"queries がリストではありません: {st}\"\n",
    "print(\"OK\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ノード 2: web_search（Web 検索）**\n",
    "\n",
    "`subtasks` 内の各サブタスクについて、`queries` を順番に MCP の `search` ツールで実行する。\n",
    "- `list(state.get(\"search_results\") or [])` で、「初回実行時は空リスト、2回目（ループ時）は前回の結果を引き継ぐ」として動作させる。list(...) で囲うことにより、元のデータをコピーして壊さないようにしている。\n",
    "- 検索結果はサブタスクの `purpose`（目的）と紐付けて `search_results` に蓄積する。\n",
    "> `results.append` で、目的: {purpose}, クエリ: {query}, {result} のように構造化したデータとして保存。\n",
    "- 実行済みのサブタスクは `subtasks` からクリアされる。\n",
    "> `\"subtasks\": []` でサブタスクを空にしている。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ノード 2: web_search（Web 検索）\n",
    "async def web_search(state: WorkflowState) -> dict:\n",
    "    \"\"\"各サブタスクの検索クエリを実行し、目的と紐付けて結果を蓄積する。\"\"\"\n",
    "    subtasks = state[\"subtasks\"]\n",
    "    results = list(state.get(\"search_results\") or [])\n",
    "\n",
    "    for st in subtasks:\n",
    "        purpose = st[\"purpose\"]\n",
    "        print(f\"[web_search] 目的: {purpose}\")\n",
    "        for query in st[\"queries\"]:\n",
    "            print(f\"  検索中: {query}\")\n",
    "            result = await search_tool.ainvoke(\n",
    "                {\"query\": query, \"max_results\": MAX_SEARCH_RESULTS}\n",
    "            )\n",
    "            results.append(f\"【目的: {purpose}】\\n【クエリ: {query}】\\n{result}\")\n",
    "\n",
    "    return {\"search_results\": results, \"subtasks\": []}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# web_search の単体確認\n",
    "_test_state = {\n",
    "    \"question\": \"2025年の日本の総人口は何人ですか？\",\n",
    "    \"subtasks\": [\n",
    "        {\n",
    "            \"purpose\": \"2025年の日本の総人口の最新の公式発表値を確認する\",\n",
    "            \"queries\": [\"2025年 日本 総人口\"],\n",
    "        }\n",
    "    ],\n",
    "    \"search_results\": [],\n",
    "}\n",
    "_result = await web_search(_test_state)\n",
    "\n",
    "print(f\"\\n検索結果数: {len(_result['search_results'])}\")\n",
    "print(f\"結果（先頭200文字）: {_result['search_results'][0][:200]}\")\n",
    "assert len(_result[\"search_results\"]) > 0, \"検索結果が空です\"\n",
    "assert \"【目的:\" in _result[\"search_results\"][0], \"目的タグが付いていません\"\n",
    "print(\"OK\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ノード 3: judge（判定）+ ルーター**\n",
    "- サブタスクの**目的ごと**に、検索結果が十分かを LLM が判断する。\n",
    "- 全ての目的が達成されていれば `generate_answer` へ進む。\n",
    "- 不足する目的があれば、追加サブタスク（目的＋クエリ）を生成して `web_search` に戻る。\n",
    "- 無限ループ防止のため、再調査は最大2回まで。\n",
    "- `should_continue_search` ルーターが judge の出力（`subtasks` の有無）を見て次のノードを決定する。\n",
    "\n",
    "**参考：JSON 配列のパース**\n",
    "- `extract_json_text` で実施。**共通ユーティリティ**を参照。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ノード 3: judge（判定）\n",
    "async def judge(state: WorkflowState) -> dict:\n",
    "    \"\"\"サブタスクの目的ごとに検索結果が十分かを判断し、不足なら追加サブタスクを生成する。\"\"\"\n",
    "    question = state[\"question\"]\n",
    "    results = state[\"search_results\"]\n",
    "    loop_count = state.get(\"loop_count\", 0)  # 値が空の場合に 0 をセットする\n",
    "\n",
    "    # 無限ループ防止: MAX_LOOP_COUNT 回まで再調査\n",
    "    if loop_count >= MAX_LOOP_COUNT:\n",
    "        print(\"[judge] ループ上限に到達 → 回答作成へ\")\n",
    "        return {\"subtasks\": [], \"loop_count\": loop_count}\n",
    "\n",
    "    # LLM テキストを渡すために、検索結果を 1つの文章に結合（区切り：\\n\\n）\n",
    "    results_text = \"\\n\\n\".join(results)\n",
    "\n",
    "    response = await llm.ainvoke(\n",
    "        messages=[\n",
    "            SystemMessage(content=SYSTEM_PROMPT_JUDGE),\n",
    "            HumanMessage(content=f\"質問: {question}\\n\\n検索結果:\\n{results_text}\"),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    text = extract_json_text(response.content)\n",
    "\n",
    "    try:\n",
    "        judgment = json.loads(text)\n",
    "    except json.JSONDecodeError:\n",
    "        # パース失敗時は十分と見なして先に進む\n",
    "        print(f\"[judge] JSON パース失敗 → 回答作成へ: {text[:100]}\")\n",
    "        return {\"subtasks\": [], \"loop_count\": loop_count + 1}\n",
    "\n",
    "    reason = judgment.get(\"reason\", \"\")  # 値が空の場合に \"\"（空）をセットする\n",
    "\n",
    "    if judgment.get(\"sufficient\", True):\n",
    "        print(f\"[judge] 情報十分 → 回答作成へ（理由: {reason}）\")\n",
    "        return {\"subtasks\": [], \"loop_count\": loop_count + 1}\n",
    "    else:\n",
    "        additional = judgment.get(\"additional_subtasks\", [])\n",
    "        print(f\"[judge] 情報不足（理由: {reason}）→ 追加サブタスク:\")\n",
    "        for i, st in enumerate(additional):\n",
    "            print(f\"  {i + 1}. 目的: {st.get('purpose', '?')}\")\n",
    "            print(f\"     クエリ: {st.get('queries', [])}\")\n",
    "        return {\"subtasks\": additional, \"loop_count\": loop_count + 1}\n",
    "\n",
    "\n",
    "# ルーター: judge の結果で分岐\n",
    "def should_continue_search(state: WorkflowState) -> str:\n",
    "    \"\"\"追加サブタスクがあれば web_search に戻り、なければ回答生成へ。\"\"\"\n",
    "    if state.get(\"subtasks\"):\n",
    "        return \"web_search\"\n",
    "    return \"generate_answer\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# judge + ルーターの単体確認\n",
    "_test_state = {\n",
    "    \"question\": \"2025年の日本の総人口は何人ですか？\",\n",
    "    \"subtasks\": [],\n",
    "    \"search_results\": [\n",
    "        \"【目的: 2025年の日本の総人口の最新の公式発表値を確認する】\\n\"\n",
    "        \"【クエリ: 2025年 日本 総人口】\\n\"\n",
    "        \"総務省発表: 2025年1月1日時点の住民基本台帳に基づく総人口は1億2433万690人\"\n",
    "    ],\n",
    "    \"loop_count\": 0,\n",
    "}\n",
    "_result = await judge(_test_state)\n",
    "\n",
    "print(f\"subtasks: {_result['subtasks']}\")\n",
    "print(f\"loop_count: {_result['loop_count']}\")\n",
    "\n",
    "# ルーターの確認\n",
    "_next = should_continue_search({**_test_state, **_result})\n",
    "print(f\"ルーター判定: {_next}\")\n",
    "assert _next in (\"web_search\", \"generate_answer\"), f\"不正なルーター出力: {_next}\"\n",
    "print(\"OK\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ノード 4: generate_answer（回答生成）**\n",
    "- 目的ごとに整理された検索結果をもとに、ユーザの質問に対する最終回答を生成する。\n",
    "- 検索結果に含まれる【目的: ...】タグにより、LLM が情報の文脈を把握しやすくなる。\n",
    "- 回答の最後に「結論」セクションを付ける。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ノード 4: generate_answer（回答生成）\n",
    "async def generate_answer(state: WorkflowState) -> dict:\n",
    "    \"\"\"蓄積した検索結果をもとに最終回答を生成する。\"\"\"\n",
    "    question = state[\"question\"]\n",
    "    results_text = \"\\n\\n\".join(state[\"search_results\"])\n",
    "\n",
    "    response = await llm.ainvoke(\n",
    "        messages=[\n",
    "            SystemMessage(content=SYSTEM_PROMPT_GENERATE_ANSWER),\n",
    "            HumanMessage(content=f\"質問: {question}\\n\\n検索結果:\\n{results_text}\"),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    print(\"[generate_answer] 回答生成完了\")\n",
    "    return {\"answer\": response.content}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate_answer の単体確認\n",
    "from IPython.display import Markdown\n",
    "\n",
    "_test_state = {\n",
    "    \"question\": \"2025年の日本の総人口は何人ですか？\",\n",
    "    \"search_results\": [\n",
    "        \"【目的: 2025年の日本の総人口の最新の公式発表値を確認する】\\n\"\n",
    "        \"【クエリ: 2025年 日本 総人口】\\n\"\n",
    "        \"総務省発表: 2025年1月1日時点の住民基本台帳に基づく総人口は1億2433万690人。\"\n",
    "        \"前年より55万4485人(0.44%)減少。日本人は1億2065万3227人で0.75%減少。\"\n",
    "    ],\n",
    "}\n",
    "_result = await generate_answer(_test_state)\n",
    "\n",
    "assert \"answer\" in _result, \"answer キーが存在しません\"\n",
    "assert len(_result[\"answer\"]) > 0, \"回答が空です\"\n",
    "display(Markdown(_result[\"answer\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Workflow グラフの構築**\n",
    "- 上記4つのノードを StateGraph に登録し、エッジで接続する。\n",
    "- `judge` ノードの後に条件分岐（`should_continue_search`）を設定する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Workflow グラフの構築とコンパイル\n",
    "workflow = StateGraph(WorkflowState)\n",
    "\n",
    "# ノードの登録\n",
    "workflow.add_node(\"task_planning\", task_planning)\n",
    "workflow.add_node(\"web_search\", web_search)\n",
    "workflow.add_node(\"judge\", judge)\n",
    "workflow.add_node(\"generate_answer\", generate_answer)\n",
    "\n",
    "# エッジの定義\n",
    "workflow.add_edge(START, \"task_planning\")\n",
    "workflow.add_edge(\"task_planning\", \"web_search\")\n",
    "workflow.add_edge(\"web_search\", \"judge\")\n",
    "\n",
    "# 条件分岐: judge → web_search（再調査） or generate_answer（回答生成）\n",
    "workflow.add_conditional_edges(\n",
    "    \"judge\",\n",
    "    should_continue_search,\n",
    "    {\n",
    "        \"web_search\": \"web_search\",\n",
    "        \"generate_answer\": \"generate_answer\",\n",
    "    },\n",
    ")\n",
    "workflow.add_edge(\"generate_answer\", END)\n",
    "\n",
    "# コンパイル\n",
    "app = workflow.compile()\n",
    "\n",
    "# グラフの可視化\n",
    "display(Image(app.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**動作確認**\n",
    "- Workflow エージェントに質問を投げ、タスク分割 → Web 検索 → 判定 → 回答生成の流れを確認する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Workflow エージェントの動作確認\n",
    "# MCPツールは非同期専用のため ainvoke を使用する\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "result = await app.ainvoke({\"question\": \"2025年の日本の総人口は何人ですか？\"})\n",
    "\n",
    "print(\"=== Workflow エージェントの実行結果 ===\\n\")\n",
    "display(Markdown(result[\"answer\"]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
