"""Gradio UI ãƒãƒ³ãƒ‰ãƒ©ï¼ˆãƒãƒ£ãƒƒãƒˆãƒ»ãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ãƒ»æ€è€ƒéç¨‹è¡¨ç¤ºï¼‰"""

from __future__ import annotations

import logging
import uuid
from collections.abc import AsyncIterator
from typing import TYPE_CHECKING, Any

import gradio as gr

if TYPE_CHECKING:
    from domain.config import WorkflowConfig
    from domain.ports.llm_port import LLMPort
    from domain.ports.reranker_port import RerankerPort
    from domain.ports.vectorstore_port import VectorStorePort
    from usecases.data_ingestion import DataIngestion

logger = logging.getLogger(__name__)


class GradioHandler:
    """Gradio UI ã®ã‚¤ãƒ™ãƒ³ãƒˆãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°

    notebook 07 ã¨åŒæ§˜ã«ã€å„ãƒãƒ¼ãƒ‰ã‚’å€‹åˆ¥ã«å‘¼ã³å‡ºã—ã¦
    æ€è€ƒéç¨‹ã®ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ è¡¨ç¤ºã¨ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°å›ç­”ã‚’å®Ÿç¾ã™ã‚‹ã€‚
    """

    def __init__(
        self,
        ingestion: DataIngestion,
        config: WorkflowConfig,
        llm: LLMPort,
        vectorstore: VectorStorePort,
        reranker: RerankerPort,
    ) -> None:
        self._ingestion = ingestion
        self._config = config
        self._llm = llm

        # ãƒãƒ¼ãƒ‰ãƒ•ã‚¡ã‚¯ãƒˆãƒªã‹ã‚‰ãƒãƒ¼ãƒ‰é–¢æ•°ã‚’ç”Ÿæˆ
        from usecases.nodes.doc_search_node import create_doc_search_node
        from usecases.nodes.judge_node import create_judge_node
        from usecases.nodes.summarize_node import create_summarize_node
        from usecases.nodes.task_planning_node import create_task_planning_node

        self._task_planning = create_task_planning_node(llm, config)
        self._doc_search = create_doc_search_node(vectorstore, reranker, config)
        self._summarize = create_summarize_node(llm, config)
        self._judge = create_judge_node(llm, config)

    async def respond(
        self,
        message: str,
        history: list[dict],
        system_prompt: str,
        temperature: float,
        thinking_log: str,
        session_state: dict,
    ) -> AsyncIterator[tuple[list[dict], str, dict]]:
        """ãƒãƒ£ãƒƒãƒˆå¿œç­”ã‚’æ®µéšçš„ã«ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ç”Ÿæˆã™ã‚‹ã€‚

        notebook 07 ã¨åŒæ§˜ã«å„ãƒãƒ¼ãƒ‰ã‚’å€‹åˆ¥ã«å‘¼ã³å‡ºã—ã€
        å„ã‚¹ãƒ†ãƒƒãƒ—ã§ yield ã—ã¦æ€è€ƒéç¨‹ã‚’ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ è¡¨ç¤ºã™ã‚‹ã€‚
        æœ€çµ‚å›ç­”ã¯ãƒˆãƒ¼ã‚¯ãƒ³å˜ä½ã§ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ã™ã‚‹ã€‚
        """
        thread_id = session_state.get("thread_id", str(uuid.uuid4()))
        session_state["thread_id"] = thread_id

        # ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’å±¥æ­´ã«è¿½åŠ 
        history = list(history) + [{"role": "user", "content": message}]

        # å‰å›ã®æ€è€ƒéç¨‹ã«åŒºåˆ‡ã‚Šç·šã‚’è¿½åŠ 
        if thinking_log.strip():
            thinking_log = thinking_log.rstrip("\n") + "\n\n" + "â”€" * 40 + "\n"

        # ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼çŠ¶æ…‹ã®åˆæœŸåŒ–
        state: dict[str, Any] = {
            "question": message,
            "subtasks": [],
            "search_results": [],
            "summary": "",
            "answer": "",
            "loop_count": 0,
        }

        # --- Phase 1: ã‚¿ã‚¹ã‚¯åˆ†å‰² ---
        thinking_log += "ğŸ“‹ ã‚¿ã‚¹ã‚¯åˆ†å‰²ä¸­...\n"
        yield history, thinking_log, session_state

        try:
            result = await self._task_planning(state)
            state.update(result)
        except Exception:
            logger.exception("ã‚¿ã‚¹ã‚¯åˆ†å‰²ã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ")
            state["subtasks"] = [{"purpose": "åŸºæœ¬èª¿æŸ»", "queries": [message]}]

        # ã‚µãƒ–ã‚¿ã‚¹ã‚¯æƒ…å ±ã‚’ãƒ­ã‚°ã«è¿½è¨˜
        thinking_log += f"ã‚µãƒ–ã‚¿ã‚¹ã‚¯æ•°: {len(state['subtasks'])}\n"
        for i, st in enumerate(state["subtasks"]):
            thinking_log += f"  {i + 1}. ç›®çš„: {st.get('purpose', '')}\n"
            thinking_log += f"     ã‚¯ã‚¨ãƒª: {st.get('queries', [])}\n"
        thinking_log += "\n"
        yield history, thinking_log, session_state

        # --- Phase 2: æ¤œç´¢ + è¦ç´„ + åˆ¤å®šãƒ«ãƒ¼ãƒ— ---
        while state["subtasks"]:
            # æ¤œç´¢
            thinking_log += "ğŸ” ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæ¤œç´¢ä¸­...\n"
            yield history, thinking_log, session_state

            try:
                result = self._doc_search(state)
                state.update(result)
            except Exception:
                logger.exception("æ¤œç´¢ã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ")
                state["subtasks"] = []

            thinking_log += f"  æ¤œç´¢çµæœãƒ–ãƒ­ãƒƒã‚¯æ•°: {len(state['search_results'])}\n\n"
            yield history, thinking_log, session_state

            # ãƒ«ãƒ¼ãƒ—ä¸Šé™ãƒã‚§ãƒƒã‚¯
            if state["loop_count"] >= self._config.max_loop_count:
                thinking_log += "âš ï¸ ãƒ«ãƒ¼ãƒ—ä¸Šé™ã«åˆ°é” â†’ å›ç­”ä½œæˆã¸\n\n"
                yield history, thinking_log, session_state
                break

            # è¦ç´„
            thinking_log += "ğŸ“ æ¤œç´¢çµæœã‚’è¦ç´„ä¸­...\n"
            yield history, thinking_log, session_state

            try:
                result = await self._summarize(state)
                state.update(result)
            except Exception:
                logger.exception("è¦ç´„ã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ")
                state["summary"] = "\n\n".join(state["search_results"])

            thinking_log += f"  è¦ç´„æ–‡å­—æ•°: {len(state['summary'])}\n\n"
            yield history, thinking_log, session_state

            # åˆ¤å®š
            thinking_log += "âš–ï¸ æƒ…å ±ã®ååˆ†æ€§ã‚’åˆ¤å®šä¸­...\n"
            yield history, thinking_log, session_state

            try:
                result = await self._judge(state)
                state.update(result)
            except Exception:
                logger.exception("åˆ¤å®šã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ")
                state["subtasks"] = []

            if state["subtasks"]:
                thinking_log += "ğŸ”„ æƒ…å ±ä¸è¶³ â†’ å†æ¤œç´¢\n"
                for i, st in enumerate(state["subtasks"]):
                    thinking_log += (
                        f"  è¿½åŠ  {i + 1}. {st.get('purpose', '')}: "
                        f"{st.get('queries', [])}\n"
                    )
            else:
                thinking_log += "âœ… æƒ…å ±ååˆ† â†’ å›ç­”ä½œæˆã¸\n"
            thinking_log += "\n"
            yield history, thinking_log, session_state

        # --- Phase 3: å›ç­”ç”Ÿæˆï¼ˆã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ï¼‰ ---
        thinking_log += "âœï¸ å›ç­”ã‚’ç”Ÿæˆä¸­...\n"
        yield history, thinking_log, session_state

        # ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæ§‹ç¯‰
        sys_content = (
            system_prompt + "\n\n" + self._config.system_prompt_generate_answer
        )

        # å›ç­”ç”Ÿæˆã«ã¯ç”Ÿã®æ¤œç´¢çµæœã‚’ä½¿ç”¨ï¼ˆæƒ…å ±ã®æ­£ç¢ºæ€§ã‚’ä¿æŒï¼‰
        results_text = "\n\n".join(state["search_results"])

        # ãƒãƒ«ãƒã‚¿ãƒ¼ãƒ³å¯¾å¿œ: ç›´è¿‘ã®ä¼šè©±å±¥æ­´ã‚’ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã«å«ã‚ã‚‹
        # ç¾åœ¨ã®è³ªå•ï¼ˆæœ«å°¾1ä»¶ï¼‰ã¯é™¤å¤–ã—ã€ç›´è¿‘4ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ï¼ˆ2å¾€å¾©ï¼‰ã¾ã§å«ã‚ã‚‹
        recent_history = history[:-1][-4:]
        history_lines: list[str] = []
        for msg in recent_history:
            role = "ãƒ¦ãƒ¼ã‚¶" if msg.get("role") == "user" else "AI"
            content = msg.get("content", "")[:500]
            history_lines.append(f"{role}: {content}")

        user_content = ""
        if history_lines:
            user_content += "ä¼šè©±å±¥æ­´:\n" + "\n".join(history_lines) + "\n\n"
        user_content += f"è³ªå•: {message}\n\næ¤œç´¢çµæœ:\n{results_text}"

        messages = [
            {"role": "system", "content": sys_content},
            {"role": "user", "content": user_content},
        ]

        # ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°å›ç­”ç”Ÿæˆ
        history = list(history) + [{"role": "assistant", "content": ""}]
        bot_reply = ""

        try:
            async for token in self._llm.astream(
                messages,
                reasoning=self._config.reasoning_generate_answer,
            ):
                bot_reply += token
                history[-1] = {"role": "assistant", "content": bot_reply}
                yield history, thinking_log, session_state
        except Exception:
            logger.exception("å›ç­”ç”Ÿæˆä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ")
            if not bot_reply:
                bot_reply = "ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚ã‚‚ã†ä¸€åº¦ãŠè©¦ã—ãã ã•ã„ã€‚"
                history[-1] = {"role": "assistant", "content": bot_reply}

        thinking_log += "âœ… å›ç­”ç”Ÿæˆå®Œäº†\n"
        yield history, thinking_log, session_state

    def upload_file(
        self,
        file: Any,
        session_state: dict,
    ) -> tuple[str, dict]:
        """PDF ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãƒ™ã‚¯ãƒˆãƒ« DB ã«ç™»éŒ²ã™ã‚‹ã€‚"""
        if file is None:
            return "ãƒ•ã‚¡ã‚¤ãƒ«ãŒé¸æŠã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚", session_state

        try:
            file_path = file.name if hasattr(file, "name") else str(file)
            count = self._ingestion.ingest(file_path)
            status = f"PDF èª­ã¿è¾¼ã¿å®Œäº†: {count} ãƒãƒ£ãƒ³ã‚¯"
            logger.info(status)
            return status, session_state
        except Exception:
            logger.exception("PDF ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ")
            return "PDF ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸã€‚", session_state

    def clear_chat(
        self,
        session_state: dict,
    ) -> tuple[list, str, dict]:
        """ä¼šè©±å±¥æ­´ã‚’ã‚¯ãƒªã‚¢ã™ã‚‹ã€‚"""
        session_state["thread_id"] = str(uuid.uuid4())
        return [], "", session_state

    def launch(self) -> gr.Blocks:
        """Gradio UI ã‚’æ§‹ç¯‰ã—ã¦è¿”ã™ã€‚"""
        with gr.Blocks(
            title="RAG ãƒãƒ£ãƒƒãƒˆã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆï¼ˆAI Agent Workflow + RAGï¼‰",
        ) as demo:
            gr.Markdown("### RAG ãƒãƒ£ãƒƒãƒˆã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆï¼ˆAI Agent Workflow + RAGï¼‰")

            session_state = gr.State(value={"thread_id": str(uuid.uuid4())})

            with gr.Row():
                # å·¦ã‚«ãƒ©ãƒ 
                with gr.Column(scale=1):
                    file_input = gr.File(
                        label="PDF ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ‰ãƒ©ãƒƒã‚°ï¼†ãƒ‰ãƒ­ãƒƒãƒ—",
                        file_types=[".pdf"],
                    )
                    pdf_status = gr.Textbox(
                        label="PDF ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹",
                        interactive=False,
                    )
                    thinking_log = gr.Textbox(
                        label="AI ã®æ€è€ƒéç¨‹",
                        interactive=False,
                        lines=25,
                        max_lines=25,
                    )

                # å³ã‚«ãƒ©ãƒ 
                with gr.Column(scale=1):
                    chatbot = gr.Chatbot(
                        label="AI ãƒãƒ£ãƒƒãƒˆ",
                        height=450,
                    )

                    with gr.Accordion("ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆè¨­å®š (ä»»æ„)", open=False):
                        system_prompt = gr.Textbox(
                            value=self._config.system_prompt_user_default,
                            label="ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ",
                            lines=2,
                        )

                    temperature = gr.Slider(
                        minimum=0.0,
                        maximum=1.0,
                        value=self._config.llm_temperature,
                        step=0.1,
                        label="Temperature (ä½ã„ã»ã©æ­£ç¢ºã€é«˜ã„ã»ã©å‰µé€ çš„)",
                    )

                    msg_input = gr.Textbox(
                        label="ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’å…¥åŠ›",
                        placeholder="è³ªå•ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„...",
                        lines=2,
                    )

                    with gr.Row():
                        submit_btn = gr.Button("é€ä¿¡", variant="primary")
                        stop_btn = gr.Button("ç”Ÿæˆã‚’åœæ­¢", variant="stop")
                        clear_btn = gr.Button("ä¼šè©±ã‚’ã‚¯ãƒªã‚¢", variant="secondary")

            # ã‚¤ãƒ™ãƒ³ãƒˆãƒãƒ³ãƒ‰ãƒ©
            submit_args = {
                "fn": self.respond,
                "inputs": [
                    msg_input,
                    chatbot,
                    system_prompt,
                    temperature,
                    thinking_log,
                    session_state,
                ],
                "outputs": [chatbot, thinking_log, session_state],
            }
            submit_event_click = submit_btn.click(**submit_args)
            submit_event_enter = msg_input.submit(**submit_args)

            # é€ä¿¡å¾Œã«å…¥åŠ›æ¬„ã‚’ã‚¯ãƒªã‚¢
            submit_btn.click(fn=lambda: "", outputs=msg_input)
            msg_input.submit(fn=lambda: "", outputs=msg_input)

            # ç”Ÿæˆåœæ­¢ãƒœã‚¿ãƒ³
            stop_btn.click(
                fn=None,
                inputs=None,
                outputs=None,
                cancels=[submit_event_click, submit_event_enter],
            )

            file_input.change(
                fn=self.upload_file,
                inputs=[file_input, session_state],
                outputs=[pdf_status, session_state],
            )

            clear_btn.click(
                fn=self.clear_chat,
                inputs=[session_state],
                outputs=[chatbot, thinking_log, session_state],
            )

        return demo
