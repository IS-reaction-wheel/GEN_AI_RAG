{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c894267f",
   "metadata": {},
   "source": [
    "### 03 WebサーチをするAIエージェント\n",
    "#### 03_01 ReAct\n",
    "- Google Colabに必要なライブラリをインストール\n",
    "- transformersを使ってLLMモデルをHugging Faceから読み込み\n",
    "- MCPサーバ(ddg-search)によるweb検索の実装\n",
    "- LangGraphによるReActの実装\n",
    "- 動作確認"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2072c39",
   "metadata": {},
   "source": [
    "**Google Colabに必要なライブラリをインストール**\n",
    "- 1行にまとめることで pip が全パッケージの依存関係を一括解決する。\n",
    "- 分割すると後勝ちで依存関係が壊れるリスクがある。\n",
    "  > （transformersとlangchain-huggingfaceで、huggingface-hubのバージョンが衝突する等）\n",
    "- NOTE: Colab では uv ではなく pip を使う。\n",
    "  > uv は依存解決の過程でnumpy 等をアップグレードし、プリインストール済みの scipy 等を壊すため。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "23577a1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (5.0.0)\n",
      "Collecting transformers\n",
      "  Downloading transformers-5.1.0-py3-none-any.whl.metadata (31 kB)\n",
      "Requirement already satisfied: accelerate in /usr/local/lib/python3.12/dist-packages (1.12.0)\n",
      "Collecting bitsandbytes\n",
      "  Downloading bitsandbytes-0.49.1-py3-none-manylinux_2_24_x86_64.whl.metadata (10 kB)\n",
      "Requirement already satisfied: langchain>=1.2.8 in /usr/local/lib/python3.12/dist-packages (1.2.8)\n",
      "Collecting langchain>=1.2.8\n",
      "  Downloading langchain-1.2.9-py3-none-any.whl.metadata (5.7 kB)\n",
      "Requirement already satisfied: langchain-core>=1.2.8 in /usr/local/lib/python3.12/dist-packages (1.2.8)\n",
      "Collecting langchain-core>=1.2.8\n",
      "  Downloading langchain_core-1.2.9-py3-none-any.whl.metadata (4.4 kB)\n",
      "Collecting langchain-huggingface>=1.2.0\n",
      "  Downloading langchain_huggingface-1.2.0-py3-none-any.whl.metadata (2.8 kB)\n",
      "Requirement already satisfied: langgraph>=1.0.7 in /usr/local/lib/python3.12/dist-packages (1.0.7)\n",
      "Collecting langgraph>=1.0.7\n",
      "  Downloading langgraph-1.0.8-py3-none-any.whl.metadata (7.4 kB)\n",
      "Collecting langchain-mcp-adapters>=0.2.1\n",
      "  Downloading langchain_mcp_adapters-0.2.1-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting duckduckgo-mcp-server\n",
      "  Downloading duckduckgo_mcp_server-0.1.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: mcp in /usr/local/lib/python3.12/dist-packages (1.26.0)\n",
      "Requirement already satisfied: huggingface-hub<2.0,>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (1.3.7)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.0.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (26.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2025.11.3)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.2)\n",
      "Requirement already satisfied: typer-slim in /usr/local/lib/python3.12/dist-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.7.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.2)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from accelerate) (5.9.5)\n",
      "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from accelerate) (2.9.0+cpu)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.12/dist-packages (from langchain>=1.2.8) (2.12.3)\n",
      "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core>=1.2.8) (1.33)\n",
      "Requirement already satisfied: langsmith<1.0.0,>=0.3.45 in /usr/local/lib/python3.12/dist-packages (from langchain-core>=1.2.8) (0.6.8)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core>=1.2.8) (9.1.2)\n",
      "Requirement already satisfied: typing-extensions<5.0.0,>=4.7.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core>=1.2.8) (4.15.0)\n",
      "Requirement already satisfied: uuid-utils<1.0,>=0.12.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core>=1.2.8) (0.14.0)\n",
      "INFO: pip is looking at multiple versions of langchain-huggingface to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting accelerate\n",
      "  Downloading accelerate-1.12.0-py3-none-any.whl.metadata (19 kB)\n",
      "  Downloading accelerate-1.11.0-py3-none-any.whl.metadata (19 kB)\n",
      "  Downloading accelerate-1.10.1-py3-none-any.whl.metadata (19 kB)\n",
      "  Downloading accelerate-1.10.0-py3-none-any.whl.metadata (19 kB)\n",
      "  Downloading accelerate-1.9.0-py3-none-any.whl.metadata (19 kB)\n",
      "  Downloading accelerate-1.8.1-py3-none-any.whl.metadata (19 kB)\n",
      "  Downloading accelerate-1.8.0-py3-none-any.whl.metadata (19 kB)\n",
      "INFO: pip is still looking at multiple versions of langchain-huggingface to determine which version is compatible with other requirements. This could take a while.\n",
      "  Downloading accelerate-1.7.0-py3-none-any.whl.metadata (19 kB)\n",
      "  Downloading accelerate-1.6.0-py3-none-any.whl.metadata (19 kB)\n",
      "  Downloading accelerate-1.5.2-py3-none-any.whl.metadata (19 kB)\n",
      "  Downloading accelerate-1.5.1-py3-none-any.whl.metadata (19 kB)\n",
      "  Downloading accelerate-1.5.0-py3-none-any.whl.metadata (19 kB)\n",
      "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
      "  Downloading accelerate-1.4.0-py3-none-any.whl.metadata (19 kB)\n",
      "  Downloading accelerate-1.3.0-py3-none-any.whl.metadata (19 kB)\n",
      "  Downloading accelerate-1.2.1-py3-none-any.whl.metadata (19 kB)\n",
      "  Downloading accelerate-1.2.0-py3-none-any.whl.metadata (19 kB)\n",
      "  Downloading accelerate-1.1.1-py3-none-any.whl.metadata (19 kB)\n",
      "  Downloading accelerate-1.1.0-py3-none-any.whl.metadata (19 kB)\n",
      "  Downloading accelerate-1.0.1-py3-none-any.whl.metadata (19 kB)\n",
      "  Downloading accelerate-1.0.0-py3-none-any.whl.metadata (19 kB)\n",
      "  Downloading accelerate-0.34.2-py3-none-any.whl.metadata (19 kB)\n",
      "  Downloading accelerate-0.34.1-py3-none-any.whl.metadata (19 kB)\n",
      "  Downloading accelerate-0.34.0-py3-none-any.whl.metadata (19 kB)\n",
      "  Downloading accelerate-0.33.0-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting numpy>=1.17 (from transformers)\n",
      "  Downloading numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting accelerate\n",
      "  Downloading accelerate-0.32.1-py3-none-any.whl.metadata (18 kB)\n",
      "  Downloading accelerate-0.32.0-py3-none-any.whl.metadata (18 kB)\n",
      "  Downloading accelerate-0.31.0-py3-none-any.whl.metadata (19 kB)\n",
      "  Downloading accelerate-0.30.1-py3-none-any.whl.metadata (18 kB)\n",
      "  Downloading accelerate-0.30.0-py3-none-any.whl.metadata (19 kB)\n",
      "  Downloading accelerate-0.29.3-py3-none-any.whl.metadata (18 kB)\n",
      "  Downloading accelerate-0.29.2-py3-none-any.whl.metadata (18 kB)\n",
      "  Downloading accelerate-0.29.1-py3-none-any.whl.metadata (18 kB)\n",
      "  Downloading accelerate-0.29.0-py3-none-any.whl.metadata (18 kB)\n",
      "  Downloading accelerate-0.28.0-py3-none-any.whl.metadata (18 kB)\n",
      "  Downloading accelerate-0.27.2-py3-none-any.whl.metadata (18 kB)\n",
      "  Downloading accelerate-0.27.1-py3-none-any.whl.metadata (18 kB)\n",
      "  Downloading accelerate-0.27.0-py3-none-any.whl.metadata (18 kB)\n",
      "  Downloading accelerate-0.26.1-py3-none-any.whl.metadata (18 kB)\n",
      "  Downloading accelerate-0.26.0-py3-none-any.whl.metadata (18 kB)\n",
      "  Downloading accelerate-0.25.0-py3-none-any.whl.metadata (18 kB)\n",
      "  Downloading accelerate-0.24.1-py3-none-any.whl.metadata (18 kB)\n",
      "  Downloading accelerate-0.24.0-py3-none-any.whl.metadata (18 kB)\n",
      "  Downloading accelerate-0.23.0-py3-none-any.whl.metadata (18 kB)\n",
      "  Downloading accelerate-0.22.0-py3-none-any.whl.metadata (17 kB)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.20.3)\n",
      "Collecting transformers\n",
      "  Downloading transformers-5.0.0-py3-none-any.whl.metadata (37 kB)\n",
      "  Downloading transformers-4.57.6-py3-none-any.whl.metadata (43 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.0/44.0 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting huggingface-hub<1.0,>=0.34.0 (from transformers)\n",
      "  Downloading huggingface_hub-0.36.2-py3-none-any.whl.metadata (15 kB)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.4)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.3.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.2.0)\n",
      "Requirement already satisfied: langgraph-checkpoint<5.0.0,>=2.1.0 in /usr/local/lib/python3.12/dist-packages (from langgraph>=1.0.7) (4.0.0)\n",
      "Requirement already satisfied: langgraph-prebuilt<1.1.0,>=1.0.7 in /usr/local/lib/python3.12/dist-packages (from langgraph>=1.0.7) (1.0.7)\n",
      "Requirement already satisfied: langgraph-sdk<0.4.0,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from langgraph>=1.0.7) (0.3.3)\n",
      "Requirement already satisfied: xxhash>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from langgraph>=1.0.7) (3.6.0)\n",
      "Requirement already satisfied: beautifulsoup4>=4.13.3 in /usr/local/lib/python3.12/dist-packages (from duckduckgo-mcp-server) (4.13.5)\n",
      "Requirement already satisfied: httpx>=0.28.1 in /usr/local/lib/python3.12/dist-packages (from duckduckgo-mcp-server) (0.28.1)\n",
      "Requirement already satisfied: anyio>=4.5 in /usr/local/lib/python3.12/dist-packages (from mcp) (4.12.1)\n",
      "Requirement already satisfied: httpx-sse>=0.4 in /usr/local/lib/python3.12/dist-packages (from mcp) (0.4.3)\n",
      "Requirement already satisfied: jsonschema>=4.20.0 in /usr/local/lib/python3.12/dist-packages (from mcp) (4.26.0)\n",
      "Requirement already satisfied: pydantic-settings>=2.5.2 in /usr/local/lib/python3.12/dist-packages (from mcp) (2.12.0)\n",
      "Requirement already satisfied: pyjwt>=2.10.1 in /usr/local/lib/python3.12/dist-packages (from pyjwt[crypto]>=2.10.1->mcp) (2.11.0)\n",
      "Requirement already satisfied: python-multipart>=0.0.9 in /usr/local/lib/python3.12/dist-packages (from mcp) (0.0.22)\n",
      "Requirement already satisfied: sse-starlette>=1.6.1 in /usr/local/lib/python3.12/dist-packages (from mcp) (3.2.0)\n",
      "Requirement already satisfied: starlette>=0.27 in /usr/local/lib/python3.12/dist-packages (from mcp) (0.50.0)\n",
      "Requirement already satisfied: typing-inspection>=0.4.1 in /usr/local/lib/python3.12/dist-packages (from mcp) (0.4.2)\n",
      "Requirement already satisfied: uvicorn>=0.31.1 in /usr/local/lib/python3.12/dist-packages (from mcp) (0.40.0)\n",
      "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio>=4.5->mcp) (3.11)\n",
      "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4>=4.13.3->duckduckgo-mcp-server) (2.8.3)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx>=0.28.1->duckduckgo-mcp-server) (2026.1.4)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx>=0.28.1->duckduckgo-mcp-server) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx>=0.28.1->duckduckgo-mcp-server) (0.16.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core>=1.2.8) (3.0.0)\n",
      "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.20.0->mcp) (25.4.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.20.0->mcp) (2025.9.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.20.0->mcp) (0.37.0)\n",
      "Requirement already satisfied: rpds-py>=0.25.0 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.20.0->mcp) (0.30.0)\n",
      "Requirement already satisfied: ormsgpack>=1.12.0 in /usr/local/lib/python3.12/dist-packages (from langgraph-checkpoint<5.0.0,>=2.1.0->langgraph>=1.0.7) (1.12.2)\n",
      "Requirement already satisfied: orjson>=3.10.1 in /usr/local/lib/python3.12/dist-packages (from langgraph-sdk<0.4.0,>=0.3.0->langgraph>=1.0.7) (3.11.7)\n",
      "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core>=1.2.8) (1.0.0)\n",
      "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core>=1.2.8) (0.25.0)\n",
      "Requirement already satisfied: python-dotenv>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from mcp[cli]>=1.3.0->duckduckgo-mcp-server) (1.2.1)\n",
      "Requirement already satisfied: typer>=0.16.0 in /usr/local/lib/python3.12/dist-packages (from mcp[cli]>=1.3.0->duckduckgo-mcp-server) (0.21.1)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain>=1.2.8) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain>=1.2.8) (2.41.4)\n",
      "Requirement already satisfied: cryptography>=3.4.0 in /usr/local/lib/python3.12/dist-packages (from pyjwt[crypto]>=2.10.1->mcp) (43.0.3)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (75.2.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (3.6.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (3.1.6)\n",
      "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.12/dist-packages (from uvicorn>=0.31.1->mcp) (8.3.1)\n",
      "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.12/dist-packages (from cryptography>=3.4.0->pyjwt[crypto]>=2.10.1->mcp) (2.0.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=2.0.0->accelerate) (1.3.0)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from typer>=0.16.0->mcp[cli]>=1.3.0->duckduckgo-mcp-server) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.12/dist-packages (from typer>=0.16.0->mcp[cli]>=1.3.0->duckduckgo-mcp-server) (13.9.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.3)\n",
      "Requirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from cffi>=1.12->cryptography>=3.4.0->pyjwt[crypto]>=2.10.1->mcp) (3.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->typer>=0.16.0->mcp[cli]>=1.3.0->duckduckgo-mcp-server) (4.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->typer>=0.16.0->mcp[cli]>=1.3.0->duckduckgo-mcp-server) (2.19.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer>=0.16.0->mcp[cli]>=1.3.0->duckduckgo-mcp-server) (0.1.2)\n",
      "Downloading transformers-4.57.6-py3-none-any.whl (12.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.0/12.0 MB\u001b[0m \u001b[31m92.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m:01\u001b[0m\n",
      "\u001b[?25hDownloading langchain_huggingface-1.2.0-py3-none-any.whl (30 kB)\n",
      "Downloading huggingface_hub-0.36.2-py3-none-any.whl (566 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m566.4/566.4 kB\u001b[0m \u001b[31m35.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading bitsandbytes-0.49.1-py3-none-manylinux_2_24_x86_64.whl (59.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.1/59.1 MB\u001b[0m \u001b[31m15.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading langchain-1.2.9-py3-none-any.whl (111 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m111.2/111.2 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading langchain_core-1.2.9-py3-none-any.whl (496 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m496.3/496.3 kB\u001b[0m \u001b[31m34.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading langgraph-1.0.8-py3-none-any.whl (158 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m158.1/158.1 kB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading langchain_mcp_adapters-0.2.1-py3-none-any.whl (22 kB)\n",
      "Downloading duckduckgo_mcp_server-0.1.1-py3-none-any.whl (6.5 kB)\n",
      "Installing collected packages: huggingface-hub, bitsandbytes, transformers, langchain-core, langchain-mcp-adapters, langchain-huggingface, duckduckgo-mcp-server, langgraph, langchain\n",
      "  Attempting uninstall: huggingface-hub\n",
      "    Found existing installation: huggingface_hub 1.3.7\n",
      "    Uninstalling huggingface_hub-1.3.7:\n",
      "      Successfully uninstalled huggingface_hub-1.3.7\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 5.0.0\n",
      "    Uninstalling transformers-5.0.0:\n",
      "      Successfully uninstalled transformers-5.0.0\n",
      "  Attempting uninstall: langchain-core\n",
      "    Found existing installation: langchain-core 1.2.8\n",
      "    Uninstalling langchain-core-1.2.8:\n",
      "      Successfully uninstalled langchain-core-1.2.8\n",
      "  Attempting uninstall: langgraph\n",
      "    Found existing installation: langgraph 1.0.7\n",
      "    Uninstalling langgraph-1.0.7:\n",
      "      Successfully uninstalled langgraph-1.0.7\n",
      "  Attempting uninstall: langchain\n",
      "    Found existing installation: langchain 1.2.8\n",
      "    Uninstalling langchain-1.2.8:\n",
      "      Successfully uninstalled langchain-1.2.8\n",
      "Successfully installed bitsandbytes-0.49.1 duckduckgo-mcp-server-0.1.1 huggingface-hub-0.36.2 langchain-1.2.9 langchain-core-1.2.9 langchain-huggingface-1.2.0 langchain-mcp-adapters-0.2.1 langgraph-1.0.8 transformers-4.57.6\n"
     ]
    }
   ],
   "source": [
    "# Google Colabに必要なライブラリをインストールする。\n",
    "# 1行にまとめることで pip が全パッケージの依存関係を一括解決する。\n",
    "# 分割すると後勝ちで依存関係が壊れるリスクがある。\n",
    "# （transformersとlangchain-huggingfaceで、huggingface-hubのバージョンが衝突する等）\n",
    "# NOTE: Colab では uv ではなく pip を使う。uv は依存解決の過程で\n",
    "#       numpy 等をアップグレードし、プリインストール済みの scipy 等を壊すため。\n",
    "# NOTE: langchain 関連は 1.x 系に明示的に指定する。\n",
    "#       Colab プリインストールの 0.3.x が残ると langchain-mcp-adapters が動作しない。\n",
    "%pip install -U transformers accelerate bitsandbytes \\\n",
    "     \"langchain>=1.2.8\" \"langchain-core>=1.2.8\" \\\n",
    "     \"langchain-huggingface>=1.2.0\" \"langgraph>=1.0.7\" \\\n",
    "     \"langchain-mcp-adapters>=0.2.1\" duckduckgo-mcp-server mcp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b944db5",
   "metadata": {},
   "source": [
    "**transformersを使ってLLMモデルをHugging Faceから読み込み**\n",
    "\n",
    "##### LLM バックエンドの選定\n",
    "\n",
    "このリポジトリでは **transformers + bitsandbytes** を採用。\n",
    "- Colab GPU との相性が良い\n",
    "- 量子化などの細かい設定が可能\n",
    "- 非同期処理に対応\n",
    "\n",
    "**将来の本番環境向け:**\n",
    "- 非同期/スケーラビリティ重視が必要となる場合、Ollamaなどサーバ管理されたものを使用する。\n",
    "\n",
    "##### モデルの選定\n",
    "\n",
    "**granite-3.3-2b-instruct** を使用。\n",
    "- 同シリーズの granite-4.0-h-micro は Mamba-Attention ハイブリッド構造のため、\n",
    "  ツール定義を含む長いプロンプトの推論で Mamba レイヤーが ~12GB の中間テンソルを生成し、\n",
    "  T4 (15GB) では量子化しても VRAM が不足する。\n",
    "- granite-3.3-2b-instruct は純粋な Transformer 構造で、T4 GPU で問題なく動作する。\n",
    "\n",
    "**ChatHuggingFace のツールコール問題とパッチ**\n",
    "\n",
    "ChatHuggingFace のローカルモデル（HuggingFacePipeline）は、`bind_tools()` でツール定義を\n",
    "`apply_chat_template` に渡さない既知の不具合がある（[langchain #29033](https://github.com/langchain-ai/langchain/issues/29033)）。\n",
    "以下の3点をパッチして対応する（特定モデルに依存しない汎用実装）:\n",
    "\n",
    "1. **`_to_chatml_format`** を上書きし、ReActループで必要な ToolMessage と tool_calls 付き AIMessage に対応。\n",
    "2. **`_generate`** を上書きし、ツール定義の注入とモデル出力のパースを行う:\n",
    "   - `apply_chat_template(tools=...)` でツール定義をプロンプトに注入\n",
    "   - `model.generate()` + `skip_special_tokens=False` で特殊トークンを保持（pipeline は `skip_special_tokens=True` 固定で `<|tool_call|>` 等が消えるため）\n",
    "   - 汎用パーサーで以下の形式に対応:\n",
    "     - 特殊トークン形式: `<|tool_call|>` + JSON（Granite 3.x）\n",
    "     - XML タグ形式: `<tool_call>...</tool_call>`（Granite 4.0, Qwen 2.5）\n",
    "     - JSON 配列形式: `[{\"name\":..., \"arguments\":...}]`（Llama 3.x 等）\n",
    "3. **`_agenerate`** を上書きし、非同期コンテキストから同期の `_generate` を呼び出す（HuggingFacePipeline は非同期未対応だが、MCPツールは非同期専用のため）。\n",
    "\n",
    "**NOTE**: 2B モデルでは `do_sample=True`（サンプリング）時にツールコールが不安定な場合がある。\n",
    "その場合は `generation_params` の `do_sample` を `False` に変更する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2aea36c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:torchao.kernel.intmm:Warning: Detected no triton, on systems without Triton certain kernels will not work\n",
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:104: UserWarning: \n",
      "Error while fetching `HF_TOKEN` secret value from your vault: 'Requesting secret HF_TOKEN timed out. Secrets can only be fetched when running from the Colab UI.'.\n",
      "You are not authenticated with the Hugging Face Hub in this notebook.\n",
      "If the error persists, please let us know by opening an issue on GitHub (https://github.com/huggingface/huggingface_hub/issues/new).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58f9dee0541d4240bb693e804a197fc8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb4fa7c12f9b4756a4cc882c6b5ebb74",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a2a8eca5c204956bdbd46a720e5e941",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d567973d2db42cbb24cb453406b0ace",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dfdbbd65e5f4409eaa91358fe0119c5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/207 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "663fa057b5e94278bf059663b2c4d236",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/801 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be23bb04146948c696cd14b32c48527d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/787 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9a5413ec87240febf270dc3f1afafda",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03ac7e6fb221414098e4bdaf701b00c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1204bee628ae44f8829464846f5fb0fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/67.1M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4543d81f79944c9ac0bef20d856480e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38fc8248c2b64ecf88f5798d5c86c16a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "109d799241214ba4bc02e71720a3e8b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/132 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "モデルのメモリ消費量: 4.72 GB\n"
     ]
    }
   ],
   "source": [
    "# transformersを使ってLLMモデルをHugging Faceから読み込みする。\n",
    "import json\n",
    "import re\n",
    "import uuid\n",
    "import torch  # type: ignore\n",
    "from transformers import (  # type: ignore\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,  # noqa: F401\n",
    "    pipeline,\n",
    ")  # type: ignore\n",
    "from langchain_huggingface import HuggingFacePipeline, ChatHuggingFace  # type: ignore\n",
    "from langchain_core.messages import (  # type: ignore\n",
    "    AIMessage,\n",
    "    HumanMessage,\n",
    "    SystemMessage,\n",
    "    ToolMessage,\n",
    ")\n",
    "\n",
    "\n",
    "# --- ChatHuggingFace のツールコール対応パッチ ---\n",
    "# ChatHuggingFace (HuggingFacePipeline) には以下の問題がある:\n",
    "#   1. _to_chatml_format が ToolMessage / tool_calls 付き AIMessage に未対応\n",
    "#   2. bind_tools() のツール定義が apply_chat_template に渡されない\n",
    "#   3. モデル出力のツールコールが LangChain の tool_calls にパースされない\n",
    "#   4. HuggingFacePipeline は非同期未対応だが、MCPツールは非同期専用\n",
    "# 対処: _to_chatml_format, _generate, _agenerate の3つをパッチする\n",
    "# NOTE: このパッチは特定モデルに依存しない汎用的な実装\n",
    "\n",
    "\n",
    "# --- パッチ1: _to_chatml_format ---\n",
    "# オリジナルは SystemMessage / HumanMessage / AIMessage のみ対応。\n",
    "# ReAct ループでは ToolMessage（ツール実行結果）と\n",
    "# tool_calls 付き AIMessage（ツール呼び出し指示）が必要。\n",
    "def _patched_to_chatml_format(self, message):\n",
    "    if isinstance(message, SystemMessage):\n",
    "        return {\"role\": \"system\", \"content\": message.content}\n",
    "    elif isinstance(message, HumanMessage):\n",
    "        return {\"role\": \"user\", \"content\": message.content}\n",
    "    elif isinstance(message, AIMessage):\n",
    "        msg = {\"role\": \"assistant\", \"content\": message.content or \"\"}\n",
    "        if message.tool_calls:\n",
    "            msg[\"tool_calls\"] = [\n",
    "                {\n",
    "                    \"id\": tc.get(\"id\", str(uuid.uuid4())),\n",
    "                    \"type\": \"function\",\n",
    "                    \"function\": {\n",
    "                        \"name\": tc[\"name\"],\n",
    "                        \"arguments\": json.dumps(tc[\"args\"]),\n",
    "                    },\n",
    "                }\n",
    "                for tc in message.tool_calls\n",
    "            ]\n",
    "        return msg\n",
    "    elif isinstance(message, ToolMessage):\n",
    "        return {\n",
    "            \"role\": \"tool\",\n",
    "            \"content\": message.content,\n",
    "            \"tool_call_id\": message.tool_call_id,\n",
    "        }\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown message type: {type(message)}\")\n",
    "\n",
    "\n",
    "# --- 汎用ツールコール出力パーサー ---\n",
    "# モデルごとにツールコールの出力形式が異なるため、複数形式を順に試行する。\n",
    "# 対応形式:\n",
    "#   1. 特殊トークン形式: <|tool_call|> + JSON（Granite 3.x）\n",
    "#   2. XML タグ形式: <tool_call>...</tool_call>（Granite 4.0 H, Qwen 2.5）\n",
    "#   3. JSON 配列形式: [{\"name\":..., \"arguments\":...}]（Llama 3.x 等）\n",
    "\n",
    "\n",
    "def _normalize_tool_call(item: dict) -> dict:\n",
    "    \"\"\"個別のツールコール dict を LangChain 形式に正規化する\"\"\"\n",
    "    # arguments / parameters の両方に対応（Llama は parameters を使う）\n",
    "    args = item.get(\"arguments\", item.get(\"parameters\", {}))\n",
    "    if isinstance(args, str):\n",
    "        try:\n",
    "            args = json.loads(args)\n",
    "        except json.JSONDecodeError:\n",
    "            args = {}\n",
    "    return {\n",
    "        \"name\": item.get(\"name\", \"\"),\n",
    "        \"args\": args,\n",
    "        \"id\": str(uuid.uuid4()),\n",
    "        \"type\": \"tool_call\",\n",
    "    }\n",
    "\n",
    "\n",
    "def _try_parse_json_tool_calls(json_str: str) -> list[dict]:\n",
    "    \"\"\"JSON 文字列からツールコールのリストをパースする\"\"\"\n",
    "    json_str = json_str.strip()\n",
    "    if not json_str:\n",
    "        return []\n",
    "    try:\n",
    "        parsed = json.loads(json_str)\n",
    "        if isinstance(parsed, dict):\n",
    "            parsed = [parsed]\n",
    "        if isinstance(parsed, list):\n",
    "            return [\n",
    "                _normalize_tool_call(item) for item in parsed if isinstance(item, dict)\n",
    "            ]\n",
    "    except json.JSONDecodeError:\n",
    "        pass\n",
    "    return []\n",
    "\n",
    "\n",
    "def _parse_tool_calls(content: str) -> tuple[str, list[dict]]:\n",
    "    \"\"\"モデル出力からツールコールをパースする（複数形式対応）\"\"\"\n",
    "    # 形式1: 特殊トークン形式 — <|tool_call|> + JSON（Granite 3.x）\n",
    "    if \"<|tool_call|>\" in content:\n",
    "        parts = content.rsplit(\"<|tool_call|>\")\n",
    "        cleaned = parts[0].strip()\n",
    "        json_str = re.sub(r\"<\\|[^|]*\\|>\", \"\", parts[-1]).strip()\n",
    "        tool_calls = _try_parse_json_tool_calls(json_str)\n",
    "        if tool_calls:\n",
    "            return cleaned, tool_calls\n",
    "\n",
    "    # 形式2: XML タグ形式 — <tool_call>...</tool_call>（Granite 4.0 H, Qwen 2.5）\n",
    "    xml_pattern = r\"<tool_call>\\s*(\\{.*?\\})\\s*</tool_call>\"\n",
    "    xml_matches = re.findall(xml_pattern, content, re.DOTALL)\n",
    "    if xml_matches:\n",
    "        tool_calls = []\n",
    "        for match in xml_matches:\n",
    "            calls = _try_parse_json_tool_calls(match)\n",
    "            tool_calls.extend(calls)\n",
    "        if tool_calls:\n",
    "            cleaned = re.sub(\n",
    "                r\"<tool_call>\\s*\\{.*?\\}\\s*</tool_call>\", \"\", content, flags=re.DOTALL\n",
    "            ).strip()\n",
    "            return cleaned, tool_calls\n",
    "\n",
    "    # 形式3: JSON 配列形式 — [{\"name\":..., ...}]（Llama 3.x 等）\n",
    "    # 出力末尾の JSON 配列を検出\n",
    "    json_array_pattern = r\"\\[(\\s*\\{.*?\\}\\s*(?:,\\s*\\{.*?\\}\\s*)*)\\]\"\n",
    "    json_matches = list(re.finditer(json_array_pattern, content, re.DOTALL))\n",
    "    if json_matches:\n",
    "        last_match = json_matches[-1]\n",
    "        json_str = last_match.group(0)\n",
    "        tool_calls = _try_parse_json_tool_calls(json_str)\n",
    "        # name と arguments/parameters を持つものだけツールコールとみなす\n",
    "        if tool_calls and all(tc[\"name\"] for tc in tool_calls):\n",
    "            cleaned = content[: last_match.start()].strip()\n",
    "            return cleaned, tool_calls\n",
    "\n",
    "    return content, []\n",
    "\n",
    "\n",
    "# --- パッチ2: _generate ---\n",
    "# tools 使用時は pipeline をバイパスし model.generate() を直接呼び出す。\n",
    "# 理由: pipeline の postprocess は skip_special_tokens=True 固定のため、\n",
    "# ツールコール用の特殊トークン（<|tool_call|> 等）が除去されてしまう。\n",
    "# model.generate() + tokenizer.decode(skip_special_tokens=False) は\n",
    "# 全 HF モデル共通の API であり、モデル非依存。\n",
    "_original_generate = ChatHuggingFace._generate\n",
    "\n",
    "\n",
    "def _patched_generate(self, messages, stop=None, run_manager=None, **kwargs):\n",
    "    \"\"\"_generate をパッチし、tools の注入とツールコールのパースを行う\"\"\"\n",
    "    from langchain_core.outputs import ChatGeneration, ChatResult\n",
    "\n",
    "    tools = kwargs.pop(\"tools\", None)\n",
    "    if tools:\n",
    "        messages_dicts = [self._to_chatml_format(m) for m in messages]\n",
    "        llm_input = self.tokenizer.apply_chat_template(\n",
    "            messages_dicts,\n",
    "            tools=tools,\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=True,\n",
    "        )\n",
    "        # model.generate() を直接呼び出し（全 HF モデル共通 API）\n",
    "        hf_pipeline = self.llm.pipeline\n",
    "        inputs = self.tokenizer(llm_input, return_tensors=\"pt\").to(hf_pipeline.device)\n",
    "        with torch.no_grad():\n",
    "            output_ids = hf_pipeline.model.generate(\n",
    "                **inputs,\n",
    "                **self.llm.pipeline_kwargs,\n",
    "            )\n",
    "        # skip_special_tokens=False でツールコールトークンを保持\n",
    "        new_ids = output_ids[0][inputs[\"input_ids\"].shape[1] :]\n",
    "        llm_result = self.tokenizer.decode(new_ids, skip_special_tokens=False)\n",
    "        # EOS トークン以降を除去\n",
    "        llm_result = re.sub(r\"<\\|end_of_text\\|>.*\", \"\", llm_result).strip()\n",
    "        llm_result = re.sub(r\"</s>.*\", \"\", llm_result).strip()\n",
    "        llm_result = re.sub(r\"<\\|eot_id\\|>.*\", \"\", llm_result).strip()\n",
    "\n",
    "        gen = ChatGeneration(message=AIMessage(content=llm_result))\n",
    "        result = ChatResult(generations=[gen])\n",
    "    else:\n",
    "        result = _original_generate(\n",
    "            self, messages, stop=stop, run_manager=run_manager, **kwargs\n",
    "        )\n",
    "\n",
    "    # ツールコールをパースして tool_calls に変換\n",
    "    for gen in result.generations:\n",
    "        if isinstance(gen.message, AIMessage) and gen.message.content:\n",
    "            cleaned, tool_calls = _parse_tool_calls(gen.message.content)\n",
    "            if tool_calls:\n",
    "                gen.message.content = cleaned\n",
    "                gen.message.tool_calls = tool_calls\n",
    "    return result\n",
    "\n",
    "\n",
    "# --- パッチ3: _agenerate ---\n",
    "# HuggingFacePipeline は非同期未対応のため、同期の _generate を呼び出す。\n",
    "# MCPツールが非同期専用のため、agent.ainvoke() → _agenerate が呼ばれる。\n",
    "async def _patched_agenerate(self, messages, stop=None, run_manager=None, **kwargs):\n",
    "    return _patched_generate(\n",
    "        self, messages, stop=stop, run_manager=run_manager, **kwargs\n",
    "    )\n",
    "\n",
    "\n",
    "# パッチを適用\n",
    "ChatHuggingFace._to_chatml_format = _patched_to_chatml_format\n",
    "ChatHuggingFace._generate = _patched_generate\n",
    "ChatHuggingFace._agenerate = _patched_agenerate\n",
    "\n",
    "model_name = \"ibm-granite/granite-3.3-2b-instruct\"  # Hugging Faceのモデルの名称\n",
    "\n",
    "# LLMモデルの量子化の設定（GPU メモリ節約が必要な場合に有効化する）\n",
    "# quantization_config = BitsAndBytesConfig(\n",
    "#     load_in_4bit=True,  # 4 ビットに量子化された形式で読み込むように指定\n",
    "#     bnb_4bit_quant_type=\"nf4\",  # 4 ビット量子化のデータ型として NF4 を指定\n",
    "#     bnb_4bit_use_double_quant=True,  # 二重量子化の指定\n",
    "#     bnb_4bit_compute_dtype=torch.bfloat16,  # 計算時のデータ型を指定\n",
    "# )\n",
    "\n",
    "# LLMに生成させる文章の計算条件\n",
    "generation_params = {\n",
    "    \"do_sample\": True,\n",
    "    \"temperature\": 0.1,\n",
    "    \"top_k\": 40,\n",
    "    \"top_p\": 0.9,\n",
    "    \"repetition_penalty\": 1.1,\n",
    "    \"max_new_tokens\": 512,\n",
    "}\n",
    "\n",
    "# LLMモデルの読み込み\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,  # Hugging Faceから読み込みするモデルの名称\n",
    "    # quantization_config=quantization_config,\n",
    "    torch_dtype=torch.bfloat16,  # 読み込みするモデルのデータ型\n",
    "    device_map=\"auto\",  # 読み込んだモデルを CPU / GPUに自動で割り当てする指示\n",
    ")\n",
    "\n",
    "# transformersのパイプライン（tools 不使用時の通常生成で使用）\n",
    "hf_pipeline = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    return_full_text=False,  # 入力プロンプトを出力に含めない\n",
    ")\n",
    "\n",
    "# LangChainのHuggingFaceパイプラインに接続\n",
    "llm = HuggingFacePipeline(pipeline=hf_pipeline, pipeline_kwargs=generation_params)\n",
    "\n",
    "# チャットモデルインターフェースとして接続（パッチ適用済み）\n",
    "chat_llm = ChatHuggingFace(llm=llm)\n",
    "\n",
    "# モデルのメモリ消費量（バイト）を取得し、GBに変換して表示\n",
    "memory_footprint_bytes = model.get_memory_footprint()\n",
    "memory_footprint_gb = memory_footprint_bytes / (1024 * 1024 * 1024)\n",
    "print(f\"モデルのメモリ消費量: {memory_footprint_gb:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04c196c8",
   "metadata": {},
   "source": [
    "**MCPサーバ(ddg-search)によるweb検索の実装**\n",
    "- langchain-mcp-adapters の MultiServerMCPClient を使用。\n",
    "- MCPサーバのツールが自動的に LangChain ツールに変換される（@tool の手動定義が不要）。\n",
    "- ReActエージェントが bind_tools() で認識し、自律的にツールを呼び出せる形式になる。\n",
    "\n",
    "**MCPサーバ設定の補足**\n",
    "\n",
    "github に記載されているMCP接続の設定は以下\n",
    "> \"mcpServers\": {\"ddg-search\": {\"command\": \"uvx\", \"args\": [\"duckduckgo-mcp-server\"]}}\n",
    "\n",
    "今回は、pip install済なので、command=\"duckduckgo-mcp-server\" で直接起動。uvx経由ではないので、args=[] としてよい。\n",
    "\n",
    "**Colab の stderr 問題の回避（Colab 特有。通常の Python 環境では不要）**\n",
    "\n",
    "Colab の stderr は fileno() 未対応のため、MCP の stdio_client が失敗する。\n",
    "stdio_client の関数シグネチャ `errlog=sys.stderr` は**インポート時に評価が確定**するため、\n",
    "後から sys.stderr を差し替えても効果がない（Python のデフォルト引数の仕様）。\n",
    "そこで、langchain_mcp_adapters.sessions 内の stdio_client 参照自体を、\n",
    "errlog のデフォルトを /dev/null に変更したラッパー関数に差し替えて回避する。\n",
    "\n",
    "1. /dev/null を書き込みモードで開き、`_devnull` として保持する（stderr の代替出力先）。\n",
    "2. 差し替え前のオリジナル `stdio_client` への参照を `_original_stdio_client` に退避する。\n",
    "3. ラッパー関数 `_patched_stdio_client` を定義する。errlog のデフォルト値だけを `_devnull` に変更し、内部ではオリジナルの `stdio_client` を呼び出す。\n",
    "4. `@contextlib.asynccontextmanager` と `yield` により async with 構文に対応し、MCP セッション完了まで errlog の差し替えが維持される。\n",
    "5. `_sessions.stdio_client` の参照先をラッパー関数に差し替えることで、MultiServerMCPClient が内部で呼ぶ stdio_client もパッチ済みになる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "23f71da7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== ReActエージェント用ツール ===\n",
      "  - search: \n",
      "    Search DuckDuckGo and return formatted results.\n",
      "\n",
      "    Args:\n",
      "        query: The search query string\n",
      "        max_results: Maximum number of results to return (default: 10)\n",
      "        ctx: MCP context for logging\n",
      "    \n",
      "  - fetch_content: \n",
      "    Fetch and parse content from a webpage URL.\n",
      "\n",
      "    Args:\n",
      "        url: The webpage URL to fetch content from\n",
      "        ctx: MCP context for logging\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "# MCPサーバ（duckduckgo-mcp-server）に接続し、LangChainツールを自動取得\n",
    "# langchain-mcp-adapters が MCP ツールを LangChain 互換に自動変換するため、\n",
    "# @tool による手動ラップが不要\n",
    "import os\n",
    "import contextlib\n",
    "from langchain_mcp_adapters import sessions as _sessions  # type: ignore\n",
    "from langchain_mcp_adapters.client import MultiServerMCPClient  # type: ignore\n",
    "\n",
    "# --- Colab stderr 問題の回避パッチ（Colab 特有。通常環境では不要） ---\n",
    "# stdio_client(server, errlog=sys.stderr) のデフォルト値はインポート時に確定する。\n",
    "# Colab の stderr は fileno() 未対応のため、デフォルトのまま呼ぶと失敗する。\n",
    "# → errlog のデフォルトを /dev/null に差し替えたラッパーで上書きして回避する。\n",
    "# NOTE: _devnull はセッション中ずっと開いたままにする（閉じると書き込み先がなくなる）\n",
    "_devnull = open(os.devnull, \"w\")\n",
    "_original_stdio_client = _sessions.stdio_client\n",
    "\n",
    "\n",
    "@contextlib.asynccontextmanager\n",
    "async def _patched_stdio_client(server, errlog=_devnull):\n",
    "    async with _original_stdio_client(server, errlog=errlog) as result:\n",
    "        yield result\n",
    "\n",
    "\n",
    "_sessions.stdio_client = _patched_stdio_client\n",
    "\n",
    "# --- MCPサーバに接続 ---\n",
    "mcp_client = MultiServerMCPClient(\n",
    "    {\n",
    "        \"ddg-search\": {\n",
    "            \"transport\": \"stdio\",\n",
    "            \"command\": \"duckduckgo-mcp-server\",\n",
    "            \"args\": [],\n",
    "        }\n",
    "    }\n",
    ")\n",
    "\n",
    "# MCP ツールを LangChain ツールとして自動取得\n",
    "tools = await mcp_client.get_tools()\n",
    "\n",
    "print(\"=== ReActエージェント用ツール ===\")\n",
    "for t in tools:\n",
    "    print(f\"  - {t.name}: {t.description}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c15n4qfw7i",
   "metadata": {},
   "source": [
    "**LangGraphによるReActエージェントの実装**\n",
    "\n",
    "ReAct（Reasoning + Acting）は、LLM が以下のループを自律的に繰り返すアーキテクチャ。\n",
    "1. **Thought（思考）**: 現在の状況を分析し、次に何をすべきか推論する。\n",
    "2. **Action（行動）**: 必要に応じてツール（Web検索など）を呼び出す。\n",
    "3. **Observation（観察）**: ツールの結果を受け取り、次の思考に反映する。\n",
    "4. 十分な情報が得られたら、最終回答を生成する。\n",
    "\n",
    "LangGraph の `create_react_agent` を使用すると、このループが自動構築される。\n",
    "- `chat_llm.bind_tools(tools)` により、LLM がどのツールを使えるか認識する。\n",
    "- LLM がツールコールを出力 → ツール実行 → 結果をLLMに返却、のサイクルが自動で回る。\n",
    "- `InMemorySaver` により、スレッドごとに会話履歴を保持する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4uy4596zfli",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipython-input-513573795.py:13: LangGraphDeprecatedSinceV10: create_react_agent has been moved to `langchain.agents`. Please update your import to `from langchain.agents import create_agent`. Deprecated in LangGraph V1.0 to be removed in V2.0.\n",
      "  agent = create_react_agent(\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# LangGraph の create_react_agent で ReAct エージェントを構築\n",
    "from langgraph.prebuilt import create_react_agent  # type: ignore\n",
    "from langgraph.checkpoint.memory import InMemorySaver  # type: ignore\n",
    "from IPython.display import Image, display\n",
    "\n",
    "# InMemorySaver：スレッドごとに会話履歴をインメモリで保持\n",
    "memory = InMemorySaver()\n",
    "\n",
    "# ReAct エージェントの構築\n",
    "# - model: ツールコール対応の ChatModel（bind_tools が内部で呼ばれる）\n",
    "# - tools: MCP から自動取得した LangChain ツール（search, fetch_content）\n",
    "# - checkpointer: 会話履歴の永続化\n",
    "agent = create_react_agent(\n",
    "    model=chat_llm,\n",
    "    tools=tools,\n",
    "    checkpointer=memory,\n",
    "    prompt=\"あなたはWeb検索ができるAIアシスタントです。ユーザの質問に日本語で回答してください。必要に応じてsearchツールで検索し、詳細が必要ならfetch_contentで内容を取得してください。\",\n",
    ")\n",
    "\n",
    "# 構築したグラフを図示\n",
    "display(Image(agent.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7i5xv6joy5o",
   "metadata": {},
   "source": [
    "**動作確認**\n",
    "- ReAct エージェントに質問を投げ、自律的に Web 検索 → 回答生成が行われることを確認する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2abd7b30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== content ===\n",
      "(empty)\n",
      "\n",
      "=== tool_calls ===\n",
      "[{'name': 'search', 'args': {'query': '2025年の日本の総人口'}, 'id': 'd9e050c4-8891-4831-8302-a92f44826e63', 'type': 'tool_call'}]\n",
      "\n",
      "=== ツールコール検出 ===\n",
      "OK: ツールコールあり\n"
     ]
    }
   ],
   "source": [
    "# ツールコール生成の診断セル\n",
    "# エージェントを経由せず、パッチ済み chat_llm を直接呼び出して\n",
    "# モデル出力にツールコール形式が含まれるかを確認する。\n",
    "from langchain_core.messages import HumanMessage  # type: ignore\n",
    "\n",
    "bound = chat_llm.bind_tools(tools)\n",
    "result = bound.invoke(\n",
    "    [HumanMessage(content=\"2025年の日本の総人口は何人ですか？ use ddg-search\")]\n",
    ")\n",
    "\n",
    "print(\"=== content ===\")\n",
    "print(result.content[:300] if result.content else \"(empty)\")\n",
    "print()\n",
    "print(\"=== tool_calls ===\")\n",
    "print(result.tool_calls if result.tool_calls else \"(none)\")\n",
    "print()\n",
    "print(\"=== ツールコール検出 ===\")\n",
    "print(\n",
    "    \"OK: ツールコールあり\"\n",
    "    if result.tool_calls\n",
    "    else \"NG: ツールコールなし（再実行してください）\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ab2b6bef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 返答 ===\n",
      "[human] 2025年の日本の総人口は何人ですか？ use ddg-search\n",
      "\n",
      "[ai] 2025年の日本の総人口についての情報を調べました。現在の予測では、2025年までに日本の人口は約9億6,000万人と推定されています。ただし、これはあくまで予測であり、実際の数値は変動する可能性があります。最新の統計や正確なデータについては、国勢調査や厚生労働省などの公式機関の発表を参照することをお勧めします。\n",
      "\n",
      "### 検索結果:\n",
      "- 2025年の日本の人口：約9億6,000万人（予測）\n",
      "- \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ReAct エージェントの動作確認\n",
    "# MCPツールは非同期専用のため ainvoke を使用する\n",
    "config = {\"configurable\": {\"thread_id\": \"react-test-1\"}}\n",
    "\n",
    "# Web 検索が必要な質問\n",
    "response1 = await agent.ainvoke(\n",
    "    {\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": \"2025年の日本の総人口は何人ですか？ use ddg-search\",\n",
    "            }\n",
    "        ]\n",
    "    },\n",
    "    config=config,\n",
    ")\n",
    "\n",
    "print(\"=== 返答 ===\")\n",
    "for msg in response1[\"messages\"]:\n",
    "    print(\n",
    "        f\"[{msg.type}] {msg.content[:200] if isinstance(msg.content, str) else msg.content}\"\n",
    "    )\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
